2023/06/07 19:08:44 kutt-test config testdirs is overridden with args: [ /home/skatyal/gitops-operator/scripts/../test/openshift/e2e/parallel ]
=== RUN   kuttl
    harness.go:457: starting setup
    harness.go:248: running tests using configured kubeconfig.
I0607 19:08:47.369889  250633 request.go:655] Throttling request took 1.000254577s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/helm.openshift.io/v1beta1?timeout=32s
    harness.go:285: Successful connection to cluster at: https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443
I0607 19:08:57.382940  250633 request.go:655] Throttling request took 1.400170023s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/console.openshift.io/v1alpha1?timeout=32s
    harness.go:353: running tests
    harness.go:74: going to run test suite with timeout of 1200 seconds for each step
    harness.go:365: testsuite: /home/skatyal/gitops-operator/scripts/../test/openshift/e2e/parallel has 57 tests
=== RUN   kuttl/harness
=== RUN   kuttl/harness/1-003_validate_console_link
=== PAUSE kuttl/harness/1-003_validate_console_link
=== RUN   kuttl/harness/1-007_validate_namespace_scoped_install
=== PAUSE kuttl/harness/1-007_validate_namespace_scoped_install
=== RUN   kuttl/harness/1-008_validate-custom-argocd-namespace
=== PAUSE kuttl/harness/1-008_validate-custom-argocd-namespace
=== RUN   kuttl/harness/1-009_validate-manage-other-namespace
=== PAUSE kuttl/harness/1-009_validate-manage-other-namespace
=== RUN   kuttl/harness/1-012_validate-managed-by-chain
=== PAUSE kuttl/harness/1-012_validate-managed-by-chain
=== RUN   kuttl/harness/1-019_validate_volume_mounts
=== PAUSE kuttl/harness/1-019_validate_volume_mounts
=== RUN   kuttl/harness/1-021_validate_rolebindings
=== PAUSE kuttl/harness/1-021_validate_rolebindings
=== RUN   kuttl/harness/1-023_validate_repo_server_tls
=== PAUSE kuttl/harness/1-023_validate_repo_server_tls
=== RUN   kuttl/harness/1-025-validate-managed-by-change
=== PAUSE kuttl/harness/1-025-validate-managed-by-change
=== RUN   kuttl/harness/1-029_validate_tls_secret_no_scale
=== PAUSE kuttl/harness/1-029_validate_tls_secret_no_scale
=== RUN   kuttl/harness/1-030_validate_reencrypt
=== PAUSE kuttl/harness/1-030_validate_reencrypt
=== RUN   kuttl/harness/1-032_validate_resource_inclusions
=== PAUSE kuttl/harness/1-032_validate_resource_inclusions
=== RUN   kuttl/harness/1-033_validate_resource_exclusions
=== PAUSE kuttl/harness/1-033_validate_resource_exclusions
=== RUN   kuttl/harness/1-036_validate_keycloak_resource_reqs
=== PAUSE kuttl/harness/1-036_validate_keycloak_resource_reqs
=== RUN   kuttl/harness/1-037_validate_argocd_setting_replicas
=== PAUSE kuttl/harness/1-037_validate_argocd_setting_replicas
=== RUN   kuttl/harness/1-038_validate_productized_images
=== PAUSE kuttl/harness/1-038_validate_productized_images
=== RUN   kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm
=== PAUSE kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm
=== RUN   kuttl/harness/1-043_validate_log_level_format
=== PAUSE kuttl/harness/1-043_validate_log_level_format
=== RUN   kuttl/harness/1-044_validate_resource_limit_changes
=== PAUSE kuttl/harness/1-044_validate_resource_limit_changes
=== RUN   kuttl/harness/1-045_validate_repo_exec_timeout
=== PAUSE kuttl/harness/1-045_validate_repo_exec_timeout
=== RUN   kuttl/harness/1-047_validate_custom_env
=== PAUSE kuttl/harness/1-047_validate_custom_env
=== RUN   kuttl/harness/1-048_validate_controller_sharding
=== PAUSE kuttl/harness/1-048_validate_controller_sharding
=== RUN   kuttl/harness/1-049_validate_parallelism_limit
=== PAUSE kuttl/harness/1-049_validate_parallelism_limit
=== RUN   kuttl/harness/1-051-validate_csv_permissions
=== PAUSE kuttl/harness/1-051-validate_csv_permissions
=== RUN   kuttl/harness/1-052_validate_rolebinding_number
=== PAUSE kuttl/harness/1-052_validate_rolebinding_number
=== RUN   kuttl/harness/1-053_validate_cluster_admin_rbac
=== PAUSE kuttl/harness/1-053_validate_cluster_admin_rbac
=== RUN   kuttl/harness/1-054_validate_deploymentconfig
=== PAUSE kuttl/harness/1-054_validate_deploymentconfig
=== RUN   kuttl/harness/1-055_validate_notification_controller
=== PAUSE kuttl/harness/1-055_validate_notification_controller
=== RUN   kuttl/harness/1-057_validate_notifications
=== PAUSE kuttl/harness/1-057_validate_notifications
=== RUN   kuttl/harness/1-058_validate_prometheus_rule
=== PAUSE kuttl/harness/1-058_validate_prometheus_rule
=== RUN   kuttl/harness/1-061_validate_resource_tracking_method
=== PAUSE kuttl/harness/1-061_validate_resource_tracking_method
=== RUN   kuttl/harness/1-062_validate_extra_config
=== PAUSE kuttl/harness/1-062_validate_extra_config
=== RUN   kuttl/harness/1-063_validate_dex_liveness_probe
=== PAUSE kuttl/harness/1-063_validate_dex_liveness_probe
=== RUN   kuttl/harness/1-063_validate_statefulset_restart
=== PAUSE kuttl/harness/1-063_validate_statefulset_restart
=== RUN   kuttl/harness/1-064_validate_security_contexts
=== PAUSE kuttl/harness/1-064_validate_security_contexts
=== RUN   kuttl/harness/1-065_validate_redis_ha_anti_affinity
=== PAUSE kuttl/harness/1-065_validate_redis_ha_anti_affinity
=== RUN   kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
=== PAUSE kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
=== RUN   kuttl/harness/1-067_validate_redis_secure_comm_no_autotls_ha
=== PAUSE kuttl/harness/1-067_validate_redis_secure_comm_no_autotls_ha
=== RUN   kuttl/harness/1-068_validate_redis_secure_comm_autotls_no_ha
=== PAUSE kuttl/harness/1-068_validate_redis_secure_comm_autotls_no_ha
=== RUN   kuttl/harness/1-069_validate_redis_secure_comm_autotls_ha
=== PAUSE kuttl/harness/1-069_validate_redis_secure_comm_autotls_ha
=== RUN   kuttl/harness/1-070_validate_config_management_plugin
=== PAUSE kuttl/harness/1-070_validate_config_management_plugin
=== RUN   kuttl/harness/1-071_validate_SCC_HA
=== PAUSE kuttl/harness/1-071_validate_SCC_HA
=== RUN   kuttl/harness/1-072_validate_liveness_probe_removed
=== PAUSE kuttl/harness/1-072_validate_liveness_probe_removed
=== RUN   kuttl/harness/1-073_validate_rhsso
=== PAUSE kuttl/harness/1-073_validate_rhsso
=== RUN   kuttl/harness/1-074_validate_terminating_namespace_block
=== PAUSE kuttl/harness/1-074_validate_terminating_namespace_block
=== RUN   kuttl/harness/1-075_validate_dex_anyuid
=== PAUSE kuttl/harness/1-075_validate_dex_anyuid
=== RUN   kuttl/harness/1-077_validate_disable_dex_removed
=== PAUSE kuttl/harness/1-077_validate_disable_dex_removed
=== RUN   kuttl/harness/1-079_validate_vars_for_notificaitons
=== PAUSE kuttl/harness/1-079_validate_vars_for_notificaitons
=== RUN   kuttl/harness/1-080_validate_regex_support_argocd_rbac
=== PAUSE kuttl/harness/1-080_validate_regex_support_argocd_rbac
=== RUN   kuttl/harness/1-081_validate_applicationset_deployment
=== PAUSE kuttl/harness/1-081_validate_applicationset_deployment
=== RUN   kuttl/harness/1-082_validate_node_placement
=== PAUSE kuttl/harness/1-082_validate_node_placement
=== RUN   kuttl/harness/1-083_validate_kustomize_namereference
=== PAUSE kuttl/harness/1-083_validate_kustomize_namereference
=== RUN   kuttl/harness/1-083_validate_resource_customization_subkeys
=== PAUSE kuttl/harness/1-083_validate_resource_customization_subkeys
=== RUN   kuttl/harness/1-084_validate_status_host_ingress
=== PAUSE kuttl/harness/1-084_validate_status_host_ingress
=== RUN   kuttl/harness/1-087_validate_repo_server_settings
=== PAUSE kuttl/harness/1-087_validate_repo_server_settings
=== RUN   kuttl/harness/1-090_validate_permissions
=== PAUSE kuttl/harness/1-090_validate_permissions
=== RUN   kuttl/harness/1-098_validate_dex_clientsecret_deprecated
=== PAUSE kuttl/harness/1-098_validate_dex_clientsecret_deprecated
=== CONT  kuttl/harness/1-003_validate_console_link
    logger.go:42: 19:09:00 | 1-003_validate_console_link | Creating namespace: kuttl-test-square-eagle
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
=== CONT  kuttl/harness/1-038_validate_productized_images
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
=== CONT  kuttl/harness/1-057_validate_notifications
=== CONT  kuttl/harness/1-038_validate_productized_images
    logger.go:42: 19:09:00 | 1-038_validate_productized_images | Creating namespace: kuttl-test-witty-chamois
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:00 | 1-058_validate_prometheus_rule | Creating namespace: kuttl-test-bright-seal
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:09:00 | 1-049_validate_parallelism_limit | Creating namespace: kuttl-test-unbiased-rhino
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:09:00 | 1-057_validate_notifications | Creating namespace: kuttl-test-able-grouper
=== CONT  kuttl/harness/1-003_validate_console_link
    logger.go:42: 19:09:00 | 1-003_validate_console_link/1-check-console-link | starting test step 1-check-console-link
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:00 | 1-058_validate_prometheus_rule/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:09:00 | 1-057_validate_notifications/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:09:00 | 1-049_validate_parallelism_limit/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-038_validate_productized_images
    logger.go:42: 19:09:00 | 1-038_validate_productized_images/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-003_validate_console_link
    logger.go:42: 19:09:05 | 1-003_validate_console_link/1-check-console-link | test step completed 1-check-console-link
    logger.go:42: 19:09:05 | 1-003_validate_console_link | skipping kubernetes event logging
I0607 19:09:07.411271  250633 request.go:655] Throttling request took 1.698298752s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/network.operator.openshift.io/v1?timeout=32s
    logger.go:42: 19:09:09 | 1-003_validate_console_link | Deleting namespace: kuttl-test-square-eagle
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:09:10 | 1-055_validate_notification_controller | Ignoring errors.yaml as it does not match file name regexp: ^(\d+)-(?:[^\.]+)(?:\.yaml)?$
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:15 | 1-058_validate_prometheus_rule/1-install | ArgoCD:kuttl-test-bright-seal/example-argocd created
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:09:15 | 1-057_validate_notifications/1-install | ArgoCD:kuttl-test-able-grouper/argocd created
    logger.go:42: 19:09:16 | 1-057_validate_notifications/1-install | Service:kuttl-test-able-grouper/smtp4dev created
    logger.go:42: 19:09:16 | 1-057_validate_notifications/1-install | Deployment:kuttl-test-able-grouper/smtp4dev created
I0607 19:09:17.433845  250633 request.go:655] Throttling request took 2.300142132s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/monitoring.coreos.com/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:09:19 | 1-055_validate_notification_controller | Creating namespace: kuttl-test-dashing-donkey
    logger.go:42: 19:09:19 | 1-055_validate_notification_controller/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:24 | 1-058_validate_prometheus_rule/1-install | test step completed 1-install
    logger.go:42: 19:09:24 | 1-058_validate_prometheus_rule/2-wait | starting test step 2-wait
=== CONT  kuttl/harness/1-038_validate_productized_images
    logger.go:42: 19:09:25 | 1-038_validate_productized_images/1-install | ArgoCD:kuttl-test-witty-chamois/argocd created
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:09:25 | 1-049_validate_parallelism_limit/1-install | ArgoCD:kuttl-test-unbiased-rhino/argocd created
I0607 19:09:27.444854  250633 request.go:655] Throttling request took 2.447414695s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/authorization.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:29 | 1-058_validate_prometheus_rule/2-wait | running command: [sh -c function wait_until_pods_running() {
          echo -n "Waiting until all pods in namespace $1 are up"
          for i in {1..150}; do  # timeout after 5 minutes
            local pods="$(oc get pods --no-headers -n $1 2>/dev/null)"
            # All pods must be running
            local not_running=$(echo "${pods}" | grep -v Running | grep -v Completed | wc -l)
            if [[ -n "${pods}" && ${not_running} -eq 0 ]]; then
              local all_ready=1
              while read pod ; do
                local status=(`echo -n ${pod} | cut -f2 -d' ' | tr '/' ' '`)
                # All containers must be ready
                [[ -z ${status[0]} ]] && all_ready=0 && break
                [[ -z ${status[1]} ]] && all_ready=0 && break
                [[ ${status[0]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[1]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[0]} -ne ${status[1]} ]] && all_ready=0 && break
              done <<< $(echo "${pods}" | grep -v Completed)
              if (( all_ready )); then
                echo -e "\nAll pods are up:\n${pods}"
                return 0
              fi
            fi
            echo -n "."
            sleep 2
          done
          echo -e "\n\nERROR: timeout waiting for pods to come up\n${pods}"
          return 1
        }
        
        wait_until_pods_running $NAMESPACE
        ]
=== CONT  kuttl/harness/1-038_validate_productized_images
    logger.go:42: 19:09:29 | 1-038_validate_productized_images/1-install | test step completed 1-install
    logger.go:42: 19:09:29 | 1-038_validate_productized_images/2-check-images | starting test step 2-check-images
    logger.go:42: 19:09:29 | 1-038_validate_productized_images/2-check-images | running command: [sh -c sleep 10
        for wl in deployment/argocd-server deployment/argocd-repo-server statefulset/argocd-application-controller; do
          image=$(oc -n $NAMESPACE get ${wl} -o jsonpath='{.spec.template.spec.containers[0].image}' | awk -F'@' '{print $1}')
          if test "$image" != "registry.redhat.io/openshift-gitops-1/argocd-rhel8"; then
            echo "Non-productized image in workload $wl detected."
            if [ -z $CI ]; then 
              exit 1
            else 
              exit 0
            fi
          fi
        done
        ]
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:09:29 | 1-055_validate_notification_controller/1-install | ArgoCD:kuttl-test-dashing-donkey/example-argocd created
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:31 | 1-058_validate_prometheus_rule/2-wait | Waiting until all pods in namespace kuttl-test-bright-seal are up
    logger.go:42: 19:09:31 | 1-058_validate_prometheus_rule/2-wait | All pods are up:
    logger.go:42: 19:09:31 | 1-058_validate_prometheus_rule/2-wait | example-argocd-application-controller-0       1/1   Running   0     15s
    logger.go:42: 19:09:31 | 1-058_validate_prometheus_rule/2-wait | example-argocd-redis-5bff48857f-sz9t2         1/1   Running   0     15s
    logger.go:42: 19:09:31 | 1-058_validate_prometheus_rule/2-wait | example-argocd-repo-server-7b86d86496-c57qr   1/1   Running   0     15s
    logger.go:42: 19:09:31 | 1-058_validate_prometheus_rule/2-wait | example-argocd-server-fbbdd4844-jjjsp         1/1   Running   0     15s
    logger.go:42: 19:09:36 | 1-058_validate_prometheus_rule/2-wait | test step completed 2-wait
    logger.go:42: 19:09:36 | 1-058_validate_prometheus_rule/3-create_application | starting test step 3-create_application
    logger.go:42: 19:09:36 | 1-058_validate_prometheus_rule/3-create_application | running command: [sh -c set -e
        
        cat << EOF | oc apply -f -
        apiVersion: argoproj.io/v1alpha1
        kind: Application
        metadata:
          name: test-1-58-custom
          namespace: ${NAMESPACE}
        spec:
          project: default
          source:
            repoURL: https://github.com/jaideepr97/gitops-examples
            path: bgd-k8s
            targetRevision: "HEAD"
          destination:
            server: "https://kubernetes.default.svc"
            namespace: ${NAMESPACE}
          syncPolicy:
            automated: {}
        EOF
        ]
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:09:37 | 1-055_validate_notification_controller/1-install | test step completed 1-install
    logger.go:42: 19:09:37 | 1-055_validate_notification_controller/2-enable_notification | starting test step 2-enable_notification
I0607 19:09:39.125905  250633 request.go:655] Throttling request took 1.049505424s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/psmdb.percona.com/v1-12-0?timeout=32s
=== CONT  kuttl/harness/1-038_validate_productized_images
    logger.go:42: 19:09:41 | 1-038_validate_productized_images/2-check-images | Non-productized image in workload deployment/argocd-server detected.
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:09:43 | 1-055_validate_notification_controller/2-enable_notification | ArgoCD:kuttl-test-dashing-donkey/example-argocd updated
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:43 | 1-058_validate_prometheus_rule/3-create_application | application.argoproj.io/test-1-58-custom created
=== CONT  kuttl/harness/1-038_validate_productized_images
    case.go:361: failed in step 2-check-images
    case.go:363: exit status 1
    logger.go:42: 19:09:46 | 1-038_validate_productized_images | skipping kubernetes event logging
    logger.go:42: 19:09:46 | 1-038_validate_productized_images | Deleting namespace: kuttl-test-witty-chamois
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:09:47 | 1-049_validate_parallelism_limit/1-install | test step completed 1-install
    logger.go:42: 19:09:47 | 1-049_validate_parallelism_limit/2-check-deployment | starting test step 2-check-deployment
I0607 19:09:49.167458  250633 request.go:655] Throttling request took 1.748435645s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/tekton.dev/v1?timeout=32s
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:09:51 | 1-054_validate_deploymentconfig | Creating namespace: kuttl-test-literate-goose
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:09:51 | 1-049_validate_parallelism_limit/2-check-deployment | running command: [sh -c set -e
        set -o pipefail
        expected=10
        wlCommand=$(oc get -n $NAMESPACE statefulset/argocd-application-controller -o jsonpath='{.spec.template.spec.containers[0].command}'| jq -r '.[]' )
        if ! echo "$wlCommand" | grep -qPz -- "--kubectl-parallelism-limit\\n${expected}(\$|\\n)"; then
          echo "Incorrect or missing --kubectl-parallelism-limit detected."
          echo "$wlCommand"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:09:51 | 1-054_validate_deploymentconfig/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-058_validate_prometheus_rule
    logger.go:42: 19:09:52 | 1-058_validate_prometheus_rule/3-create_application | test step completed 3-create_application
    logger.go:42: 19:09:52 | 1-058_validate_prometheus_rule | skipping kubernetes event logging
    logger.go:42: 19:09:56 | 1-058_validate_prometheus_rule | Deleting namespace: kuttl-test-bright-seal
=== CONT  kuttl/harness/1-053_validate_cluster_admin_rbac
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:09:57 | 1-054_validate_deploymentconfig/1-install | ArgoCD:kuttl-test-literate-goose/argocd created
I0607 19:09:59.215542  250633 request.go:655] Throttling request took 2.199083642s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/node.k8s.io/v1?timeout=32s
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:01 | 1-049_validate_parallelism_limit/2-check-deployment | test step completed 2-check-deployment
    logger.go:42: 19:10:01 | 1-049_validate_parallelism_limit/3-change-limit | starting test step 3-change-limit
=== CONT  kuttl/harness/1-053_validate_cluster_admin_rbac
    logger.go:42: 19:10:05 | 1-053_validate_cluster_admin_rbac | Creating namespace: kuttl-test-chief-javelin
    logger.go:42: 19:10:06 | 1-053_validate_cluster_admin_rbac/1- | starting test step 1-
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:10:06 | 1-054_validate_deploymentconfig/1-install | test step completed 1-install
    logger.go:42: 19:10:06 | 1-054_validate_deploymentconfig/2-install-deploymentconfig | starting test step 2-install-deploymentconfig
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:06 | 1-049_validate_parallelism_limit/3-change-limit | ArgoCD:kuttl-test-unbiased-rhino/argocd updated
    logger.go:42: 19:10:06 | 1-049_validate_parallelism_limit/3-change-limit | test step completed 3-change-limit
    logger.go:42: 19:10:06 | 1-049_validate_parallelism_limit/4-check-deployment | starting test step 4-check-deployment
I0607 19:10:09.236335  250633 request.go:655] Throttling request took 2.549790567s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/discovery.k8s.io/v1?timeout=32s
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:10:10 | 1-054_validate_deploymentconfig/2-install-deploymentconfig | running command: [sh -c set -eo pipefail
        # Install ArgoCD Application with 2 replicas
        cat << EOF | oc apply -f -
        apiVersion: argoproj.io/v1alpha1
        kind: Application
        metadata:
          name: app-deploymentconfig
          namespace: ${NAMESPACE}
        spec:
          project: default
          source:
            repoURL: https://github.com/bluengo/argocd-apps-examples
            path: ./deploymentconfig-example
            targetRevision: deploymentconfig_2-replicas
          destination:
            server: https://kubernetes.default.svc
            namespace: ${NAMESPACE}
          syncPolicy:
            automated: {}
        EOF
        # Give some time to Application before asserting
        sleep 10
        
        exit 0
        ]
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:10 | 1-049_validate_parallelism_limit/4-check-deployment | running command: [sh -c sleep 5]
=== CONT  kuttl/harness/1-053_validate_cluster_admin_rbac
    logger.go:42: 19:10:11 | 1-053_validate_cluster_admin_rbac/1- | test step completed 1-
    logger.go:42: 19:10:11 | 1-053_validate_cluster_admin_rbac/2- | starting test step 2-
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:10:11 | 1-057_validate_notifications/1-install | test step completed 1-install
    logger.go:42: 19:10:11 | 1-057_validate_notifications/2-update-notifications-cm | starting test step 2-update-notifications-cm
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:10:13 | 1-054_validate_deploymentconfig/2-install-deploymentconfig | application.argoproj.io/app-deploymentconfig created
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:15 | 1-049_validate_parallelism_limit/4-check-deployment | running command: [sh -c set -e
        set -o pipefail
        expected=20
        wlCommand=$(oc get -n $NAMESPACE statefulset/argocd-application-controller -o jsonpath='{.spec.template.spec.containers[0].command}'| jq -r '.[]' )
        if ! echo "$wlCommand" | grep -qPz -- "--kubectl-parallelism-limit\\n${expected}(\$|\\n)"; then
          echo "Incorrect or missing --kubectl-parallelism-limit detected."
          echo "$wlCommand"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:10:16 | 1-057_validate_notifications/2-update-notifications-cm | running command: [sh -c sleep 15]
=== CONT  kuttl/harness/1-053_validate_cluster_admin_rbac
    logger.go:42: 19:10:17 | 1-053_validate_cluster_admin_rbac/2- | test step completed 2-
    logger.go:42: 19:10:17 | 1-053_validate_cluster_admin_rbac | skipping kubernetes event logging
I0607 19:10:19.275370  250633 request.go:655] Throttling request took 1.89988639s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/console.openshift.io/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:21 | 1-049_validate_parallelism_limit/4-check-deployment | test step completed 4-check-deployment
    logger.go:42: 19:10:21 | 1-049_validate_parallelism_limit/5-change-back-to-default | starting test step 5-change-back-to-default
I0607 19:10:29.310588  250633 request.go:655] Throttling request took 2.59995567s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/tekton.dev/v1beta1?timeout=32s
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:10:30 | 1-054_validate_deploymentconfig/2-install-deploymentconfig | test step completed 2-install-deploymentconfig
    logger.go:42: 19:10:30 | 1-054_validate_deploymentconfig/3- | starting test step 3-
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:10:31 | 1-057_validate_notifications/2-update-notifications-cm | running command: [sh -c set -e 
        
        kubectl patch cm argocd-notifications-cm -n $NAMESPACE --type merge -p '{"data": {"service.email.gmail": "{host: smtp4dev, port: 2525, from: fake@email.com }" }}'
        ]
    logger.go:42: 19:10:33 | 1-057_validate_notifications/2-update-notifications-cm | configmap/argocd-notifications-cm patched
    logger.go:42: 19:10:33 | 1-057_validate_notifications/2-update-notifications-cm | running command: [sh -c sleep 5]
=== CONT  kuttl/harness/1-053_validate_cluster_admin_rbac
    logger.go:42: 19:10:35 | 1-053_validate_cluster_admin_rbac | Deleting namespace: kuttl-test-chief-javelin
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:10:36 | 1-025-validate-managed-by-change | Creating namespace: kuttl-test-precious-gelding
    logger.go:42: 19:10:36 | 1-025-validate-managed-by-change/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:36 | 1-049_validate_parallelism_limit/5-change-back-to-default | ArgoCD:kuttl-test-unbiased-rhino/argocd updated
    logger.go:42: 19:10:36 | 1-049_validate_parallelism_limit/5-change-back-to-default | test step completed 5-change-back-to-default
    logger.go:42: 19:10:36 | 1-049_validate_parallelism_limit/6-check-deployment | starting test step 6-check-deployment
I0607 19:10:39.324305  250633 request.go:655] Throttling request took 2.150168158s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1alpha1?timeout=32s
    logger.go:42: 19:10:41 | 1-049_validate_parallelism_limit/6-check-deployment | running command: [sh -c set -e
        oc patch -n $NAMESPACE argocds/argocd --type=json --patch '[{"op": "remove", "path": "/spec/controller/parallelismLimit"}]'
        ]
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:10:41 | 1-054_validate_deploymentconfig/3- | test step completed 3-
    logger.go:42: 19:10:41 | 1-054_validate_deploymentconfig/4-scaledown-deploymentconfig | starting test step 4-scaledown-deploymentconfig
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:10:42 | 1-025-validate-managed-by-change/1-install | Namespace:/test-1-25-target created
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:42 | 1-049_validate_parallelism_limit/6-check-deployment | argocd.argoproj.io/argocd patched
    logger.go:42: 19:10:42 | 1-049_validate_parallelism_limit/6-check-deployment | running command: [sh -c sleep 5]
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:10:42 | 1-025-validate-managed-by-change/1-install | Namespace:/test-1-25-argo1 created
    logger.go:42: 19:10:43 | 1-025-validate-managed-by-change/1-install | Namespace:/test-1-25-argo2 created
    logger.go:42: 19:10:43 | 1-025-validate-managed-by-change/1-install | ArgoCD:test-1-25-argo1/argocd created
    logger.go:42: 19:10:44 | 1-025-validate-managed-by-change/1-install | ArgoCD:test-1-25-argo2/argocd created
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:10:45 | 1-057_validate_notifications/2-update-notifications-cm | test step completed 2-update-notifications-cm
    logger.go:42: 19:10:45 | 1-057_validate_notifications/3-create-app | starting test step 3-create-app
    logger.go:42: 19:10:45 | 1-057_validate_notifications/3-create-app | running command: [sh -c set -e
        
        cat << EOF | oc apply -f -
        apiVersion: argoproj.io/v1alpha1
        kind: Application
        metadata:
          name: my-app-3
          namespace: $NAMESPACE
          annotations:
            "notifications.argoproj.io/subscribe.on-created.gmail": "jdfake@email.com"
        spec:
          destination:
            namespace: $NAMESPACE
            server: https://kubernetes.default.svc
          project: default
          source:
            repoURL: https://github.com/redhat-developer/gitops-operator
            path: test/examples/nginx
            targetRevision: HEAD
        EOF
        ]
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:47 | 1-049_validate_parallelism_limit/6-check-deployment | running command: [sh -c set -e
        set -o pipefail
        expected=10
        wlCommand=$(oc get -n $NAMESPACE statefulset/argocd-application-controller -o jsonpath='{.spec.template.spec.containers[0].command}'| jq -r '.[]' )
        if ! echo "$wlCommand" | grep -qPz -- "--kubectl-parallelism-limit\\n${expected}(\$|\\n)"; then
          echo "Incorrect or missing --kubectl-parallelism-limit detected."
          echo "$wlCommand"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:10:47 | 1-057_validate_notifications/3-create-app | application.argoproj.io/my-app-3 created
    logger.go:42: 19:10:47 | 1-057_validate_notifications/3-create-app | running command: [sh -c sleep 5]
I0607 19:10:49.370040  250633 request.go:655] Throttling request took 2.894341507s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/operator.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-049_validate_parallelism_limit
    logger.go:42: 19:10:55 | 1-049_validate_parallelism_limit/6-check-deployment | test step completed 6-check-deployment
    logger.go:42: 19:10:55 | 1-049_validate_parallelism_limit | skipping kubernetes event logging
    logger.go:42: 19:10:55 | 1-049_validate_parallelism_limit | Deleting namespace: kuttl-test-unbiased-rhino
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:10:56 | 1-054_validate_deploymentconfig/4-scaledown-deploymentconfig | Application:kuttl-test-literate-goose/app-deploymentconfig updated
I0607 19:10:59.412231  250633 request.go:655] Throttling request took 3.199768633s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-4-0?timeout=32s
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:11:00 | 1-057_validate_notifications/3-create-app | test step completed 3-create-app
    logger.go:42: 19:11:00 | 1-057_validate_notifications/4-delete-app | starting test step 4-delete-app
    logger.go:42: 19:11:00 | 1-057_validate_notifications/4-delete-app | running command: [sh -c set -e 
        
        kubectl delete -n $NAMESPACE application.argoproj.io my-app-3
        ]
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:00 | 1-037_validate_argocd_setting_replicas | Creating namespace: kuttl-test-noble-phoenix
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:00 | 1-025-validate-managed-by-change/1-install | test step completed 1-install
    logger.go:42: 19:11:00 | 1-025-validate-managed-by-change/2-install-app | starting test step 2-install-app
    logger.go:42: 19:11:00 | 1-025-validate-managed-by-change/2-install-app | running command: [sh -c function wait_until_pods_running() {
          echo -n "Waiting until all pods in namespace $1 are up"
          for i in {1..150}; do  # timeout after 5 minutes
            local pods="$(oc get pods --no-headers -n $1 2>/dev/null)"
            # All pods must be running
            local not_running=$(echo "${pods}" | grep -v Running | grep -v Completed | wc -l)
            if [[ -n "${pods}" && ${not_running} -eq 0 ]]; then
              local all_ready=1
              while read pod ; do
                local status=(`echo -n ${pod} | cut -f2 -d' ' | tr '/' ' '`)
                # All containers must be ready
                [[ -z ${status[0]} ]] && all_ready=0 && break
                [[ -z ${status[1]} ]] && all_ready=0 && break
                [[ ${status[0]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[1]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[0]} -ne ${status[1]} ]] && all_ready=0 && break
              done <<< $(echo "${pods}" | grep -v Completed)
              if (( all_ready )); then
                echo -e "\nAll pods are up:\n${pods}"
                return 0
              fi
            fi
            echo -n "."
            sleep 2
          done
          echo -e "\n\nERROR: timeout waiting for pods to come up\n${pods}"
          return 1
        }
        
        wait_until_pods_running "test-1-25-argo1"
        wait_until_pods_running "test-1-25-argo2"
        ]
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:00 | 1-037_validate_argocd_setting_replicas/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:11:02 | 1-057_validate_notifications/4-delete-app | application.argoproj.io "my-app-3" deleted
    logger.go:42: 19:11:03 | 1-057_validate_notifications/4-delete-app | running command: [sh -c sleep 5]
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:11:05 | 1-055_validate_notification_controller/2-enable_notification | test step completed 2-enable_notification
    logger.go:42: 19:11:05 | 1-055_validate_notification_controller/3-disable_notification | starting test step 3-disable_notification
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:11:05 | 1-054_validate_deploymentconfig/4-scaledown-deploymentconfig | test step completed 4-scaledown-deploymentconfig
    logger.go:42: 19:11:05 | 1-054_validate_deploymentconfig | skipping kubernetes event logging
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:06 | 1-037_validate_argocd_setting_replicas/1-install | ArgoCD:kuttl-test-noble-phoenix/argocd created
I0607 19:11:09.442347  250633 request.go:655] Throttling request took 2.99929987s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/scheduling.k8s.io/v1?timeout=32s
    logger.go:42: 19:11:11 | 1-037_validate_argocd_setting_replicas/1-install | test step completed 1-install
    logger.go:42: 19:11:11 | 1-037_validate_argocd_setting_replicas/2-scale_out_server | starting test step 2-scale_out_server
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:11:11 | 1-055_validate_notification_controller/3-disable_notification | ArgoCD:kuttl-test-dashing-donkey/example-argocd updated
    logger.go:42: 19:11:11 | 1-055_validate_notification_controller/3-disable_notification | test step completed 3-disable_notification
    logger.go:42: 19:11:11 | 1-055_validate_notification_controller/4-check | starting test step 4-check
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:15 | 1-037_validate_argocd_setting_replicas/2-scale_out_server | running command: [sh -c oc patch argocd argocd \
        -n $NAMESPACE \
        --type='json' \
        -p='[{"op": "replace", "path": "/spec/server/replicas", "value": 3 }]'
        exit 0
        ]
    logger.go:42: 19:11:16 | 1-037_validate_argocd_setting_replicas/2-scale_out_server | argocd.argoproj.io/argocd patched
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:17 | 1-025-validate-managed-by-change/2-install-app | Waiting until all pods in namespace test-1-25-argo1 are up.....
    logger.go:42: 19:11:17 | 1-025-validate-managed-by-change/2-install-app | All pods are up:
    logger.go:42: 19:11:17 | 1-025-validate-managed-by-change/2-install-app | argocd-application-controller-0       1/1   Running   0     31s
    logger.go:42: 19:11:17 | 1-025-validate-managed-by-change/2-install-app | argocd-redis-78c854d978-k4l89         1/1   Running   0     31s
    logger.go:42: 19:11:17 | 1-025-validate-managed-by-change/2-install-app | argocd-repo-server-67b4dfb8d9-m6jnh   1/1   Running   0     31s
    logger.go:42: 19:11:17 | 1-025-validate-managed-by-change/2-install-app | argocd-server-7d44fc8b57-vbk97        1/1   Running   0     31s
    logger.go:42: 19:11:18 | 1-025-validate-managed-by-change/2-install-app | Waiting until all pods in namespace test-1-25-argo2 are up
    logger.go:42: 19:11:18 | 1-025-validate-managed-by-change/2-install-app | All pods are up:
    logger.go:42: 19:11:18 | 1-025-validate-managed-by-change/2-install-app | argocd-application-controller-0       1/1   Running   0     31s
    logger.go:42: 19:11:18 | 1-025-validate-managed-by-change/2-install-app | argocd-redis-78c854d978-f7wh9         1/1   Running   0     31s
    logger.go:42: 19:11:18 | 1-025-validate-managed-by-change/2-install-app | argocd-repo-server-587d686568-mk2lz   1/1   Running   0     31s
    logger.go:42: 19:11:18 | 1-025-validate-managed-by-change/2-install-app | argocd-server-5d7b5bd8cd-hp9cx        1/1   Running   0     31s
I0607 19:11:19.477246  250633 request.go:655] Throttling request took 3.798640334s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/operators.coreos.com/v1?timeout=32s
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:24 | 1-037_validate_argocd_setting_replicas/2-scale_out_server | test step completed 2-scale_out_server
    logger.go:42: 19:11:24 | 1-037_validate_argocd_setting_replicas/3-scale_out_repo | starting test step 3-scale_out_repo
    logger.go:42: 19:11:24 | 1-037_validate_argocd_setting_replicas/3-scale_out_repo | running command: [sh -c oc patch argocd argocd \
        -n $NAMESPACE \
        --type='json' \
        -p='[{"op": "replace", "path": "/spec/repo/replicas", "value": 3 }]'
        exit 0
        ]
    logger.go:42: 19:11:25 | 1-037_validate_argocd_setting_replicas/3-scale_out_repo | argocd.argoproj.io/argocd patched
=== CONT  kuttl/harness/1-054_validate_deploymentconfig
    logger.go:42: 19:11:29 | 1-054_validate_deploymentconfig | Deleting namespace: kuttl-test-literate-goose
=== CONT  kuttl/harness/1-036_validate_keycloak_resource_reqs
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:29 | 1-025-validate-managed-by-change/2-install-app | Application:test-1-25-argo1/guestbook created
I0607 19:11:30.637108  250633 request.go:655] Throttling request took 1.04839301s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/tekton.dev/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:33 | 1-037_validate_argocd_setting_replicas/3-scale_out_repo | test step completed 3-scale_out_repo
    logger.go:42: 19:11:33 | 1-037_validate_argocd_setting_replicas/4- | starting test step 4-
=== CONT  kuttl/harness/1-036_validate_keycloak_resource_reqs
    logger.go:42: 19:11:38 | 1-036_validate_keycloak_resource_reqs | Creating namespace: kuttl-test-tolerant-ant
    logger.go:42: 19:11:38 | 1-036_validate_keycloak_resource_reqs/1-install | starting test step 1-install
I0607 19:11:40.675125  250633 request.go:655] Throttling request took 1.249212557s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/discovery.k8s.io/v1?timeout=32s
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:43 | 1-025-validate-managed-by-change/2-install-app | test step completed 2-install-app
    logger.go:42: 19:11:43 | 1-025-validate-managed-by-change/3-update-namespace | starting test step 3-update-namespace
=== CONT  kuttl/harness/1-036_validate_keycloak_resource_reqs
    logger.go:42: 19:11:44 | 1-036_validate_keycloak_resource_reqs/1-install | ArgoCD:kuttl-test-tolerant-ant/argocd created
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:49 | 1-037_validate_argocd_setting_replicas/4- | test step completed 4-
    logger.go:42: 19:11:49 | 1-037_validate_argocd_setting_replicas/5-scale_in_server | starting test step 5-scale_in_server
    logger.go:42: 19:11:49 | 1-037_validate_argocd_setting_replicas/5-scale_in_server | running command: [sh -c oc patch argocd argocd \
        -n $NAMESPACE \
        --type='json' \
        -p='[{"op": "replace", "path": "/spec/server/replicas", "value": 1 }]'
        exit 0
        ]
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:49 | 1-025-validate-managed-by-change/3-update-namespace | Namespace:/test-1-25-target updated
=== CONT  kuttl/harness/1-055_validate_notification_controller
    logger.go:42: 19:11:49 | 1-055_validate_notification_controller/4-check | test step completed 4-check
    logger.go:42: 19:11:49 | 1-055_validate_notification_controller | skipping kubernetes event logging
    logger.go:42: 19:11:49 | 1-055_validate_notification_controller | Deleting namespace: kuttl-test-dashing-donkey
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:49 | 1-025-validate-managed-by-change/3-update-namespace | Application:test-1-25-argo1/guestbook updated
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:11:49 | 1-033_validate_resource_exclusions | Creating namespace: kuttl-test-safe-koala
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:50 | 1-037_validate_argocd_setting_replicas/5-scale_in_server | argocd.argoproj.io/argocd patched
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:11:50 | 1-033_validate_resource_exclusions/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:11:50 | 1-057_validate_notifications/4-delete-app | test step completed 4-delete-app
    logger.go:42: 19:11:50 | 1-057_validate_notifications/5-verify-email | starting test step 5-verify-email
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:11:50 | 1-025-validate-managed-by-change/3-update-namespace | Application:test-1-25-argo2/guestbook created
I0607 19:11:51.936192  250633 request.go:655] Throttling request took 1.049785271s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-1-0?timeout=32s
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:11:55 | 1-057_validate_notifications/5-verify-email | running command: [sh -c set -e
        sleep 5
        smtp4dev_pod=$(kubectl get pod -l=app=smtp4dev -o NAME -n $NAMESPACE)
        exit_code=$(kubectl -n $NAMESPACE exec --stdin "${smtp4dev_pod}" -- /bin/bash \
        -c 'if [[ $(grep -rnw /tmp -e "Subject: Application my-app-3 has been created.") ]]; then
        exit 0; else
        exit 1;
        fi')
        
        
        if [[ $exit_code -eq 0 ]]; then
            exit 0
        else
            exit 1
        fi
        ]
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:11:55 | 1-033_validate_resource_exclusions/1-install | ArgoCD:kuttl-test-safe-koala/argocd created
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:11:59 | 1-037_validate_argocd_setting_replicas/5-scale_in_server | test step completed 5-scale_in_server
    logger.go:42: 19:11:59 | 1-037_validate_argocd_setting_replicas/6-scale_in_repo | starting test step 6-scale_in_repo
    logger.go:42: 19:11:59 | 1-037_validate_argocd_setting_replicas/6-scale_in_repo | running command: [sh -c oc patch argocd argocd \
        -n $NAMESPACE \
        --type='json' \
        -p='[{"op": "replace", "path": "/spec/repo/replicas", "value": 1 }]'
        exit 0
        ]
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:12:00 | 1-033_validate_resource_exclusions/1-install | test step completed 1-install
    logger.go:42: 19:12:00 | 1-033_validate_resource_exclusions/2-add_resource_exclusions | starting test step 2-add_resource_exclusions
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:12:00 | 1-025-validate-managed-by-change/3-update-namespace | test step completed 3-update-namespace
    logger.go:42: 19:12:00 | 1-025-validate-managed-by-change/99-delete | starting test step 99-delete
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:12:00 | 1-037_validate_argocd_setting_replicas/6-scale_in_repo | argocd.argoproj.io/argocd patched
I0607 19:12:01.957635  250633 request.go:655] Throttling request took 1.300956729s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/cloud.network.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:12:05 | 1-033_validate_resource_exclusions/2-add_resource_exclusions | ArgoCD:kuttl-test-safe-koala/argocd updated
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:12:09 | 1-037_validate_argocd_setting_replicas/6-scale_in_repo | test step completed 6-scale_in_repo
    logger.go:42: 19:12:09 | 1-037_validate_argocd_setting_replicas/7- | starting test step 7-
I0607 19:12:12.006107  250633 request.go:655] Throttling request took 1.94977238s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/samples.operator.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-057_validate_notifications
    logger.go:42: 19:12:18 | 1-057_validate_notifications/5-verify-email | test step completed 5-verify-email
    logger.go:42: 19:12:18 | 1-057_validate_notifications | skipping kubernetes event logging
    logger.go:42: 19:12:18 | 1-057_validate_notifications | Deleting namespace: kuttl-test-able-grouper
=== CONT  kuttl/harness/1-052_validate_rolebinding_number
    logger.go:42: 19:12:19 | 1-052_validate_rolebinding_number | Creating namespace: kuttl-test-inspired-dinosaur
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:12:19 | 1-033_validate_resource_exclusions/2-add_resource_exclusions | test step completed 2-add_resource_exclusions
    logger.go:42: 19:12:19 | 1-033_validate_resource_exclusions | skipping kubernetes event logging
=== CONT  kuttl/harness/1-052_validate_rolebinding_number
    logger.go:42: 19:12:19 | 1-052_validate_rolebinding_number/1-label_namespace | starting test step 1-label_namespace
    logger.go:42: 19:12:19 | 1-052_validate_rolebinding_number/1-label_namespace | running command: [sh -c oc label namespace $NAMESPACE argocd.argoproj.io/managed-by=openshift-gitops
        ]
=== CONT  kuttl/harness/1-033_validate_resource_exclusions
    logger.go:42: 19:12:20 | 1-033_validate_resource_exclusions | Deleting namespace: kuttl-test-safe-koala
=== CONT  kuttl/harness/1-037_validate_argocd_setting_replicas
    logger.go:42: 19:12:20 | 1-037_validate_argocd_setting_replicas/7- | test step completed 7-
    logger.go:42: 19:12:20 | 1-037_validate_argocd_setting_replicas | skipping kubernetes event logging
    logger.go:42: 19:12:20 | 1-037_validate_argocd_setting_replicas | Deleting namespace: kuttl-test-noble-phoenix
=== CONT  kuttl/harness/1-073_validate_rhsso
=== CONT  kuttl/harness/1-051-validate_csv_permissions
    logger.go:42: 19:12:20 | 1-051-validate_csv_permissions | Creating namespace: kuttl-test-smooth-stork
=== CONT  kuttl/harness/1-036_validate_keycloak_resource_reqs
    logger.go:42: 19:12:20 | 1-036_validate_keycloak_resource_reqs/1-install | test step completed 1-install
    logger.go:42: 19:12:20 | 1-036_validate_keycloak_resource_reqs/2-update-sso-keycloak-provider | starting test step 2-update-sso-keycloak-provider
    logger.go:42: 19:12:20 | 1-036_validate_keycloak_resource_reqs/2-update-sso-keycloak-provider | running command: [sh -c oc patch -n $NAMESPACE argocd/argocd --type='json' -p='[{"op": "add", "path": "/spec/sso", "value": {"provider": "keycloak"}}]'
        ]
=== CONT  kuttl/harness/1-073_validate_rhsso
    logger.go:42: 19:12:20 | 1-073_validate_rhsso | Creating namespace: kuttl-test-magical-phoenix
    logger.go:42: 19:12:20 | 1-073_validate_rhsso/1-argocd-rhsso | starting test step 1-argocd-rhsso
=== CONT  kuttl/harness/1-051-validate_csv_permissions
    logger.go:42: 19:12:20 | 1-051-validate_csv_permissions/1-validate | starting test step 1-validate
=== CONT  kuttl/harness/1-052_validate_rolebinding_number
    logger.go:42: 19:12:20 | 1-052_validate_rolebinding_number/1-label_namespace | namespace/kuttl-test-inspired-dinosaur labeled
=== CONT  kuttl/harness/1-036_validate_keycloak_resource_reqs
    logger.go:42: 19:12:21 | 1-036_validate_keycloak_resource_reqs/2-update-sso-keycloak-provider | argocd.argoproj.io/argocd patched
I0607 19:12:22.348427  250633 request.go:655] Throttling request took 1.050465346s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/operator.openshift.io/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-051-validate_csv_permissions
    logger.go:42: 19:12:25 | 1-051-validate_csv_permissions/1-validate | running command: [sh -c set -e
        res=$(oc auth can-i delete resourcequotas -n openshift-gitops --as system:serviceaccount:openshift-operators:gitops-operator-controller-manager)
        if test "$res" != "yes"; then
          echo "Can't delete resourcequotas"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-073_validate_rhsso
    logger.go:42: 19:12:26 | 1-073_validate_rhsso/1-argocd-rhsso | ArgoCD:kuttl-test-magical-phoenix/example-argocd-keycloak created
=== CONT  kuttl/harness/1-052_validate_rolebinding_number
    logger.go:42: 19:12:30 | 1-052_validate_rolebinding_number/1-label_namespace | test step completed 1-label_namespace
    logger.go:42: 19:12:30 | 1-052_validate_rolebinding_number/2-check_rolebindings | starting test step 2-check_rolebindings
    logger.go:42: 19:12:30 | 1-052_validate_rolebinding_number/2-check_rolebindings | running command: [sh -c set -eo pipefail
        
        # Expected and Current RoleBindings
        expected_rb=(
          "openshift-gitops-argocd-application-controller"
          "openshift-gitops-argocd-server"
          "openshift-gitops-argocd-grafana"
          "openshift-gitops-argocd-redis"
        )
        current_rb=( $(oc get rolebindings -n "${NAMESPACE}" | awk '/gitops/ {print $1}') )
        
        # Check that the required RoleBindings exist:
        for rb in "${expected_rb[@]}"
        do
          oc get rolebinding "${rb}" -n "${NAMESPACE}" > /dev/null
        done
        
        # Check that there are only two RoleBindings
        echo "Current RoleBindings: ${current_rb[*]}"
        [[ "${#current_rb[@]}" == "4" ]]
        ]
I0607 19:12:32.367537  250633 request.go:655] Throttling request took 1.650070878s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/autoscaling.openshift.io/v1beta1?timeout=32s
    logger.go:42: 19:12:34 | 1-052_validate_rolebinding_number/2-check_rolebindings | Current RoleBindings: openshift-gitops-argocd-application-controller openshift-gitops-argocd-grafana openshift-gitops-argocd-redis openshift-gitops-argocd-server
=== CONT  kuttl/harness/1-025-validate-managed-by-change
    logger.go:42: 19:12:39 | 1-025-validate-managed-by-change/99-delete | test step completed 99-delete
    logger.go:42: 19:12:39 | 1-025-validate-managed-by-change | skipping kubernetes event logging
    logger.go:42: 19:12:39 | 1-025-validate-managed-by-change | Deleting namespace: kuttl-test-precious-gelding
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
I0607 19:12:42.387660  250633 request.go:655] Throttling request took 2.350533105s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-5-0?timeout=32s
=== CONT  kuttl/harness/1-051-validate_csv_permissions
    case.go:361: failed in step 1-validate
    case.go:363: exit status 1
    logger.go:42: 19:12:44 | 1-051-validate_csv_permissions | skipping kubernetes event logging
    logger.go:42: 19:12:44 | 1-051-validate_csv_permissions | Deleting namespace: kuttl-test-smooth-stork
=== CONT  kuttl/harness/1-098_validate_dex_clientsecret_deprecated
=== CONT  kuttl/harness/1-052_validate_rolebinding_number
    logger.go:42: 19:12:48 | 1-052_validate_rolebinding_number/2-check_rolebindings | test step completed 2-check_rolebindings
    logger.go:42: 19:12:48 | 1-052_validate_rolebinding_number | skipping kubernetes event logging
    logger.go:42: 19:12:48 | 1-052_validate_rolebinding_number | Deleting namespace: kuttl-test-inspired-dinosaur
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
    logger.go:42: 19:12:48 | 1-032_validate_resource_inclusions | Creating namespace: kuttl-test-prompt-magpie
=== CONT  kuttl/harness/1-098_validate_dex_clientsecret_deprecated
    logger.go:42: 19:12:48 | 1-098_validate_dex_clientsecret_deprecated | Creating namespace: kuttl-test-profound-quagga
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
    logger.go:42: 19:12:49 | 1-032_validate_resource_inclusions/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-030_validate_reencrypt
=== CONT  kuttl/harness/1-098_validate_dex_clientsecret_deprecated
    logger.go:42: 19:12:49 | 1-098_validate_dex_clientsecret_deprecated/1-install | starting test step 1-install
I0607 19:12:52.421030  250633 request.go:655] Throttling request took 2.748281763s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/tekton.dev/v1?timeout=32s
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
    logger.go:42: 19:12:54 | 1-032_validate_resource_inclusions/1-install | ArgoCD:kuttl-test-prompt-magpie/argocd created
=== CONT  kuttl/harness/1-030_validate_reencrypt
    logger.go:42: 19:12:58 | 1-030_validate_reencrypt | Creating namespace: kuttl-test-vital-reindeer
    logger.go:42: 19:12:58 | 1-030_validate_reencrypt/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
    logger.go:42: 19:12:58 | 1-032_validate_resource_inclusions/1-install | test step completed 1-install
    logger.go:42: 19:12:58 | 1-032_validate_resource_inclusions/2-add_resource_inclusions | starting test step 2-add_resource_inclusions
=== CONT  kuttl/harness/1-098_validate_dex_clientsecret_deprecated
    logger.go:42: 19:12:59 | 1-098_validate_dex_clientsecret_deprecated/1-install | ArgoCD:kuttl-test-profound-quagga/example-argocd created
I0607 19:13:02.468847  250633 request.go:655] Throttling request took 3.248023587s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/whereabouts.cni.cncf.io/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-030_validate_reencrypt
    logger.go:42: 19:13:04 | 1-030_validate_reencrypt/1-install | Namespace:/test-1-30-argo1 created
    logger.go:42: 19:13:04 | 1-030_validate_reencrypt/1-install | ArgoCD:test-1-30-argo1/argocd created
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
    logger.go:42: 19:13:08 | 1-032_validate_resource_inclusions/2-add_resource_inclusions | ArgoCD:kuttl-test-prompt-magpie/argocd updated
=== CONT  kuttl/harness/1-030_validate_reencrypt
    logger.go:42: 19:13:08 | 1-030_validate_reencrypt/1-install | test step completed 1-install
    logger.go:42: 19:13:08 | 1-030_validate_reencrypt/2-wait | starting test step 2-wait
    logger.go:42: 19:13:08 | 1-030_validate_reencrypt/2-wait | running command: [sh -c function wait_until_pods_running() {
          echo -n "Waiting until all pods in namespace $1 are up"
          for i in {1..150}; do  # timeout after 5 minutes
            local pods="$(oc get pods --no-headers -n $1 2>/dev/null)"
            # All pods must be running
            local not_running=$(echo "${pods}" | grep -v Running | grep -v Completed | wc -l)
            if [[ -n "${pods}" && ${not_running} -eq 0 ]]; then
              local all_ready=1
              while read pod ; do
                local status=(`echo -n ${pod} | cut -f2 -d' ' | tr '/' ' '`)
                # All containers must be ready
                [[ -z ${status[0]} ]] && all_ready=0 && break
                [[ -z ${status[1]} ]] && all_ready=0 && break
                [[ ${status[0]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[1]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[0]} -ne ${status[1]} ]] && all_ready=0 && break
              done <<< $(echo "${pods}" | grep -v Completed)
              if (( all_ready )); then
                echo -e "\nAll pods are up:\n${pods}"
                return 0
              fi
            fi
            echo -n "."
            sleep 2
          done
          echo -e "\n\nERROR: timeout waiting for pods to come up\n${pods}"
          return 1
        }
        
        wait_until_pods_running "test-1-30-argo1"
        ]
=== CONT  kuttl/harness/1-032_validate_resource_inclusions
    logger.go:42: 19:13:09 | 1-032_validate_resource_inclusions/2-add_resource_inclusions | test step completed 2-add_resource_inclusions
    logger.go:42: 19:13:09 | 1-032_validate_resource_inclusions | skipping kubernetes event logging
    logger.go:42: 19:13:09 | 1-032_validate_resource_inclusions | Deleting namespace: kuttl-test-prompt-magpie
=== CONT  kuttl/harness/1-090_validate_permissions
    logger.go:42: 19:13:10 | 1-090_validate_permissions | Creating namespace: kuttl-test-warm-magpie
    logger.go:42: 19:13:10 | 1-090_validate_permissions/1- | starting test step 1-
I0607 19:13:12.510655  250633 request.go:655] Throttling request took 1.549653879s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/coordination.k8s.io/v1?timeout=32s
=== CONT  kuttl/harness/1-098_validate_dex_clientsecret_deprecated
    logger.go:42: 19:13:15 | 1-098_validate_dex_clientsecret_deprecated/1-install | test step completed 1-install
    logger.go:42: 19:13:15 | 1-098_validate_dex_clientsecret_deprecated/2-verify-clientsecret | starting test step 2-verify-clientsecret
    logger.go:42: 19:13:15 | 1-098_validate_dex_clientsecret_deprecated/2-verify-clientsecret | running command: [sh -c # This test validates the Dex Client Secret copied by the operator from dex serviceaccount token secret in to argocd-secret.
        # To verify the behavior we should first get the token secret name of the dex service account.
        secret=$(oc get -n $NAMESPACE sa example-argocd-argocd-dex-server -o json | jq -r '.secrets' | grep token | sed 's/    "name": "//g' | sed 's/"//g')
        
        # Extract the clientSecret 
        expectedClientSecret=$(oc get secret $secret -n $NAMESPACE -o json | jq -r '.data.token')
        
        # actualClientSecret is the value of the secret in argocd-secret where argocd-operator should copy the secret from
        actualClientSecret=$(oc get secret argocd-secret -o json -n $NAMESPACE | jq -r '.data."oidc.dex.clientSecret"')
        
        # Verify
        if $expectedClientSecret != $actualClientSecret; then
          echo "Error: Dex Client Secret for OIDC is not valid"
          exit 1
        fi
        ]
    logger.go:42: 19:13:18 | 1-098_validate_dex_clientsecret_deprecated/2-verify-clientsecret | sh: line 11: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklrb3pjMGd3TFc1alZqbHliVzlwYVRaRGNWSm5iVkZNYWtwZmNHTkxSRmxIT0U5Uk5WUkNjMUpYZEhjaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFhSMGJDMTBaWE4wTFhCeWIyWnZkVzVrTFhGMVlXZG5ZU0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVmpjbVYwTG01aGJXVWlPaUpoY21kdlkyUXRaR1Y0TFhObGNuWmxjaTEwYjJ0bGJpMWpaalEyWXlJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExtNWhiV1VpT2lKbGVHRnRjR3hsTFdGeVoyOWpaQzFoY21kdlkyUXRaR1Y0TFhObGNuWmxjaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbUU0Wm1FM1l6a3dMV0kyWVdZdE5HSmtPUzA0WlRoakxUZzNOREZsWkRWak9EUTNOaUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwcmRYUjBiQzEwWlhOMExYQnliMlp2ZFc1a0xYRjFZV2RuWVRwbGVHRnRjR3hsTFdGeVoyOWpaQzFoY21kdlkyUXRaR1Y0TFhObGNuWmxjaUo5LnBsXzFQcjlpeG4zUUR4SW1EZXRtRno1dnMyMzNtaENBMFRkWHhKOWRSUlh6dXlQdnoxQ195QUJKb0VfcEVfaF82eXZCWUVZdjBSV3VGcEFpYVFLUDcwb3RUT0ZqLXlMVW1rLS1lQk9MVUFwQWk1QjZHQ0ZnV0hzbXMzdTEwcDE5SEM4NGhvWGlCa1NZWHRoaTVJQTIycjJmQTJlRGpWUFlacVZJT1Zrc01fZlh5bnowU2FXalYwcVB3a0VkNzJqWFQ3cGNkOXUyQWR6d0dkQTZWNDhqUFpnaHFDTk5mNHV5aXlpNWp1QjJmQ3RvVkt0N3hPSm5LRTZDWXBPcVJpbGd2WEZlNlI3MWpzbEMxME9YOWtvQi12QllKN3ZBTEJBdkhOaE9sdFJjR2F5d1ZCRE5PTmdBejBZNWE2TklxNDUzbUpLZ1hyLTNQRGZwb293OGxXOXRyZ2U3Slo2Vm9YaFg1a0NSaEMtcXRkYTdadk5HR1Uzdm14LXQ1YWdpNlBEUzNSeUQ3R1BkUjBUQTIwbDlPX1c0NmIyZXFSQmh4d0N6RGFzaGtQMjV1d0NldDdUQzdUdnI4Mk5hU0VlejVNaV9HNllhRHRKdV9wTy00MlNnZXRXYUlwUVRCRHZVaHZUc2hJWHNzZXRQcHY0TlZ1UmNEX3ZKSEFvQ01jV05XV2FJYjU5SkJOQUhZb3ZXaXVydFNZbFlZbnRlOHczVDNiUVpxdklJQkJldVdHN0NHZGJObnBRYnkxbnRXc2tlYlF2a2hPSnc1UVpQcGFlZ0RZWmVqQmRkdnRIVHQzWEVLSUUtLVdTaGpvZHEtS0xQM2NGSTRaM3FBM3MtMHlIVVFWSjhfcmllYU82VGo4QVo5MEVYNmllc2FhaDdLYWtOdTU4ZFI0bTBTSk0wQjBZ: command not found
    logger.go:42: 19:13:22 | 1-098_validate_dex_clientsecret_deprecated/2-verify-clientsecret | test step completed 2-verify-clientsecret
    logger.go:42: 19:13:22 | 1-098_validate_dex_clientsecret_deprecated | skipping kubernetes event logging
    logger.go:42: 19:13:22 | 1-098_validate_dex_clientsecret_deprecated | Deleting namespace: kuttl-test-profound-quagga
=== CONT  kuttl/harness/1-087_validate_repo_server_settings
    logger.go:42: 19:13:22 | 1-087_validate_repo_server_settings | Creating namespace: kuttl-test-glorious-sparrow
    logger.go:42: 19:13:23 | 1-087_validate_repo_server_settings/1-install | starting test step 1-install
I0607 19:13:24.866275  250633 request.go:655] Throttling request took 1.000892508s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/route.openshift.io/v1?timeout=32s
    logger.go:42: 19:13:28 | 1-087_validate_repo_server_settings/1-install | ArgoCD:kuttl-test-glorious-sparrow/example-argocd created
    logger.go:42: 19:13:30 | 1-087_validate_repo_server_settings/1-install | test step completed 1-install
    logger.go:42: 19:13:30 | 1-087_validate_repo_server_settings/2-modify-argocd | starting test step 2-modify-argocd
I0607 19:13:34.869795  250633 request.go:655] Throttling request took 3.899259993s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/resolution.tekton.dev/v1beta1?timeout=32s
=== CONT  kuttl/harness/1-030_validate_reencrypt
    logger.go:42: 19:13:37 | 1-030_validate_reencrypt/2-wait | Waiting until all pods in namespace test-1-30-argo1 are up.........
    logger.go:42: 19:13:37 | 1-030_validate_reencrypt/2-wait | All pods are up:
    logger.go:42: 19:13:37 | 1-030_validate_reencrypt/2-wait | argocd-application-controller-0       1/1   Running   0     32s
    logger.go:42: 19:13:37 | 1-030_validate_reencrypt/2-wait | argocd-redis-78c854d978-gcrg8         1/1   Running   0     32s
    logger.go:42: 19:13:37 | 1-030_validate_reencrypt/2-wait | argocd-repo-server-5bc988d96c-g5255   1/1   Running   0     32s
    logger.go:42: 19:13:37 | 1-030_validate_reencrypt/2-wait | argocd-server-6c59cb875f-bqjcx        1/1   Running   0     32s
    logger.go:42: 19:13:42 | 1-030_validate_reencrypt/2-wait | test step completed 2-wait
    logger.go:42: 19:13:42 | 1-030_validate_reencrypt/3-check-route | starting test step 3-check-route
    logger.go:42: 19:13:42 | 1-030_validate_reencrypt/3-check-route | running command: [sh -c sleep 30s
        routeURL=$(oc -n test-1-30-argo1 get route argocd-server -o jsonpath='{.status.ingress[0].host}')
        if ! curl --silent -k https://${routeURL} | grep -q "Your browser does not support JavaScript."; then
          echo "Route not configured properly"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-087_validate_repo_server_settings
    logger.go:42: 19:14:03 | 1-087_validate_repo_server_settings/2-modify-argocd | ArgoCD:kuttl-test-glorious-sparrow/example-argocd updated
    logger.go:42: 19:14:03 | 1-087_validate_repo_server_settings/2-modify-argocd | test step completed 2-modify-argocd
    logger.go:42: 19:14:03 | 1-087_validate_repo_server_settings/4-modify-serviceaccount | starting test step 4-modify-serviceaccount
I0607 19:14:05.293041  250633 request.go:655] Throttling request took 1.000270373s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/machineconfiguration.openshift.io/v1?timeout=32s
    logger.go:42: 19:14:09 | 1-087_validate_repo_server_settings/4-modify-serviceaccount | ServiceAccount:kuttl-test-glorious-sparrow/modified-default created
    logger.go:42: 19:14:10 | 1-087_validate_repo_server_settings/4-modify-serviceaccount | ArgoCD:kuttl-test-glorious-sparrow/example-argocd updated
    logger.go:42: 19:14:11 | 1-087_validate_repo_server_settings/4-modify-serviceaccount | test step completed 4-modify-serviceaccount
    logger.go:42: 19:14:11 | 1-087_validate_repo_server_settings/5-reset-repo-settings | starting test step 5-reset-repo-settings
I0607 19:14:15.336445  250633 request.go:655] Throttling request took 3.341712915s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/network.operator.openshift.io/v1?timeout=32s
    logger.go:42: 19:14:16 | 1-087_validate_repo_server_settings/5-reset-repo-settings | ArgoCD:kuttl-test-glorious-sparrow/example-argocd updated
=== CONT  kuttl/harness/1-030_validate_reencrypt
    logger.go:42: 19:14:20 | 1-030_validate_reencrypt/3-check-route | test step completed 3-check-route
    logger.go:42: 19:14:20 | 1-030_validate_reencrypt | skipping kubernetes event logging
    logger.go:42: 19:14:20 | 1-030_validate_reencrypt | Deleting namespace: kuttl-test-vital-reindeer
=== CONT  kuttl/harness/1-029_validate_tls_secret_no_scale
    logger.go:42: 19:14:21 | 1-029_validate_tls_secret_no_scale | Creating namespace: kuttl-test-hot-owl
=== CONT  kuttl/harness/1-087_validate_repo_server_settings
    logger.go:42: 19:14:21 | 1-087_validate_repo_server_settings/5-reset-repo-settings | test step completed 5-reset-repo-settings
    logger.go:42: 19:14:21 | 1-087_validate_repo_server_settings | skipping kubernetes event logging
=== CONT  kuttl/harness/1-029_validate_tls_secret_no_scale
    logger.go:42: 19:14:21 | 1-029_validate_tls_secret_no_scale/1-install | starting test step 1-install
I0607 19:14:25.380720  250633 request.go:655] Throttling request took 2.999254172s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-8-0?timeout=32s
    logger.go:42: 19:14:27 | 1-029_validate_tls_secret_no_scale/1-install | ArgoCD:kuttl-test-hot-owl/argocd created
=== CONT  kuttl/harness/1-087_validate_repo_server_settings
    logger.go:42: 19:14:27 | 1-087_validate_repo_server_settings | Deleting namespace: kuttl-test-glorious-sparrow
=== CONT  kuttl/harness/1-084_validate_status_host_ingress
    logger.go:42: 19:14:27 | 1-084_validate_status_host_ingress | Creating namespace: kuttl-test-rich-parakeet
    logger.go:42: 19:14:28 | 1-084_validate_status_host_ingress/1-argocd-ingress | starting test step 1-argocd-ingress
=== CONT  kuttl/harness/1-029_validate_tls_secret_no_scale
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/1-install | test step completed 1-install
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | starting test step 2-create-tls-secret
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | running command: [sh -c openssl req -x509 -newkey rsa:4096 -keyout /tmp/test-029-key.pem -out /tmp/test-029-cert.pem -subj '/CN=test-029-cert' -days 365 -nodes
        cert=$(cat /tmp/test-029-cert.pem | base64 -w 0)
        key=$(cat /tmp/test-029-key.pem | base64 -w 0)
        # Dirty hack to replace argocd-tls
        cat <<_EOF_ | oc replace -n $NAMESPACE secret argocd-tls -f -
        apiVersion: v1
        kind: Secret
        type: kubernetes.io/tls
        metadata:
          name: argocd-tls
          namespace: $NAMESPACE
        data:
          tls.key: $key
          tls.crt: $cert
        _EOF_
        ]
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | Generating a RSA private key
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | .......................++++
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | ..............................++++
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | writing new private key to '/tmp/test-029-key.pem'
    logger.go:42: 19:14:33 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | -----
=== CONT  kuttl/harness/1-084_validate_status_host_ingress
    logger.go:42: 19:14:33 | 1-084_validate_status_host_ingress/1-argocd-ingress | ArgoCD:kuttl-test-rich-parakeet/example-argocd created
=== CONT  kuttl/harness/1-029_validate_tls_secret_no_scale
    logger.go:42: 19:14:34 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | secret/argocd-tls replaced
I0607 19:14:36.032819  250633 request.go:655] Throttling request took 1.049715853s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/apps.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-084_validate_status_host_ingress
    logger.go:42: 19:14:39 | 1-084_validate_status_host_ingress/1-argocd-ingress | test step completed 1-argocd-ingress
    logger.go:42: 19:14:39 | 1-084_validate_status_host_ingress/2- | starting test step 2-
    logger.go:42: 19:14:44 | 1-084_validate_status_host_ingress/2- | test step completed 2-
    logger.go:42: 19:14:44 | 1-084_validate_status_host_ingress | skipping kubernetes event logging
    logger.go:42: 19:14:44 | 1-084_validate_status_host_ingress | Deleting namespace: kuttl-test-rich-parakeet
=== CONT  kuttl/harness/1-012_validate-managed-by-chain
    logger.go:42: 19:14:45 | 1-012_validate-managed-by-chain | Creating namespace: kuttl-test-evolved-chicken
    logger.go:42: 19:14:45 | 1-012_validate-managed-by-chain/1-install | starting test step 1-install
I0607 19:14:47.016945  250633 request.go:655] Throttling request took 1.000128427s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/security.internal.openshift.io/v1?timeout=32s
    logger.go:42: 19:14:50 | 1-012_validate-managed-by-chain/1-install | Namespace:/test-1-12-custom created
    logger.go:42: 19:14:51 | 1-012_validate-managed-by-chain/1-install | Namespace:/test-1-12-custom2 created
    logger.go:42: 19:14:52 | 1-012_validate-managed-by-chain/1-install | ArgoCD:kuttl-test-evolved-chicken/argocd created
    logger.go:42: 19:14:54 | 1-012_validate-managed-by-chain/1-install | test step completed 1-install
    logger.go:42: 19:14:54 | 1-012_validate-managed-by-chain/2-label-namespace | starting test step 2-label-namespace
    logger.go:42: 19:14:54 | 1-012_validate-managed-by-chain/2-label-namespace | running command: [sh -c kubectl label ns test-1-12-custom argocd.argoproj.io/managed-by=$NAMESPACE --overwrite]
    logger.go:42: 19:14:55 | 1-012_validate-managed-by-chain/2-label-namespace | namespace/test-1-12-custom labeled
I0607 19:14:57.127552  250633 request.go:655] Throttling request took 1.00074646s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/performance.openshift.io/v1alpha1?timeout=32s
    logger.go:42: 19:15:01 | 1-012_validate-managed-by-chain/2-label-namespace | test step completed 2-label-namespace
    logger.go:42: 19:15:01 | 1-012_validate-managed-by-chain/3-check-secret | starting test step 3-check-secret
    logger.go:42: 19:15:01 | 1-012_validate-managed-by-chain/3-check-secret | running command: [sh -c namespaces=$(oc get secret -n $NAMESPACE argocd-default-cluster-config -o jsonpath='{.data.namespaces}' | base64 -d)
        if test "$namespaces" != "$NAMESPACE,test-1-12-custom"; then
          echo "Assertion for cluster secret failed!"
          exit 1
        fi
        exit 0
        ]
=== CONT  kuttl/harness/1-029_validate_tls_secret_no_scale
    logger.go:42: 19:15:01 | 1-029_validate_tls_secret_no_scale/2-create-tls-secret | test step completed 2-create-tls-secret
    logger.go:42: 19:15:01 | 1-029_validate_tls_secret_no_scale/3-sleep-and-recheck | starting test step 3-sleep-and-recheck
    logger.go:42: 19:15:01 | 1-029_validate_tls_secret_no_scale/3-sleep-and-recheck | running command: [sh -c sleep 10]
=== CONT  kuttl/harness/1-012_validate-managed-by-chain
    logger.go:42: 19:15:06 | 1-012_validate-managed-by-chain/3-check-secret | test step completed 3-check-secret
    logger.go:42: 19:15:06 | 1-012_validate-managed-by-chain/4-create-application | starting test step 4-create-application
I0607 19:15:08.133811  250633 request.go:655] Throttling request took 1.049258242s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/rbac.authorization.k8s.io/v1?timeout=32s
    logger.go:42: 19:15:11 | 1-012_validate-managed-by-chain/4-create-application | Application:kuttl-test-evolved-chicken/test-1-12-custom created
=== CONT  kuttl/harness/1-029_validate_tls_secret_no_scale
    logger.go:42: 19:15:17 | 1-029_validate_tls_secret_no_scale/3-sleep-and-recheck | test step completed 3-sleep-and-recheck
    logger.go:42: 19:15:17 | 1-029_validate_tls_secret_no_scale | skipping kubernetes event logging
    logger.go:42: 19:15:17 | 1-029_validate_tls_secret_no_scale | Deleting namespace: kuttl-test-hot-owl
=== CONT  kuttl/harness/1-083_validate_resource_customization_subkeys
    logger.go:42: 19:15:17 | 1-083_validate_resource_customization_subkeys | Creating namespace: kuttl-test-wanted-magpie
    logger.go:42: 19:15:17 | 1-083_validate_resource_customization_subkeys/1-argocd-with-resource-customization-subkeys | starting test step 1-argocd-with-resource-customization-subkeys
I0607 19:15:19.170281  250633 request.go:655] Throttling request took 1.000407607s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-3-0?timeout=32s
=== CONT  kuttl/harness/1-012_validate-managed-by-chain
    logger.go:42: 19:15:22 | 1-012_validate-managed-by-chain/4-create-application | test step completed 4-create-application
    logger.go:42: 19:15:22 | 1-012_validate-managed-by-chain/7-label-namespace | starting test step 7-label-namespace
    logger.go:42: 19:15:22 | 1-012_validate-managed-by-chain/7-label-namespace | running command: [sh -c kubectl label ns test-1-12-custom2 argocd.argoproj.io/managed-by=$NAMESPACE]
Warning: unknown field "spec.resourceActions"
Warning: unknown field "spec.resourceHealthChecks"
Warning: unknown field "spec.resourceIgnoreDifferences"
=== CONT  kuttl/harness/1-083_validate_resource_customization_subkeys
    logger.go:42: 19:15:23 | 1-083_validate_resource_customization_subkeys/1-argocd-with-resource-customization-subkeys | ArgoCD:kuttl-test-wanted-magpie/example-argocd created
=== CONT  kuttl/harness/1-012_validate-managed-by-chain
    logger.go:42: 19:15:23 | 1-012_validate-managed-by-chain/7-label-namespace | namespace/test-1-12-custom2 labeled
    logger.go:42: 19:15:29 | 1-012_validate-managed-by-chain/7-label-namespace | test step completed 7-label-namespace
    logger.go:42: 19:15:29 | 1-012_validate-managed-by-chain/8-create-application | starting test step 8-create-application
I0607 19:15:30.510493  250633 request.go:655] Throttling request took 1.001037108s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/psmdb.percona.com/v1?timeout=32s
    logger.go:42: 19:15:34 | 1-012_validate-managed-by-chain/8-create-application | Application:kuttl-test-evolved-chicken/test-1-12-custom2 created
    logger.go:42: 19:15:38 | 1-012_validate-managed-by-chain/8-create-application | test step completed 8-create-application
    logger.go:42: 19:15:38 | 1-012_validate-managed-by-chain/9-delete-app-and-namespace | starting test step 9-delete-app-and-namespace
I0607 19:15:54.884044  250633 request.go:655] Throttling request took 1.000116613s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/image.openshift.io/v1?timeout=32s
    logger.go:42: 19:15:58 | 1-012_validate-managed-by-chain/9-delete-app-and-namespace | test step completed 9-delete-app-and-namespace
    logger.go:42: 19:15:58 | 1-012_validate-managed-by-chain/10-check-secret | starting test step 10-check-secret
    logger.go:42: 19:15:58 | 1-012_validate-managed-by-chain/10-check-secret | running command: [sh -c should="$NAMESPACE,test-1-12-custom2"
        namespaces=$(oc get secret -n $NAMESPACE argocd-default-cluster-config -o jsonpath='{.data.namespaces}' | base64 -d)
        if test "$namespaces" != "$should"; then
          echo "Assertion for cluster secret failed! '$namespaces' != '$should'"
          # 1.2.0 doesn't reconcile the cluster secret on namespace deletion
          if test GITOPS_TARGET_VERSION = "1.2.0"; then
            exit 0
          else
            exit 1
          fi
        fi
        exit 0
        ]
    logger.go:42: 19:16:04 | 1-012_validate-managed-by-chain/10-check-secret | test step completed 10-check-secret
    logger.go:42: 19:16:04 | 1-012_validate-managed-by-chain/11-recreate-applications | starting test step 11-recreate-applications
I0607 19:16:05.497615  250633 request.go:655] Throttling request took 1.000347622s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/console.openshift.io/v1?timeout=32s
    logger.go:42: 19:16:09 | 1-012_validate-managed-by-chain/11-recreate-applications | Namespace:/test-1-12-custom created
    logger.go:42: 19:16:10 | 1-012_validate-managed-by-chain/11-recreate-applications | Application:kuttl-test-evolved-chicken/test-1-12-custom created
    logger.go:42: 19:16:11 | 1-012_validate-managed-by-chain/11-recreate-applications | Application:kuttl-test-evolved-chicken/test-1-12-custom2 created
    logger.go:42: 19:16:12 | 1-012_validate-managed-by-chain/11-recreate-applications | test step completed 11-recreate-applications
    logger.go:42: 19:16:12 | 1-012_validate-managed-by-chain/99-delete | starting test step 99-delete
I0607 19:16:22.121665  250633 request.go:655] Throttling request took 1.000669724s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/monitoring.coreos.com/v1beta1?timeout=32s
    logger.go:42: 19:16:25 | 1-012_validate-managed-by-chain/99-delete | test step completed 99-delete
    logger.go:42: 19:16:25 | 1-012_validate-managed-by-chain | skipping kubernetes event logging
    logger.go:42: 19:16:25 | 1-012_validate-managed-by-chain | Deleting namespace: kuttl-test-evolved-chicken
=== CONT  kuttl/harness/1-008_validate-custom-argocd-namespace
    logger.go:42: 19:16:25 | 1-008_validate-custom-argocd-namespace | Creating namespace: kuttl-test-hot-bear
    logger.go:42: 19:16:25 | 1-008_validate-custom-argocd-namespace/1-install | starting test step 1-install
    logger.go:42: 19:16:31 | 1-008_validate-custom-argocd-namespace/1-install | Namespace:/test-1-8-custom created
    logger.go:42: 19:16:31 | 1-008_validate-custom-argocd-namespace/1-install | ArgoCD:test-1-8-custom/argocd created
    logger.go:42: 19:16:32 | 1-008_validate-custom-argocd-namespace/1-install | test step completed 1-install
    logger.go:42: 19:16:32 | 1-008_validate-custom-argocd-namespace/2-wait | starting test step 2-wait
    logger.go:42: 19:16:32 | 1-008_validate-custom-argocd-namespace/2-wait | running command: [sh -c function wait_until_pods_running() {
          echo -n "Waiting until all pods in namespace $1 are up"
          for i in {1..150}; do  # timeout after 5 minutes
            local pods="$(oc get pods --no-headers -n $1 2>/dev/null)"
            # All pods must be running
            local not_running=$(echo "${pods}" | grep -v Running | grep -v Completed | wc -l)
            if [[ -n "${pods}" && ${not_running} -eq 0 ]]; then
              local all_ready=1
              while read pod ; do
                local status=(`echo -n ${pod} | cut -f2 -d' ' | tr '/' ' '`)
                # All containers must be ready
                [[ -z ${status[0]} ]] && all_ready=0 && break
                [[ -z ${status[1]} ]] && all_ready=0 && break
                [[ ${status[0]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[1]} -lt 1 ]] && all_ready=0 && break
                [[ ${status[0]} -ne ${status[1]} ]] && all_ready=0 && break
              done <<< $(echo "${pods}" | grep -v Completed)
              if (( all_ready )); then
                echo -e "\nAll pods are up:\n${pods}"
                return 0
              fi
            fi
            echo -n "."
            sleep 2
          done
          echo -e "\n\nERROR: timeout waiting for pods to come up\n${pods}"
          return 1
        }
        
        wait_until_pods_running "test-1-8-custom"
        ]
    logger.go:42: 19:17:05 | 1-008_validate-custom-argocd-namespace/2-wait | Waiting until all pods in namespace test-1-8-custom are up..........
    logger.go:42: 19:17:05 | 1-008_validate-custom-argocd-namespace/2-wait | All pods are up:
    logger.go:42: 19:17:05 | 1-008_validate-custom-argocd-namespace/2-wait | argocd-application-controller-0       1/1   Running   0     33s
    logger.go:42: 19:17:05 | 1-008_validate-custom-argocd-namespace/2-wait | argocd-redis-78c854d978-q44gr         1/1   Running   0     33s
    logger.go:42: 19:17:05 | 1-008_validate-custom-argocd-namespace/2-wait | argocd-repo-server-84d94d4777-4q9hn   1/1   Running   0     33s
    logger.go:42: 19:17:05 | 1-008_validate-custom-argocd-namespace/2-wait | argocd-server-67974fc8bf-fgjbd        1/1   Running   0     33s
I0607 19:17:07.575835  250633 request.go:655] Throttling request took 1.050708503s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/admissionregistration.k8s.io/v1?timeout=32s
    logger.go:42: 19:17:10 | 1-008_validate-custom-argocd-namespace/2-wait | test step completed 2-wait
    logger.go:42: 19:17:10 | 1-008_validate-custom-argocd-namespace/3-create-app | starting test step 3-create-app
    logger.go:42: 19:17:16 | 1-008_validate-custom-argocd-namespace/3-create-app | Application:test-1-8-custom/validate-custom-argocd created
    logger.go:42: 19:17:23 | 1-008_validate-custom-argocd-namespace/3-create-app | test step completed 3-create-app
    logger.go:42: 19:17:23 | 1-008_validate-custom-argocd-namespace/99-delete | starting test step 99-delete
I0607 19:17:40.008518  250633 request.go:655] Throttling request took 1.000612137s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/project.openshift.io/v1?timeout=32s
    logger.go:42: 19:17:43 | 1-008_validate-custom-argocd-namespace/99-delete | test step completed 99-delete
    logger.go:42: 19:17:43 | 1-008_validate-custom-argocd-namespace | skipping kubernetes event logging
    logger.go:42: 19:17:43 | 1-008_validate-custom-argocd-namespace | Deleting namespace: kuttl-test-hot-bear
=== CONT  kuttl/harness/1-083_validate_kustomize_namereference
    logger.go:42: 19:17:43 | 1-083_validate_kustomize_namereference | Creating namespace: kuttl-test-grand-oarfish
    logger.go:42: 19:17:43 | 1-083_validate_kustomize_namereference/1-install | starting test step 1-install
    logger.go:42: 19:17:49 | 1-083_validate_kustomize_namereference/1-install | Namespace:/namespace-gitops-2038 created
    logger.go:42: 19:17:49 | 1-083_validate_kustomize_namereference/1-install | ArgoCD:namespace-gitops-2038/argocd created
    logger.go:42: 19:17:49 | 1-083_validate_kustomize_namereference/1-install | test step completed 1-install
    logger.go:42: 19:17:49 | 1-083_validate_kustomize_namereference/2-install-kustomize-app | starting test step 2-install-kustomize-app
I0607 19:17:51.566643  250633 request.go:655] Throttling request took 1.047983252s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/api/v1?timeout=32s
    logger.go:42: 19:17:55 | 1-083_validate_kustomize_namereference/2-install-kustomize-app | Application:namespace-gitops-2038/app-kustomize created
    logger.go:42: 19:18:04 | 1-083_validate_kustomize_namereference/2-install-kustomize-app | test step completed 2-install-kustomize-app
    logger.go:42: 19:18:04 | 1-083_validate_kustomize_namereference/99-delete | starting test step 99-delete
I0607 19:18:19.779151  250633 request.go:655] Throttling request took 1.00014946s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pipelinesascode.tekton.dev/v1alpha1?timeout=32s
    logger.go:42: 19:18:22 | 1-083_validate_kustomize_namereference/99-delete | test step completed 99-delete
    logger.go:42: 19:18:22 | 1-083_validate_kustomize_namereference | skipping kubernetes event logging
    logger.go:42: 19:18:22 | 1-083_validate_kustomize_namereference | Deleting namespace: kuttl-test-grand-oarfish
=== CONT  kuttl/harness/1-009_validate-manage-other-namespace
    logger.go:42: 19:18:23 | 1-009_validate-manage-other-namespace | Ignoring README.md as it does not match file name regexp: ^(\d+)-(?:[^\.]+)(?:\.yaml)?$
    logger.go:42: 19:18:23 | 1-009_validate-manage-other-namespace | Ignoring errors.yaml as it does not match file name regexp: ^(\d+)-(?:[^\.]+)(?:\.yaml)?$
    logger.go:42: 19:18:23 | 1-009_validate-manage-other-namespace | Creating namespace: kuttl-test-curious-osprey
    logger.go:42: 19:18:23 | 1-009_validate-manage-other-namespace/1-install | starting test step 1-install
    logger.go:42: 19:18:29 | 1-009_validate-manage-other-namespace/1-install | Namespace:/test-1-9-custom created
    logger.go:42: 19:18:29 | 1-009_validate-manage-other-namespace/1-install | ArgoCD:kuttl-test-curious-osprey/argocd created
    logger.go:42: 19:18:31 | 1-009_validate-manage-other-namespace/1-install | test step completed 1-install
    logger.go:42: 19:18:31 | 1-009_validate-manage-other-namespace/2-label-namespace | starting test step 2-label-namespace
    logger.go:42: 19:18:31 | 1-009_validate-manage-other-namespace/2-label-namespace | running command: [sh -c kubectl label ns test-1-9-custom argocd.argoproj.io/managed-by=$NAMESPACE --overwrite]
    logger.go:42: 19:18:32 | 1-009_validate-manage-other-namespace/2-label-namespace | namespace/test-1-9-custom labeled
I0607 19:18:34.091528  250633 request.go:655] Throttling request took 1.050120455s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/policy/v1?timeout=32s
    logger.go:42: 19:18:38 | 1-009_validate-manage-other-namespace/2-label-namespace | test step completed 2-label-namespace
    logger.go:42: 19:18:38 | 1-009_validate-manage-other-namespace/3-check-secret | starting test step 3-check-secret
    logger.go:42: 19:18:38 | 1-009_validate-manage-other-namespace/3-check-secret | running command: [sh -c namespaces=$(oc get secret -n $NAMESPACE argocd-default-cluster-config -o jsonpath='{.data.namespaces}' | base64 -d)
        if test "$namespaces" != "$NAMESPACE,test-1-9-custom"; then
          echo "Assertion for cluster secret failed!"
          exit 1
        fi
        exit 0
        ]
    logger.go:42: 19:18:43 | 1-009_validate-manage-other-namespace/3-check-secret | test step completed 3-check-secret
    logger.go:42: 19:18:43 | 1-009_validate-manage-other-namespace/4-create-application | starting test step 4-create-application
I0607 19:18:45.020983  250633 request.go:655] Throttling request took 1.000548832s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/user.openshift.io/v1?timeout=32s
    logger.go:42: 19:18:49 | 1-009_validate-manage-other-namespace/4-create-application | Application:kuttl-test-curious-osprey/test-1-9-custom created
    logger.go:42: 19:18:58 | 1-009_validate-manage-other-namespace/4-create-application | test step completed 4-create-application
    logger.go:42: 19:18:58 | 1-009_validate-manage-other-namespace/5-unlabel-namespace | starting test step 5-unlabel-namespace
    logger.go:42: 19:18:58 | 1-009_validate-manage-other-namespace/5-unlabel-namespace | running command: [sh -c kubectl label ns test-1-9-custom argocd.argoproj.io/managed-by-]
    logger.go:42: 19:18:59 | 1-009_validate-manage-other-namespace/5-unlabel-namespace | namespace/test-1-9-custom unlabeled
    logger.go:42: 19:18:59 | 1-009_validate-manage-other-namespace/5-unlabel-namespace | running command: [sh -c sleep 5]
I0607 19:19:06.535707  250633 request.go:655] Throttling request took 1.049983333s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/oauth.openshift.io/v1?timeout=32s
    logger.go:42: 19:19:09 | 1-009_validate-manage-other-namespace/5-unlabel-namespace | test step completed 5-unlabel-namespace
    logger.go:42: 19:19:09 | 1-009_validate-manage-other-namespace/6-check-secret | starting test step 6-check-secret
    logger.go:42: 19:19:09 | 1-009_validate-manage-other-namespace/6-check-secret | running command: [sh -c namespaces=$(oc get secret -n $NAMESPACE argocd-default-cluster-config -o jsonpath='{.data.namespaces}' | base64 -d)
        if test "$namespaces" != "$NAMESPACE"; then
          echo "Assertion for cluster secret failed!"
          exit 1
        fi
        exit 0
        ]
    logger.go:42: 19:19:15 | 1-009_validate-manage-other-namespace/6-check-secret | test step completed 6-check-secret
    logger.go:42: 19:19:15 | 1-009_validate-manage-other-namespace/7-check | starting test step 7-check
I0607 19:19:16.809635  250633 request.go:655] Throttling request took 1.04714646s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/samples.operator.openshift.io/v1?timeout=32s
    logger.go:42: 19:19:20 | 1-009_validate-manage-other-namespace/7-check | test step completed 7-check
    logger.go:42: 19:19:20 | 1-009_validate-manage-other-namespace/99-delete | starting test step 99-delete
I0607 19:19:35.272491  250633 request.go:655] Throttling request took 1.000537826s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/apps.openshift.io/v1?timeout=32s
    logger.go:42: 19:19:38 | 1-009_validate-manage-other-namespace/99-delete | test step completed 99-delete
    logger.go:42: 19:19:38 | 1-009_validate-manage-other-namespace | skipping kubernetes event logging
    logger.go:42: 19:19:38 | 1-009_validate-manage-other-namespace | Deleting namespace: kuttl-test-curious-osprey
=== CONT  kuttl/harness/1-023_validate_repo_server_tls
    logger.go:42: 19:19:38 | 1-023_validate_repo_server_tls | Creating namespace: kuttl-test-flowing-grub
    logger.go:42: 19:19:38 | 1-023_validate_repo_server_tls/1-install | starting test step 1-install
    logger.go:42: 19:19:44 | 1-023_validate_repo_server_tls/1-install | Namespace:/test-1-23-custom created
    logger.go:42: 19:19:45 | 1-023_validate_repo_server_tls/1-install | ArgoCD:test-1-23-custom/argocd created
    logger.go:42: 19:19:49 | 1-023_validate_repo_server_tls/1-install | test step completed 1-install
    logger.go:42: 19:19:49 | 1-023_validate_repo_server_tls/2-change-argocd | starting test step 2-change-argocd
I0607 19:19:50.866337  250633 request.go:655] Throttling request took 1.048957305s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/migration.k8s.io/v1alpha1?timeout=32s
    logger.go:42: 19:20:18 | 1-023_validate_repo_server_tls/2-change-argocd | ArgoCD:test-1-23-custom/argocd updated
    logger.go:42: 19:20:18 | 1-023_validate_repo_server_tls/2-change-argocd | test step completed 2-change-argocd
    logger.go:42: 19:20:18 | 1-023_validate_repo_server_tls/3-check-workloads | starting test step 3-check-workloads
    logger.go:42: 19:20:18 | 1-023_validate_repo_server_tls/3-check-workloads | running command: [sh -c sleep 5]
    logger.go:42: 19:20:23 | 1-023_validate_repo_server_tls/3-check-workloads | running command: [sh -c oc -n test-1-23-custom get deployment argocd-server \
            -o jsonpath='{.spec.template.spec.containers[0].command}' \
            | jq -r | grep -- '--repo-server-strict-tls'
        ]
    logger.go:42: 19:20:24 | 1-023_validate_repo_server_tls/3-check-workloads |   "--repo-server-strict-tls",
I0607 19:20:26.386199  250633 request.go:655] Throttling request took 1.049534115s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pipelinesascode.tekton.dev/v1alpha1?timeout=32s
    logger.go:42: 19:20:29 | 1-023_validate_repo_server_tls/3-check-workloads | test step completed 3-check-workloads
    logger.go:42: 19:20:29 | 1-023_validate_repo_server_tls/4-install-app | starting test step 4-install-app
    logger.go:42: 19:20:35 | 1-023_validate_repo_server_tls/4-install-app | Application:test-1-23-custom/guestbook created
    logger.go:42: 19:20:43 | 1-023_validate_repo_server_tls/4-install-app | test step completed 4-install-app
    logger.go:42: 19:20:43 | 1-023_validate_repo_server_tls/99-delete | starting test step 99-delete
I0607 19:20:59.527704  250633 request.go:655] Throttling request took 1.000191553s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/performance.openshift.io/v1alpha1?timeout=32s
    logger.go:42: 19:21:02 | 1-023_validate_repo_server_tls/99-delete | test step completed 99-delete
    logger.go:42: 19:21:02 | 1-023_validate_repo_server_tls | skipping kubernetes event logging
    logger.go:42: 19:21:02 | 1-023_validate_repo_server_tls | Deleting namespace: kuttl-test-flowing-grub
=== CONT  kuttl/harness/1-021_validate_rolebindings
    logger.go:42: 19:21:02 | 1-021_validate_rolebindings | Creating namespace: kuttl-test-adequate-deer
    logger.go:42: 19:21:03 | 1-021_validate_rolebindings/1-install | starting test step 1-install
    logger.go:42: 19:21:08 | 1-021_validate_rolebindings/1-install | ArgoCD:kuttl-test-adequate-deer/argocd created
    logger.go:42: 19:21:10 | 1-021_validate_rolebindings/1-install | test step completed 1-install
    logger.go:42: 19:21:10 | 1-021_validate_rolebindings | skipping kubernetes event logging
    logger.go:42: 19:21:11 | 1-021_validate_rolebindings | Deleting namespace: kuttl-test-adequate-deer
=== CONT  kuttl/harness/1-007_validate_namespace_scoped_install
    logger.go:42: 19:21:11 | 1-007_validate_namespace_scoped_install | Creating namespace: kuttl-test-immense-kite
    logger.go:42: 19:21:11 | 1-007_validate_namespace_scoped_install/1-install | starting test step 1-install
I0607 19:21:13.256373  250633 request.go:655] Throttling request took 1.049716497s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1?timeout=32s
    logger.go:42: 19:21:17 | 1-007_validate_namespace_scoped_install/1-install | ArgoCD:kuttl-test-immense-kite/argocd created
    logger.go:42: 19:21:53 | 1-007_validate_namespace_scoped_install/1-install | test step completed 1-install
    logger.go:42: 19:21:53 | 1-007_validate_namespace_scoped_install/2-check-cluster-secret | starting test step 2-check-cluster-secret
    logger.go:42: 19:21:53 | 1-007_validate_namespace_scoped_install/2-check-cluster-secret | running command: [sh -c namespaces=$(oc get secret -n $NAMESPACE argocd-default-cluster-config -o jsonpath='{.data.namespaces}' | base64 -d)
        if test "$namespaces" != "$NAMESPACE"; then
          echo "Assertion for cluster secret failed!"
          exit 1
        fi
        exit 0
        ]
I0607 19:21:58.009943  250633 request.go:655] Throttling request took 1.049441821s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/imageregistry.operator.openshift.io/v1?timeout=32s
    logger.go:42: 19:22:01 | 1-007_validate_namespace_scoped_install/2-check-cluster-secret | test step completed 2-check-cluster-secret
    logger.go:42: 19:22:01 | 1-007_validate_namespace_scoped_install | skipping kubernetes event logging
    logger.go:42: 19:22:01 | 1-007_validate_namespace_scoped_install | Deleting namespace: kuttl-test-immense-kite
=== CONT  kuttl/harness/1-082_validate_node_placement
    logger.go:42: 19:22:01 | 1-082_validate_node_placement | Creating namespace: kuttl-test-subtle-bug
    logger.go:42: 19:22:01 | 1-082_validate_node_placement/1-default-node-selector | starting test step 1-default-node-selector
    logger.go:42: 19:22:07 | 1-082_validate_node_placement/1-default-node-selector | ArgoCD:kuttl-test-subtle-bug/example-argocd created
    logger.go:42: 19:22:11 | 1-082_validate_node_placement/1-default-node-selector | test step completed 1-default-node-selector
    logger.go:42: 19:22:11 | 1-082_validate_node_placement/2-custom-node-placement | starting test step 2-custom-node-placement
I0607 19:22:13.407799  250633 request.go:655] Throttling request took 1.048515674s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/psmdb.percona.com/v1?timeout=32s
    logger.go:42: 19:22:19 | 1-082_validate_node_placement/2-custom-node-placement | ArgoCD:kuttl-test-subtle-bug/example-argocd updated
    logger.go:42: 19:22:22 | 1-082_validate_node_placement/2-custom-node-placement | test step completed 2-custom-node-placement
    logger.go:42: 19:22:22 | 1-082_validate_node_placement/3-node-placement-with-toleration | starting test step 3-node-placement-with-toleration
I0607 19:22:23.700211  250633 request.go:655] Throttling request took 1.050200403s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-4-0?timeout=32s
    logger.go:42: 19:23:20 | 1-082_validate_node_placement/3-node-placement-with-toleration | ArgoCD:kuttl-test-subtle-bug/example-argocd updated
    logger.go:42: 19:23:22 | 1-082_validate_node_placement/3-node-placement-with-toleration | test step completed 3-node-placement-with-toleration
    logger.go:42: 19:23:22 | 1-082_validate_node_placement | skipping kubernetes event logging
    logger.go:42: 19:23:23 | 1-082_validate_node_placement | Deleting namespace: kuttl-test-subtle-bug
=== CONT  kuttl/harness/1-081_validate_applicationset_deployment
    logger.go:42: 19:23:23 | 1-081_validate_applicationset_deployment | Creating namespace: kuttl-test-large-macaque
    logger.go:42: 19:23:23 | 1-081_validate_applicationset_deployment/1- | starting test step 1-
I0607 19:23:25.481998  250633 request.go:655] Throttling request took 1.049266357s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/rabbitmq.com/v1beta1?timeout=32s
    logger.go:42: 19:23:29 | 1-081_validate_applicationset_deployment/1- | test step completed 1-
    logger.go:42: 19:23:29 | 1-081_validate_applicationset_deployment | skipping kubernetes event logging
    logger.go:42: 19:23:29 | 1-081_validate_applicationset_deployment | Deleting namespace: kuttl-test-large-macaque
=== CONT  kuttl/harness/1-019_validate_volume_mounts
    logger.go:42: 19:23:29 | 1-019_validate_volume_mounts | Creating namespace: kuttl-test-on-quail
    logger.go:42: 19:23:30 | 1-019_validate_volume_mounts/1-install | starting test step 1-install
    logger.go:42: 19:23:35 | 1-019_validate_volume_mounts/1-install | ArgoCD:kuttl-test-on-quail/argocd created
    logger.go:42: 19:23:37 | 1-019_validate_volume_mounts/1-install | test step completed 1-install
    logger.go:42: 19:23:37 | 1-019_validate_volume_mounts | skipping kubernetes event logging
    logger.go:42: 19:23:37 | 1-019_validate_volume_mounts | Deleting namespace: kuttl-test-on-quail
=== CONT  kuttl/harness/1-080_validate_regex_support_argocd_rbac
    logger.go:42: 19:23:38 | 1-080_validate_regex_support_argocd_rbac | Creating namespace: kuttl-test-perfect-beagle
    logger.go:42: 19:23:38 | 1-080_validate_regex_support_argocd_rbac/1-install | starting test step 1-install
I0607 19:23:39.970263  250633 request.go:655] Throttling request took 1.050113967s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/project.openshift.io/v1?timeout=32s
    logger.go:42: 19:23:43 | 1-080_validate_regex_support_argocd_rbac/1-install | ArgoCD:kuttl-test-perfect-beagle/example-argocd created
    logger.go:42: 19:24:16 | 1-080_validate_regex_support_argocd_rbac/1-install | test step completed 1-install
    logger.go:42: 19:24:16 | 1-080_validate_regex_support_argocd_rbac/2-modify-argocd | starting test step 2-modify-argocd
I0607 19:24:18.746738  250633 request.go:655] Throttling request took 1.049215855s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/security.internal.openshift.io/v1?timeout=32s
Warning: unknown field "spec.rbac.policyMatcherMode"
    logger.go:42: 19:24:22 | 1-080_validate_regex_support_argocd_rbac/2-modify-argocd | ArgoCD:kuttl-test-perfect-beagle/example-argocd updated
=== CONT  kuttl/harness/1-073_validate_rhsso
    logger.go:42: 19:32:28 | 1-073_validate_rhsso/1-argocd-rhsso | test step failed 1-argocd-rhsso
    case.go:361: failed in step 1-argocd-rhsso
    case.go:363: --- DeploymentConfig:kuttl-test-magical-phoenix/keycloak
        +++ DeploymentConfig:kuttl-test-magical-phoenix/keycloak
        @@ -1,12 +1,251 @@
         apiVersion: apps.openshift.io/v1
         kind: DeploymentConfig
         metadata:
        +  annotations:
        +    argocd.argoproj.io/realm-created: "false"
        +  labels:
        +    application: keycloak
        +    template.openshift.io/template-instance-owner: feeffe86-5b2a-4b5c-9aa0-ef2b0c631c80
        +  managedFields:
        +  - apiVersion: apps.openshift.io/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:annotations:
        +          .: {}
        +          f:argocd.argoproj.io/realm-created: {}
        +        f:labels:
        +          .: {}
        +          f:application: {}
        +          f:template.openshift.io/template-instance-owner: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"7563f6f9-e443-4c4e-b12f-26f41898d981"}: {}
        +      f:spec:
        +        f:replicas: {}
        +        f:selector:
        +          .: {}
        +          f:deploymentConfig: {}
        +        f:strategy:
        +          f:activeDeadlineSeconds: {}
        +          f:recreateParams:
        +            .: {}
        +            f:timeoutSeconds: {}
        +          f:resources:
        +            f:limits:
        +              .: {}
        +              f:cpu: {}
        +              f:memory: {}
        +            f:requests:
        +              .: {}
        +              f:cpu: {}
        +              f:memory: {}
        +          f:type: {}
        +        f:template:
        +          .: {}
        +          f:metadata:
        +            .: {}
        +            f:creationTimestamp: {}
        +            f:labels:
        +              .: {}
        +              f:application: {}
        +              f:deploymentConfig: {}
        +            f:name: {}
        +          f:spec:
        +            .: {}
        +            f:containers:
        +              .: {}
        +              k:{"name":"keycloak"}:
        +                .: {}
        +                f:env:
        +                  .: {}
        +                  k:{"name":"DB_MAX_POOL_SIZE"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"DB_MIN_POOL_SIZE"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"DB_TX_ISOLATION"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"OPENSHIFT_DNS_PING_SERVICE_NAME"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"OPENSHIFT_DNS_PING_SERVICE_PORT"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SSO_ADMIN_PASSWORD"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SSO_ADMIN_USERNAME"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SSO_HOSTNAME"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"SSO_REALM"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"SSO_SERVICE_PASSWORD"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"SSO_SERVICE_USERNAME"}:
        +                    .: {}
        +                    f:name: {}
        +                  k:{"name":"X509_CA_BUNDLE"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:livenessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:name: {}
        +                f:ports:
        +                  .: {}
        +                  k:{"containerPort":8080,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                  k:{"containerPort":8443,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                  k:{"containerPort":8778,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                  k:{"containerPort":8888,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                f:readinessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:resources:
        +                  .: {}
        +                  f:limits:
        +                    .: {}
        +                    f:cpu: {}
        +                    f:memory: {}
        +                  f:requests:
        +                    .: {}
        +                    f:cpu: {}
        +                    f:memory: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/etc/x509/https"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                    f:readOnly: {}
        +                  k:{"mountPath":"/var/run/configmaps/service-ca"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                    f:readOnly: {}
        +            f:dnsPolicy: {}
        +            f:nodeSelector: {}
        +            f:restartPolicy: {}
        +            f:schedulerName: {}
        +            f:securityContext: {}
        +            f:terminationGracePeriodSeconds: {}
        +            f:volumes:
        +              .: {}
        +              k:{"name":"service-ca"}:
        +                .: {}
        +                f:configMap:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:name: {}
        +                f:name: {}
        +              k:{"name":"sso-x509-https-volume"}:
        +                .: {}
        +                f:name: {}
        +                f:secret:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:secretName: {}
        +        f:triggers: {}
        +    manager: openshift-controller-manager
        +    operation: Update
        +    time: "2023-06-07T13:42:26Z"
        +  - apiVersion: apps.openshift.io/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:status:
        +        f:conditions:
        +          .: {}
        +          k:{"type":"Available"}:
        +            .: {}
        +            f:lastTransitionTime: {}
        +            f:lastUpdateTime: {}
        +            f:message: {}
        +            f:status: {}
        +            f:type: {}
        +          k:{"type":"Progressing"}:
        +            .: {}
        +            f:lastTransitionTime: {}
        +            f:lastUpdateTime: {}
        +            f:message: {}
        +            f:reason: {}
        +            f:status: {}
        +            f:type: {}
        +        f:details:
        +          .: {}
        +          f:causes: {}
        +          f:message: {}
        +        f:latestVersion: {}
        +        f:observedGeneration: {}
        +        f:replicas: {}
        +        f:unavailableReplicas: {}
        +        f:updatedReplicas: {}
        +    manager: openshift-controller-manager
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T13:52:33Z"
           name: keycloak
           namespace: kuttl-test-magical-phoenix
        +  ownerReferences:
        +  - apiVersion: argoproj.io/v1alpha1
        +    controller: true
        +    kind: ArgoCD
        +    name: example-argocd-keycloak
        +    uid: 7563f6f9-e443-4c4e-b12f-26f41898d981
         spec:
        +  replicas: 1
        +  revisionHistoryLimit: 10
           selector:
             deploymentConfig: keycloak
           strategy:
        +    activeDeadlineSeconds: 21600
        +    recreateParams:
        +      timeoutSeconds: 600
             resources:
               limits:
                 cpu: 500m
        @@ -17,19 +256,78 @@
             type: Recreate
           template:
             metadata:
        +      creationTimestamp: null
               labels:
                 application: keycloak
                 deploymentConfig: keycloak
               name: keycloak
             spec:
               containers:
        -      - resources:
        +      - env:
        +        - name: SSO_HOSTNAME
        +        - name: DB_MIN_POOL_SIZE
        +        - name: DB_MAX_POOL_SIZE
        +        - name: DB_TX_ISOLATION
        +        - name: OPENSHIFT_DNS_PING_SERVICE_NAME
        +          value: keycloak-ping
        +        - name: OPENSHIFT_DNS_PING_SERVICE_PORT
        +          value: "8888"
        +        - name: X509_CA_BUNDLE
        +          value: /var/run/configmaps/service-ca/service-ca.crt /var/run/secrets/kubernetes.io/serviceaccount/*.crt
        +        - name: SSO_ADMIN_USERNAME
        +          value: nQpPI1c7
        +        - name: SSO_ADMIN_PASSWORD
        +          value: pXghaT3J
        +        - name: SSO_REALM
        +        - name: SSO_SERVICE_USERNAME
        +        - name: SSO_SERVICE_PASSWORD
        +        image: registry.redhat.io/rh-sso-7/sso7-rhel8-operator:7.6-8
        +        imagePullPolicy: Always
        +        livenessProbe:
        +          exec:
        +            command:
        +            - /bin/bash
        +            - -c
        +            - /opt/eap/bin/livenessProbe.sh
        +          failureThreshold: 3
        +          initialDelaySeconds: 60
        +          periodSeconds: 10
        +          successThreshold: 1
        +          timeoutSeconds: 1
        +        name: keycloak
        +        ports:
        +        - containerPort: 8778
        +          name: jolokia
        +          protocol: TCP
        +        - containerPort: 8080
        +          name: http
        +          protocol: TCP
        +        - containerPort: 8443
        +          name: https
        +          protocol: TCP
        +        - containerPort: 8888
        +          name: ping
        +          protocol: TCP
        +        readinessProbe:
        +          exec:
        +            command:
        +            - /bin/bash
        +            - -c
        +            - /opt/eap/bin/readinessProbe.sh
        +          failureThreshold: 20
        +          initialDelaySeconds: 60
        +          periodSeconds: 10
        +          successThreshold: 1
        +          timeoutSeconds: 1
        +        resources:
                   limits:
                     cpu: "1"
                     memory: 1Gi
                   requests:
                     cpu: 500m
                     memory: 512Mi
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
                 volumeMounts:
                 - mountPath: /etc/x509/https
                   name: sso-x509-https-volume
        @@ -37,7 +335,13 @@
                 - mountPath: /var/run/configmaps/service-ca
                   name: service-ca
                   readOnly: true
        +      dnsPolicy: ClusterFirst
        +      nodeSelector:
        +        kubernetes.io/os: linux
               restartPolicy: Always
        +      schedulerName: default-scheduler
        +      securityContext: {}
        +      terminationGracePeriodSeconds: 75
               volumes:
               - name: sso-x509-https-volume
                 secret:
        @@ -47,8 +351,30 @@
                   defaultMode: 420
                   name: keycloak-service-ca
                 name: service-ca
        +  test: false
           triggers:
           - type: ConfigChange
         status:
        -  readyReplicas: 1
        +  availableReplicas: 0
        +  conditions:
        +  - lastTransitionTime: "2023-06-07T13:42:26Z"
        +    lastUpdateTime: "2023-06-07T13:42:26Z"
        +    message: Deployment config does not have minimum availability.
        +    status: "False"
        +    type: Available
        +  - lastTransitionTime: "2023-06-07T13:52:33Z"
        +    lastUpdateTime: "2023-06-07T13:52:33Z"
        +    message: replication controller "keycloak-1" has failed progressing
        +    reason: ProgressDeadlineExceeded
        +    status: "False"
        +    type: Progressing
        +  details:
        +    causes:
        +    - type: ConfigChange
        +    message: config change
        +  latestVersion: 1
        +  observedGeneration: 1
        +  replicas: 0
        +  unavailableReplicas: 0
        +  updatedReplicas: 0
         
        
    case.go:363: resource DeploymentConfig:kuttl-test-magical-phoenix/keycloak: .status.readyReplicas: key is missing from map
    logger.go:42: 19:32:28 | 1-073_validate_rhsso | skipping kubernetes event logging
    logger.go:42: 19:32:29 | 1-073_validate_rhsso | Deleting namespace: kuttl-test-magical-phoenix
=== CONT  kuttl/harness/1-077_validate_disable_dex_removed
    logger.go:42: 19:32:29 | 1-077_validate_disable_dex_removed | Creating namespace: kuttl-test-tidy-piglet
    logger.go:42: 19:32:30 | 1-077_validate_disable_dex_removed/1-check_disable_dex_removed | starting test step 1-check_disable_dex_removed
    logger.go:42: 19:32:30 | 1-077_validate_disable_dex_removed/1-check_disable_dex_removed | running command: [sh -c set -e
        
        if [ -z $CI ]; then 
            operator_name_version=$(oc get subscription/openshift-gitops-operator -n openshift-operators -o jsonpath='{.status.installedCSV}')
            if [ "$(oc get csv/"${operator_name_version}" -n openshift-operators -o jsonpath='{.spec.install.spec.deployments[].spec.template.spec.containers[].env}' | grep DISABLE_DEX)" != '' ]; then
              echo "DISABLE_DEX env var is still present in the operator CSV."
              exit 1
            fi 
        else
            operator_name_version=$(oc get subscription `subscription=gitops-operator- && oc get subscription --all-namespaces | grep $subscription | head -1 | awk '{print $2}'` -n openshift-operators -o jsonpath='{.status.installedCSV}')
            if [ "$(oc get csv/"${operator_name_version}" -n openshift-operators -o jsonpath='{.spec.install.spec.deployments[].spec.template.spec.containers[].env}' | grep DISABLE_DEX)" != '' ]; then
              echo "DISABLE_DEX env var is still present in the operator CSV."
              exit 1
            fi
        fi
        ]
    logger.go:42: 19:32:33 | 1-077_validate_disable_dex_removed/1-check_disable_dex_removed | Error from server (NotFound): subscriptions.operators.coreos.com "openshift-gitops-operator" not found
I0607 19:32:35.157456  250633 request.go:655] Throttling request took 1.04953725s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/network.operator.openshift.io/v1?timeout=32s
    case.go:361: failed in step 1-check_disable_dex_removed
    case.go:363: exit status 1
    logger.go:42: 19:32:38 | 1-077_validate_disable_dex_removed | skipping kubernetes event logging
    logger.go:42: 19:32:38 | 1-077_validate_disable_dex_removed | Deleting namespace: kuttl-test-tidy-piglet
=== CONT  kuttl/harness/1-079_validate_vars_for_notificaitons
    logger.go:42: 19:32:38 | 1-079_validate_vars_for_notificaitons | Creating namespace: kuttl-test-adapting-antelope
    logger.go:42: 19:32:38 | 1-079_validate_vars_for_notificaitons/1-install | starting test step 1-install
    logger.go:42: 19:32:44 | 1-079_validate_vars_for_notificaitons/1-install | ArgoCD:kuttl-test-adapting-antelope/example-argocd created
=== CONT  kuttl/harness/1-036_validate_keycloak_resource_reqs
    logger.go:42: 19:32:44 | 1-036_validate_keycloak_resource_reqs/2-update-sso-keycloak-provider | test step failed 2-update-sso-keycloak-provider
    case.go:361: failed in step 2-update-sso-keycloak-provider
    case.go:363: --- Pod:kuttl-test-tolerant-ant/keycloak-1-deploy
        +++ Pod:kuttl-test-tolerant-ant/keycloak-1-deploy
        @@ -1,12 +1,277 @@
         apiVersion: v1
         kind: Pod
         metadata:
        +  annotations:
        +    k8s.v1.cni.cncf.io/network-status: |-
        +      [{
        +          "name": "openshift-sdn",
        +          "interface": "eth0",
        +          "ips": [
        +              "10.131.1.125"
        +          ],
        +          "default": true,
        +          "dns": {}
        +      }]
        +    k8s.v1.cni.cncf.io/networks-status: |-
        +      [{
        +          "name": "openshift-sdn",
        +          "interface": "eth0",
        +          "ips": [
        +              "10.131.1.125"
        +          ],
        +          "default": true,
        +          "dns": {}
        +      }]
        +    openshift.io/deployment-config.name: keycloak
        +    openshift.io/deployment.name: keycloak-1
        +    openshift.io/scc: restricted-v2
        +    seccomp.security.alpha.kubernetes.io/pod: runtime/default
        +  labels:
        +    openshift.io/deployer-pod-for.name: keycloak-1
        +  managedFields:
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:annotations:
        +          .: {}
        +          f:openshift.io/deployment-config.name: {}
        +          f:openshift.io/deployment.name: {}
        +        f:labels:
        +          .: {}
        +          f:openshift.io/deployer-pod-for.name: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"3a433055-a647-4331-883c-c880fffdefb0"}: {}
        +      f:spec:
        +        f:activeDeadlineSeconds: {}
        +        f:containers:
        +          k:{"name":"deployment"}:
        +            .: {}
        +            f:env:
        +              .: {}
        +              k:{"name":"OPENSHIFT_DEPLOYMENT_NAME"}:
        +                .: {}
        +                f:name: {}
        +                f:value: {}
        +              k:{"name":"OPENSHIFT_DEPLOYMENT_NAMESPACE"}:
        +                .: {}
        +                f:name: {}
        +                f:value: {}
        +            f:image: {}
        +            f:imagePullPolicy: {}
        +            f:name: {}
        +            f:resources:
        +              .: {}
        +              f:limits:
        +                .: {}
        +                f:cpu: {}
        +                f:memory: {}
        +              f:requests:
        +                .: {}
        +                f:cpu: {}
        +                f:memory: {}
        +            f:terminationMessagePath: {}
        +            f:terminationMessagePolicy: {}
        +        f:dnsPolicy: {}
        +        f:enableServiceLinks: {}
        +        f:nodeSelector: {}
        +        f:restartPolicy: {}
        +        f:schedulerName: {}
        +        f:securityContext: {}
        +        f:serviceAccount: {}
        +        f:serviceAccountName: {}
        +        f:shareProcessNamespace: {}
        +        f:terminationGracePeriodSeconds: {}
        +    manager: openshift-controller-manager
        +    operation: Update
        +    time: "2023-06-07T13:42:21Z"
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:annotations:
        +          f:k8s.v1.cni.cncf.io/network-status: {}
        +          f:k8s.v1.cni.cncf.io/networks-status: {}
        +    manager: multus
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T13:42:24Z"
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:status:
        +        f:conditions:
        +          k:{"type":"ContainersReady"}:
        +            .: {}
        +            f:lastProbeTime: {}
        +            f:lastTransitionTime: {}
        +            f:reason: {}
        +            f:status: {}
        +            f:type: {}
        +          k:{"type":"Initialized"}:
        +            .: {}
        +            f:lastProbeTime: {}
        +            f:lastTransitionTime: {}
        +            f:status: {}
        +            f:type: {}
        +          k:{"type":"Ready"}:
        +            .: {}
        +            f:lastProbeTime: {}
        +            f:lastTransitionTime: {}
        +            f:reason: {}
        +            f:status: {}
        +            f:type: {}
        +        f:containerStatuses: {}
        +        f:hostIP: {}
        +        f:phase: {}
        +        f:podIP: {}
        +        f:podIPs:
        +          .: {}
        +          k:{"ip":"10.131.1.125"}:
        +            .: {}
        +            f:ip: {}
        +        f:startTime: {}
        +    manager: kubelet
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T13:52:27Z"
           name: keycloak-1-deploy
           namespace: kuttl-test-tolerant-ant
        +  ownerReferences:
        +  - apiVersion: v1
        +    kind: ReplicationController
        +    name: keycloak-1
        +    uid: 3a433055-a647-4331-883c-c880fffdefb0
        +spec:
        +  activeDeadlineSeconds: 21600
        +  containers:
        +  - env:
        +    - name: OPENSHIFT_DEPLOYMENT_NAME
        +      value: keycloak-1
        +    - name: OPENSHIFT_DEPLOYMENT_NAMESPACE
        +      value: kuttl-test-tolerant-ant
        +    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:363d3a9b8128b9c6079db60b281bb775345997b8d347e057df203d2a14f6986b
        +    imagePullPolicy: IfNotPresent
        +    name: deployment
        +    resources:
        +      limits:
        +        cpu: 500m
        +        memory: 512Mi
        +      requests:
        +        cpu: 250m
        +        memory: 256Mi
        +    securityContext:
        +      allowPrivilegeEscalation: false
        +      capabilities:
        +        drop:
        +        - ALL
        +      runAsNonRoot: true
        +      runAsUser: 1000860000
        +    terminationMessagePath: /dev/termination-log
        +    terminationMessagePolicy: File
        +    volumeMounts:
        +    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        +      name: kube-api-access-hvmqn
        +      readOnly: true
        +  dnsPolicy: ClusterFirst
        +  enableServiceLinks: true
        +  imagePullSecrets:
        +  - name: deployer-dockercfg-psm6l
        +  nodeName: ip-10-0-142-112.us-east-2.compute.internal
        +  nodeSelector:
        +    kubernetes.io/os: linux
        +  preemptionPolicy: PreemptLowerPriority
        +  priority: 0
        +  restartPolicy: Never
        +  schedulerName: default-scheduler
        +  securityContext:
        +    fsGroup: 1000860000
        +    seLinuxOptions:
        +      level: s0:c29,c24
        +    seccompProfile:
        +      type: RuntimeDefault
        +  serviceAccount: deployer
        +  serviceAccountName: deployer
        +  shareProcessNamespace: false
        +  terminationGracePeriodSeconds: 10
        +  tolerations:
        +  - effect: NoExecute
        +    key: node.kubernetes.io/not-ready
        +    operator: Exists
        +    tolerationSeconds: 300
        +  - effect: NoExecute
        +    key: node.kubernetes.io/unreachable
        +    operator: Exists
        +    tolerationSeconds: 300
        +  - effect: NoSchedule
        +    key: node.kubernetes.io/memory-pressure
        +    operator: Exists
        +  volumes:
        +  - name: kube-api-access-hvmqn
        +    projected:
        +      defaultMode: 420
        +      sources:
        +      - serviceAccountToken:
        +          expirationSeconds: 3607
        +          path: token
        +      - configMap:
        +          items:
        +          - key: ca.crt
        +            path: ca.crt
        +          name: kube-root-ca.crt
        +      - downwardAPI:
        +          items:
        +          - fieldRef:
        +              apiVersion: v1
        +              fieldPath: metadata.namespace
        +            path: namespace
        +      - configMap:
        +          items:
        +          - key: service-ca.crt
        +            path: service-ca.crt
        +          name: openshift-service-ca.crt
         status:
        +  conditions:
        +  - lastProbeTime: null
        +    lastTransitionTime: "2023-06-07T13:42:21Z"
        +    status: "True"
        +    type: Initialized
        +  - lastProbeTime: null
        +    lastTransitionTime: "2023-06-07T13:52:25Z"
        +    reason: PodFailed
        +    status: "False"
        +    type: Ready
        +  - lastProbeTime: null
        +    lastTransitionTime: "2023-06-07T13:52:25Z"
        +    reason: PodFailed
        +    status: "False"
        +    type: ContainersReady
        +  - lastProbeTime: null
        +    lastTransitionTime: "2023-06-07T13:42:21Z"
        +    status: "True"
        +    type: PodScheduled
           containerStatuses:
        -  - name: deployment
        +  - containerID: cri-o://0e12e50fe03e4609f44e5793baba66edbbbade6a95e1f3655c1f3b8d9b0df27e
        +    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:363d3a9b8128b9c6079db60b281bb775345997b8d347e057df203d2a14f6986b
        +    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:363d3a9b8128b9c6079db60b281bb775345997b8d347e057df203d2a14f6986b
        +    lastState: {}
        +    name: deployment
        +    ready: false
        +    restartCount: 0
        +    started: false
             state:
               terminated:
        -        reason: Completed
        +        containerID: cri-o://0e12e50fe03e4609f44e5793baba66edbbbade6a95e1f3655c1f3b8d9b0df27e
        +        exitCode: 1
        +        finishedAt: "2023-06-07T13:52:25Z"
        +        reason: Error
        +        startedAt: "2023-06-07T13:42:24Z"
        +  hostIP: 10.0.142.112
        +  phase: Failed
        +  podIP: 10.131.1.125
        +  podIPs:
        +  - ip: 10.131.1.125
        +  qosClass: Burstable
        +  startTime: "2023-06-07T13:42:21Z"
         
        
    case.go:363: resource Pod:kuttl-test-tolerant-ant/keycloak-1-deploy: .status.containerStatuses.state.terminated.reason: value mismatch, expected: Completed != actual: Error
    logger.go:42: 19:32:44 | 1-036_validate_keycloak_resource_reqs | skipping kubernetes event logging
    logger.go:42: 19:32:44 | 1-036_validate_keycloak_resource_reqs | Deleting namespace: kuttl-test-tolerant-ant
=== CONT  kuttl/harness/1-045_validate_repo_exec_timeout
    logger.go:42: 19:32:44 | 1-045_validate_repo_exec_timeout | Creating namespace: kuttl-test-stirring-raven
    logger.go:42: 19:32:45 | 1-045_validate_repo_exec_timeout/1-install | starting test step 1-install
I0607 19:32:46.729695  250633 request.go:655] Throttling request took 1.000105707s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/autoscaling.openshift.io/v1beta1?timeout=32s
    logger.go:42: 19:32:50 | 1-045_validate_repo_exec_timeout/1-install | ArgoCD:kuttl-test-stirring-raven/argocd created
    logger.go:42: 19:32:51 | 1-045_validate_repo_exec_timeout/1-install | test step completed 1-install
    logger.go:42: 19:32:51 | 1-045_validate_repo_exec_timeout/2-change-exec-timeout | starting test step 2-change-exec-timeout
    logger.go:42: 19:32:59 | 1-045_validate_repo_exec_timeout/2-change-exec-timeout | ArgoCD:kuttl-test-stirring-raven/argocd updated
    logger.go:42: 19:32:59 | 1-045_validate_repo_exec_timeout/2-change-exec-timeout | test step completed 2-change-exec-timeout
    logger.go:42: 19:32:59 | 1-045_validate_repo_exec_timeout/4-check-workload-env | starting test step 4-check-workload-env
    logger.go:42: 19:32:59 | 1-045_validate_repo_exec_timeout/4-check-workload-env | running command: [sh -c sleep 10]
    logger.go:42: 19:33:09 | 1-045_validate_repo_exec_timeout/4-check-workload-env | running command: [sh -c timeout=$(oc get -n $NAMESPACE deployment argocd-repo-server -o json \
          | jq -r '.spec.template.spec.containers[0].env[]|select(.name=="ARGOCD_EXEC_TIMEOUT").value')
        if test "$timeout" != "300s"; then
          echo "Assertion failed. Timeout should be 300s, is '$timeout'"
          exit 1
        fi
        ]
    logger.go:42: 19:33:10 | 1-045_validate_repo_exec_timeout/4-check-workload-env | Assertion failed. Timeout should be 300s, is '300'
I0607 19:33:11.950887  250633 request.go:655] Throttling request took 1.04974799s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/performance.openshift.io/v2?timeout=32s
    case.go:361: failed in step 4-check-workload-env
    case.go:363: exit status 1
    logger.go:42: 19:33:15 | 1-045_validate_repo_exec_timeout | skipping kubernetes event logging
    logger.go:42: 19:33:15 | 1-045_validate_repo_exec_timeout | Deleting namespace: kuttl-test-stirring-raven
=== CONT  kuttl/harness/1-047_validate_custom_env
    logger.go:42: 19:33:15 | 1-047_validate_custom_env | Creating namespace: kuttl-test-calm-racer
    logger.go:42: 19:33:15 | 1-047_validate_custom_env/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-090_validate_permissions
    logger.go:42: 19:33:16 | 1-090_validate_permissions/1- | test step failed 1-
    case.go:361: failed in step 1-
    case.go:363: clusterserviceversions.operators.coreos.com "gitops-operator.v1.8.0" not found
    logger.go:42: 19:33:16 | 1-090_validate_permissions | skipping kubernetes event logging
    logger.go:42: 19:33:20 | 1-090_validate_permissions | Deleting namespace: kuttl-test-warm-magpie
=== CONT  kuttl/harness/1-075_validate_dex_anyuid
    logger.go:42: 19:33:20 | 1-075_validate_dex_anyuid | Creating namespace: kuttl-test-proven-crappie
    logger.go:42: 19:33:20 | 1-075_validate_dex_anyuid/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-047_validate_custom_env
    logger.go:42: 19:33:21 | 1-047_validate_custom_env/1-install | ArgoCD:kuttl-test-calm-racer/argocd created
I0607 19:33:22.550130  250633 request.go:655] Throttling request took 1.00077363s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/psmdb.percona.com/v1-11-0?timeout=32s
    logger.go:42: 19:33:26 | 1-047_validate_custom_env/1-install | test step completed 1-install
    logger.go:42: 19:33:26 | 1-047_validate_custom_env/2-change-env-vars | starting test step 2-change-env-vars
=== CONT  kuttl/harness/1-079_validate_vars_for_notificaitons
    logger.go:42: 19:33:26 | 1-079_validate_vars_for_notificaitons/1-install | test step completed 1-install
    logger.go:42: 19:33:26 | 1-079_validate_vars_for_notificaitons/2-modify-argocd | starting test step 2-modify-argocd
=== CONT  kuttl/harness/1-075_validate_dex_anyuid
    logger.go:42: 19:33:26 | 1-075_validate_dex_anyuid/1-install | ArgoCD:kuttl-test-proven-crappie/argocd created
=== CONT  kuttl/harness/1-047_validate_custom_env
    logger.go:42: 19:33:31 | 1-047_validate_custom_env/2-change-env-vars | ArgoCD:kuttl-test-calm-racer/argocd updated
    logger.go:42: 19:33:31 | 1-047_validate_custom_env/2-change-env-vars | test step completed 2-change-env-vars
    logger.go:42: 19:33:31 | 1-047_validate_custom_env/3-check-workloads | starting test step 3-check-workloads
I0607 19:33:32.636978  250633 request.go:655] Throttling request took 1.050304552s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/events.k8s.io/v1?timeout=32s
    logger.go:42: 19:33:35 | 1-047_validate_custom_env/3-check-workloads | running command: [sh -c sleep 10]
Warning: unknown field "spec.notifications.env"
=== CONT  kuttl/harness/1-079_validate_vars_for_notificaitons
    logger.go:42: 19:33:36 | 1-079_validate_vars_for_notificaitons/2-modify-argocd | ArgoCD:kuttl-test-adapting-antelope/example-argocd updated
=== CONT  kuttl/harness/1-075_validate_dex_anyuid
    logger.go:42: 19:33:36 | 1-075_validate_dex_anyuid/1-install | test step completed 1-install
    logger.go:42: 19:33:36 | 1-075_validate_dex_anyuid/2-change_dex_scc_to_anyuid | starting test step 2-change_dex_scc_to_anyuid
    logger.go:42: 19:33:36 | 1-075_validate_dex_anyuid/2-change_dex_scc_to_anyuid | running command: [sh -c set -e
        oc adm policy add-scc-to-user anyuid -z argocd-argocd-dex-server -n $NAMESPACE
        ]
    logger.go:42: 19:33:38 | 1-075_validate_dex_anyuid/2-change_dex_scc_to_anyuid | clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "argocd-argocd-dex-server"
I0607 19:33:42.665454  250633 request.go:655] Throttling request took 3.742399592s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/operator.tekton.dev/v1alpha1?timeout=32s
    logger.go:42: 19:33:43 | 1-075_validate_dex_anyuid/2-change_dex_scc_to_anyuid | test step completed 2-change_dex_scc_to_anyuid
    logger.go:42: 19:33:43 | 1-075_validate_dex_anyuid/3-restart_dex_pod | starting test step 3-restart_dex_pod
    logger.go:42: 19:33:43 | 1-075_validate_dex_anyuid/3-restart_dex_pod | running command: [sh -c set -e
        oc rollout restart deployment/argocd-dex-server -n $NAMESPACE
        ]
    logger.go:42: 19:33:44 | 1-075_validate_dex_anyuid/3-restart_dex_pod | deployment.apps/argocd-dex-server restarted
=== CONT  kuttl/harness/1-047_validate_custom_env
    logger.go:42: 19:33:45 | 1-047_validate_custom_env/3-check-workloads | running command: [sh -c set -e
        set -o pipefail
        for wl in deployment/argocd-server deployment/argocd-repo-server statefulset/argocd-application-controller; do
          val=$(oc get -n $NAMESPACE $wl -o json | jq -r '.spec.template.spec.containers[0].env[]|select(.name=="FOO").value')
          if test "$val" != "bar"; then
            echo "Environment for $wl was not set correctly."
            exit 1
          fi
        done
        
          
        ]
=== CONT  kuttl/harness/1-075_validate_dex_anyuid
    logger.go:42: 19:33:49 | 1-075_validate_dex_anyuid/3-restart_dex_pod | test step completed 3-restart_dex_pod
    logger.go:42: 19:33:49 | 1-075_validate_dex_anyuid | skipping kubernetes event logging
I0607 19:33:52.707654  250633 request.go:655] Throttling request took 3.099700023s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/storage.k8s.io/v1beta1?timeout=32s
=== CONT  kuttl/harness/1-047_validate_custom_env
    logger.go:42: 19:33:53 | 1-047_validate_custom_env/3-check-workloads | test step completed 3-check-workloads
    logger.go:42: 19:33:53 | 1-047_validate_custom_env | skipping kubernetes event logging
    logger.go:42: 19:33:53 | 1-047_validate_custom_env | Deleting namespace: kuttl-test-calm-racer
=== CONT  kuttl/harness/1-075_validate_dex_anyuid
    logger.go:42: 19:33:53 | 1-075_validate_dex_anyuid | Deleting namespace: kuttl-test-proven-crappie
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
=== CONT  kuttl/harness/1-044_validate_resource_limit_changes
    logger.go:42: 19:33:53 | 1-044_validate_resource_limit_changes | Creating namespace: kuttl-test-saving-feline
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:33:53 | 1-074_validate_terminating_namespace_block | Creating namespace: kuttl-test-nearby-crane
=== CONT  kuttl/harness/1-044_validate_resource_limit_changes
    logger.go:42: 19:33:54 | 1-044_validate_resource_limit_changes/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:33:54 | 1-074_validate_terminating_namespace_block/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-044_validate_resource_limit_changes
    logger.go:42: 19:34:00 | 1-044_validate_resource_limit_changes/1-install | ArgoCD:kuttl-test-saving-feline/argocd created
I0607 19:34:02.736736  250633 request.go:655] Throttling request took 2.943998383s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/image.openshift.io/v1?timeout=32s
    logger.go:42: 19:34:04 | 1-044_validate_resource_limit_changes/1-install | test step completed 1-install
    logger.go:42: 19:34:04 | 1-044_validate_resource_limit_changes/2-change-limits | starting test step 2-change-limits
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:34:04 | 1-074_validate_terminating_namespace_block/1-install | Namespace:/gitops-2242-ns-main created
    logger.go:42: 19:34:05 | 1-074_validate_terminating_namespace_block/1-install | ArgoCD:gitops-2242-ns-main/gitops-2242-argocd created
    logger.go:42: 19:34:05 | 1-074_validate_terminating_namespace_block/1-install | RoleBinding:gitops-2242-ns-main/grant-argocd created
    logger.go:42: 19:34:10 | 1-074_validate_terminating_namespace_block/1-install | test step completed 1-install
    logger.go:42: 19:34:10 | 1-074_validate_terminating_namespace_block/2-managed-ns-cm | starting test step 2-managed-ns-cm
=== CONT  kuttl/harness/1-044_validate_resource_limit_changes
    logger.go:42: 19:34:12 | 1-044_validate_resource_limit_changes/2-change-limits | ArgoCD:kuttl-test-saving-feline/argocd updated
    logger.go:42: 19:34:12 | 1-044_validate_resource_limit_changes/2-change-limits | test step completed 2-change-limits
    logger.go:42: 19:34:12 | 1-044_validate_resource_limit_changes/4-check-workloads | starting test step 4-check-workloads
I0607 19:34:12.756038  250633 request.go:655] Throttling request took 1.849430378s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/monitoring.coreos.com/v1alpha1?timeout=32s
    logger.go:42: 19:34:15 | 1-044_validate_resource_limit_changes/4-check-workloads | running command: [sh -c sleep 10]
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:34:15 | 1-074_validate_terminating_namespace_block/2-managed-ns-cm | Namespace:/gitops-2242-ns-first created
    logger.go:42: 19:34:16 | 1-074_validate_terminating_namespace_block/2-managed-ns-cm | ConfigMap:gitops-2242-ns-first/my-config-map-2 created
    logger.go:42: 19:34:17 | 1-074_validate_terminating_namespace_block/2-managed-ns-cm | test step completed 2-managed-ns-cm
    logger.go:42: 19:34:17 | 1-074_validate_terminating_namespace_block/3-delete-ns | starting test step 3-delete-ns
    logger.go:42: 19:34:17 | 1-074_validate_terminating_namespace_block/3-delete-ns | running command: [sh -c set -e
        
        oc delete ns gitops-2242-ns-first --wait=false
        
        exit 0
        ]
    logger.go:42: 19:34:17 | 1-074_validate_terminating_namespace_block/3-delete-ns | namespace "gitops-2242-ns-first" deleted
    logger.go:42: 19:34:23 | 1-074_validate_terminating_namespace_block/3-delete-ns | test step completed 3-delete-ns
    logger.go:42: 19:34:23 | 1-074_validate_terminating_namespace_block/4-create-ns | starting test step 4-create-ns
I0607 19:34:24.655897  250633 request.go:655] Throttling request took 1.050216014s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-5-0?timeout=32s
=== CONT  kuttl/harness/1-044_validate_resource_limit_changes
    logger.go:42: 19:34:25 | 1-044_validate_resource_limit_changes/4-check-workloads | running command: [sh -c set -e
        set -o pipefail
        for wl in deployments/argocd-server deployments/argocd-repo-server statefulsets/argocd-application-controller; do
          res=$(oc get -n $NAMESPACE $wl -o jsonpath='{.spec.template.spec.containers[0].resources.limits.cpu}')
          if test "$res" != "2"; then
            echo "Reconciliation of resources for $wl failed: Should be 2, is '$res'"
            exit 1
          fi
        done
        ]
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:34:28 | 1-074_validate_terminating_namespace_block/4-create-ns | Namespace:/gitops-2242-ns-second created
=== CONT  kuttl/harness/1-044_validate_resource_limit_changes
    logger.go:42: 19:34:32 | 1-044_validate_resource_limit_changes/4-check-workloads | test step completed 4-check-workloads
    logger.go:42: 19:34:32 | 1-044_validate_resource_limit_changes | skipping kubernetes event logging
    logger.go:42: 19:34:32 | 1-044_validate_resource_limit_changes | Deleting namespace: kuttl-test-saving-feline
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:34:33 | 1-048_validate_controller_sharding | Creating namespace: kuttl-test-renewing-reindeer
    logger.go:42: 19:34:33 | 1-048_validate_controller_sharding/1-install | starting test step 1-install
I0607 19:34:35.117217  250633 request.go:655] Throttling request took 1.000916479s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/scheduling.k8s.io/v1?timeout=32s
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:34:38 | 1-074_validate_terminating_namespace_block/4-create-ns | test step completed 4-create-ns
    logger.go:42: 19:34:38 | 1-074_validate_terminating_namespace_block/99-cleanup | starting test step 99-cleanup
    logger.go:42: 19:34:38 | 1-074_validate_terminating_namespace_block/99-cleanup | running command: [sh -c set -e
        
        # Removing finalizer from cm
        oc patch cm/my-config-map-2 -n gitops-2242-ns-first --type json --patch='[ { "op": "remove", "path": "/metadata/finalizers" } ]'
        
        # Deleting namespaces
        oc delete ns gitops-2242-ns-first --wait=false
        oc delete ns gitops-2242-ns-second --wait=false
        oc delete ns gitops-2242-ns-main --wait=false
        
        exit 0
        ]
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:34:39 | 1-048_validate_controller_sharding/1-install | ArgoCD:kuttl-test-renewing-reindeer/argocd created
    logger.go:42: 19:34:39 | 1-048_validate_controller_sharding/1-install | test step completed 1-install
    logger.go:42: 19:34:39 | 1-048_validate_controller_sharding/2-change-sharding | starting test step 2-change-sharding
=== CONT  kuttl/harness/1-074_validate_terminating_namespace_block
    logger.go:42: 19:34:40 | 1-074_validate_terminating_namespace_block/99-cleanup | configmap/my-config-map-2 patched
    logger.go:42: 19:34:41 | 1-074_validate_terminating_namespace_block/99-cleanup | namespace "gitops-2242-ns-first" deleted
    logger.go:42: 19:34:41 | 1-074_validate_terminating_namespace_block/99-cleanup | namespace "gitops-2242-ns-second" deleted
    logger.go:42: 19:34:42 | 1-074_validate_terminating_namespace_block/99-cleanup | namespace "gitops-2242-ns-main" deleted
I0607 19:34:45.930052  250633 request.go:655] Throttling request took 1.000166545s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/resolution.tekton.dev/v1beta1?timeout=32s
    logger.go:42: 19:34:49 | 1-074_validate_terminating_namespace_block/99-cleanup | test step completed 99-cleanup
    logger.go:42: 19:34:49 | 1-074_validate_terminating_namespace_block | skipping kubernetes event logging
    logger.go:42: 19:34:49 | 1-074_validate_terminating_namespace_block | Deleting namespace: kuttl-test-nearby-crane
=== CONT  kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm
    logger.go:42: 19:34:49 | 1-039_validate_fix_argocd-tls-certs-cm | Creating namespace: kuttl-test-liked-owl
    logger.go:42: 19:34:49 | 1-039_validate_fix_argocd-tls-certs-cm/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:34:49 | 1-048_validate_controller_sharding/2-change-sharding | ArgoCD:kuttl-test-renewing-reindeer/argocd updated
=== CONT  kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm
    logger.go:42: 19:34:55 | 1-039_validate_fix_argocd-tls-certs-cm/1-install | ArgoCD:kuttl-test-liked-owl/example-argocd created
    logger.go:42: 19:34:57 | 1-039_validate_fix_argocd-tls-certs-cm/1-install | test step completed 1-install
    logger.go:42: 19:34:57 | 1-039_validate_fix_argocd-tls-certs-cm/2-modify_configmap | starting test step 2-modify_configmap
I0607 19:34:58.798535  250633 request.go:655] Throttling request took 1.050798302s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/authentication.k8s.io/v1?timeout=32s
    logger.go:42: 19:35:02 | 1-039_validate_fix_argocd-tls-certs-cm/2-modify_configmap | ConfigMap:kuttl-test-liked-owl/argocd-tls-certs-cm updated
    logger.go:42: 19:35:02 | 1-039_validate_fix_argocd-tls-certs-cm/2-modify_configmap | test step completed 2-modify_configmap
    logger.go:42: 19:35:02 | 1-039_validate_fix_argocd-tls-certs-cm/3-modify_argocd_cr | starting test step 3-modify_argocd_cr
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:35:07 | 1-048_validate_controller_sharding/2-change-sharding | test step completed 2-change-sharding
    logger.go:42: 19:35:07 | 1-048_validate_controller_sharding/3-check-env | starting test step 3-check-env
    logger.go:42: 19:35:07 | 1-048_validate_controller_sharding/3-check-env | running command: [sh -c replicas=$(oc get -n $NAMESPACE statefulset argocd-application-controller -o json \
          | jq -r '.spec.template.spec.containers[0].env[]|select(.name=="ARGOCD_CONTROLLER_REPLICAS").value')
        if test "$replicas" != "3"; then
          echo "Environment ARGOCD_CONTROLLER_REPLICAS not correct. Should '3', is '$replicas'"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm
    logger.go:42: 19:35:08 | 1-039_validate_fix_argocd-tls-certs-cm/3-modify_argocd_cr | ArgoCD:kuttl-test-liked-owl/example-argocd updated
    logger.go:42: 19:35:08 | 1-039_validate_fix_argocd-tls-certs-cm/3-modify_argocd_cr | test step completed 3-modify_argocd_cr
    logger.go:42: 19:35:08 | 1-039_validate_fix_argocd-tls-certs-cm/4- | starting test step 4-
I0607 19:35:10.071045  250633 request.go:655] Throttling request took 1.000313575s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/postgres-operator.crunchydata.com/v1beta1?timeout=32s
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:35:13 | 1-048_validate_controller_sharding/3-check-env | test step completed 3-check-env
    logger.go:42: 19:35:13 | 1-048_validate_controller_sharding/4-change-sharding | starting test step 4-change-sharding
    logger.go:42: 19:35:19 | 1-048_validate_controller_sharding/4-change-sharding | ArgoCD:kuttl-test-renewing-reindeer/argocd updated
I0607 19:35:20.091509  250633 request.go:655] Throttling request took 1.599629419s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/performance.openshift.io/v2?timeout=32s
=== CONT  kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm
    logger.go:42: 19:35:23 | 1-039_validate_fix_argocd-tls-certs-cm/4- | test step completed 4-
    logger.go:42: 19:35:23 | 1-039_validate_fix_argocd-tls-certs-cm | skipping kubernetes event logging
    logger.go:42: 19:35:23 | 1-039_validate_fix_argocd-tls-certs-cm | Deleting namespace: kuttl-test-liked-owl
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:35:23 | 1-066_validate_redis_secure_comm_no_autotls_no_ha | Creating namespace: kuttl-test-pumped-anteater
    logger.go:42: 19:35:23 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-083_validate_resource_customization_subkeys
    logger.go:42: 19:35:24 | 1-083_validate_resource_customization_subkeys/1-argocd-with-resource-customization-subkeys | test step failed 1-argocd-with-resource-customization-subkeys
    case.go:361: failed in step 1-argocd-with-resource-customization-subkeys
    case.go:363: --- ConfigMap:kuttl-test-wanted-magpie/argocd-cm
        +++ ConfigMap:kuttl-test-wanted-magpie/argocd-cm
        @@ -12,75 +12,59 @@
           oidc.config: ""
           repositories: ""
           repository.credentials: ""
        -  resource.customizations.actions.apps_Deployment: |
        -    discovery.lua: |
        -    actions = {}
        -    actions["restart"] = {}
        -    return actions
        -    definitions:
        -    - name: restart
        -      # Lua Script to modify the obj
        -      action.lua: |
        -        local os = require("os")
        -        if obj.spec.template.metadata == nil then
        -            obj.spec.template.metadata = {}
        -        end
        -        if obj.spec.template.metadata.annotations == nil then
        -            obj.spec.template.metadata.annotations = {}
        -        end
        -        obj.spec.template.metadata.annotations["kubectl.kubernetes.io/restartedAt"] = os.date("!%!Y(MISSING)-%!m(MISSING)-%!d(MISSING)T%!X(MISSING)Z")
        -        return obj
        -  resource.customizations.health.certmanager.k8s.io_Certificate: |
        -    hs = {}
        -    if obj.status ~= nil then
        -      if obj.status.conditions ~= nil then
        -        for i, condition in ipairs(obj.status.conditions) do
        -          if condition.type == "Ready" and condition.status == "False" then
        -            hs.status = "Degraded"
        -            hs.message = condition.message
        -            return hs
        -          end
        -          if condition.type == "Ready" and condition.status == "True" then
        -            hs.status = "Healthy"
        -            hs.message = condition.message
        -            return hs
        -          end
        -        end
        -      end
        -    end
        -    hs.status = "Progressing"
        -    hs.message = "Waiting for certificate"
        -    return hs
        -  resource.customizations.ignoreDifferences.all: |
        -    jqpathexpressions:
        -    - xyz
        -    - abc
        -    jsonpointers:
        -    - xyz
        -    - abc
        -    managedfieldsmanagers:
        -    - xyz
        -    - abc
        -  resource.customizations.ignoreDifferences.apps_deployments: |
        -    jqpathexpressions:
        -    - xyz
        -    - abc
        -    jsonpointers:
        -    - xyz
        -    - abc
        -    managedfieldsmanagers: []
        -  resource.customizations.ignoreDifferences.batch_jobs: |
        -    jqpathexpressions:
        -    - xyz
        -    - abc
        -    jsonpointers:
        -    - xyz
        -    - abc
        -    managedfieldsmanagers:
        -    - xyz
        -    - abc
        +  resource.exclusions: ""
        +  resource.inclusions: ""
        +  statusbadge.enabled: "false"
        +  url: https://example-argocd-server
        +  users.anonymous.enabled: "false"
         kind: ConfigMap
         metadata:
        +  labels:
        +    app.kubernetes.io/managed-by: example-argocd
        +    app.kubernetes.io/name: argocd-cm
        +    app.kubernetes.io/part-of: argocd
        +  managedFields:
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:data:
        +        .: {}
        +        f:admin.enabled: {}
        +        f:application.instanceLabelKey: {}
        +        f:application.resourceTrackingMethod: {}
        +        f:configManagementPlugins: {}
        +        f:ga.anonymizeusers: {}
        +        f:ga.trackingid: {}
        +        f:help.chatText: {}
        +        f:help.chatUrl: {}
        +        f:kustomize.buildOptions: {}
        +        f:oidc.config: {}
        +        f:repositories: {}
        +        f:repository.credentials: {}
        +        f:resource.exclusions: {}
        +        f:resource.inclusions: {}
        +        f:statusbadge.enabled: {}
        +        f:url: {}
        +        f:users.anonymous.enabled: {}
        +      f:metadata:
        +        f:labels:
        +          .: {}
        +          f:app.kubernetes.io/managed-by: {}
        +          f:app.kubernetes.io/name: {}
        +          f:app.kubernetes.io/part-of: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"b8ad6965-76a7-4663-8db1-c20189da1f6e"}: {}
        +    manager: manager
        +    operation: Update
        +    time: "2023-06-07T13:45:28Z"
           name: argocd-cm
           namespace: kuttl-test-wanted-magpie
        +  ownerReferences:
        +  - apiVersion: argoproj.io/v1alpha1
        +    blockOwnerDeletion: true
        +    controller: true
        +    kind: ArgoCD
        +    name: example-argocd
        +    uid: b8ad6965-76a7-4663-8db1-c20189da1f6e
         
        
    case.go:363: resource ConfigMap:kuttl-test-wanted-magpie/argocd-cm: .data.resource.customizations.ignoreDifferences.all: key is missing from map
    logger.go:42: 19:35:24 | 1-083_validate_resource_customization_subkeys | skipping kubernetes event logging
    logger.go:42: 19:35:28 | 1-083_validate_resource_customization_subkeys | Deleting namespace: kuttl-test-wanted-magpie
=== CONT  kuttl/harness/1-072_validate_liveness_probe_removed
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:35:29 | 1-048_validate_controller_sharding/4-change-sharding | test step completed 4-change-sharding
    logger.go:42: 19:35:29 | 1-048_validate_controller_sharding/5-check-env | starting test step 5-check-env
    logger.go:42: 19:35:29 | 1-048_validate_controller_sharding/5-check-env | running command: [sh -c # we do expect error from jq here if env is not set at all
        replicas=$(oc get -n $NAMESPACE statefulset argocd-application-controller -o json \
          | jq -r '.spec.template.spec.containers[0].env[]|select(.name=="ARGOCD_CONTROLLER_REPLICASE").value')
        if test "$replicas" != ""; then
          echo "Environment ARGOCD_CONTROLLER_REPLICAS not correct. Should '', is '$replicas'"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-072_validate_liveness_probe_removed
    logger.go:42: 19:35:29 | 1-072_validate_liveness_probe_removed | Creating namespace: kuttl-test-aware-loon
    logger.go:42: 19:35:29 | 1-072_validate_liveness_probe_removed/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:35:29 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/1-install | ArgoCD:kuttl-test-pumped-anteater/argocd created
I0607 19:35:30.902023  250633 request.go:655] Throttling request took 1.049976928s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/imageregistry.operator.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-072_validate_liveness_probe_removed
    logger.go:42: 19:35:34 | 1-072_validate_liveness_probe_removed/1-install | ArgoCD:kuttl-test-aware-loon/argocd created
=== CONT  kuttl/harness/1-048_validate_controller_sharding
    logger.go:42: 19:35:38 | 1-048_validate_controller_sharding/5-check-env | test step completed 5-check-env
    logger.go:42: 19:35:38 | 1-048_validate_controller_sharding | skipping kubernetes event logging
    logger.go:42: 19:35:38 | 1-048_validate_controller_sharding | Deleting namespace: kuttl-test-renewing-reindeer
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:35:38 | 1-062_validate_extra_config | Creating namespace: kuttl-test-set-rodent
=== CONT  kuttl/harness/1-072_validate_liveness_probe_removed
    logger.go:42: 19:35:39 | 1-072_validate_liveness_probe_removed/1-install | test step completed 1-install
    logger.go:42: 19:35:39 | 1-072_validate_liveness_probe_removed/2-check_liveness_probe_removed | starting test step 2-check_liveness_probe_removed
    logger.go:42: 19:35:39 | 1-072_validate_liveness_probe_removed/2-check_liveness_probe_removed | running command: [sh -c set -e
        if ! test -z $(oc get statefulset.apps/argocd-application-controller -n $NAMESPACE -o jsonpath='{.spec.template.spec.containers[].livenessProbe}'); then
          echo "livenessProbe should not exist on statefulset.apps/argocd-application-controller."
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:35:39 | 1-062_validate_extra_config/1-argocd-with-extraconfig | starting test step 1-argocd-with-extraconfig
I0607 19:35:40.940779  250633 request.go:655] Throttling request took 1.149464077s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/autoscaling.openshift.io/v1?timeout=32s
    logger.go:42: 19:35:44 | 1-062_validate_extra_config/1-argocd-with-extraconfig | ArgoCD:kuttl-test-set-rodent/example-argocd created
=== CONT  kuttl/harness/1-072_validate_liveness_probe_removed
    logger.go:42: 19:35:48 | 1-072_validate_liveness_probe_removed/2-check_liveness_probe_removed | test step completed 2-check_liveness_probe_removed
    logger.go:42: 19:35:48 | 1-072_validate_liveness_probe_removed | skipping kubernetes event logging
    logger.go:42: 19:35:48 | 1-072_validate_liveness_probe_removed | Deleting namespace: kuttl-test-aware-loon
=== CONT  kuttl/harness/1-071_validate_SCC_HA
    logger.go:42: 19:35:48 | 1-071_validate_SCC_HA | Creating namespace: kuttl-test-close-llama
    logger.go:42: 19:35:49 | 1-071_validate_SCC_HA/1-create-scc | starting test step 1-create-scc
I0607 19:35:50.978706  250633 request.go:655] Throttling request took 1.199885494s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/metal3.io/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:35:54 | 1-062_validate_extra_config/1-argocd-with-extraconfig | test step completed 1-argocd-with-extraconfig
    logger.go:42: 19:35:54 | 1-062_validate_extra_config/2-argocd-with-firstclass-and-extraconfig copy | starting test step 2-argocd-with-firstclass-and-extraconfig copy
=== CONT  kuttl/harness/1-071_validate_SCC_HA
    logger.go:42: 19:35:54 | 1-071_validate_SCC_HA/1-create-scc | SecurityContextConstraints:/restricted-dropcaps created
    logger.go:42: 19:35:59 | 1-071_validate_SCC_HA/1-create-scc | test step completed 1-create-scc
    logger.go:42: 19:35:59 | 1-071_validate_SCC_HA/2-create-argocd | starting test step 2-create-argocd
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:36:00 | 1-062_validate_extra_config/2-argocd-with-firstclass-and-extraconfig copy | ArgoCD:kuttl-test-set-rodent/example-argocd updated
I0607 19:36:01.218253  250633 request.go:655] Throttling request took 1.049972285s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/flowcontrol.apiserver.k8s.io/v1beta2?timeout=32s
    logger.go:42: 19:36:04 | 1-062_validate_extra_config/2-argocd-with-firstclass-and-extraconfig copy | test step completed 2-argocd-with-firstclass-and-extraconfig copy
    logger.go:42: 19:36:04 | 1-062_validate_extra_config/3-update-configmap | starting test step 3-update-configmap
=== CONT  kuttl/harness/1-071_validate_SCC_HA
    logger.go:42: 19:36:05 | 1-071_validate_SCC_HA/2-create-argocd | ArgoCD:kuttl-test-close-llama/argocd created
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:36:09 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/1-install | test step completed 1-install
    logger.go:42: 19:36:09 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | starting test step 2-generate_cert
    logger.go:42: 19:36:09 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | running command: [sh -c set -e
        
        echo -e "\n[SAN]\nsubjectAltName=DNS:argocd-redis.$NAMESPACE.svc.cluster.local\n[req]\ndistinguished_name=req" > ${PWD}/openssl_test.cnf
        
        openssl req -new -x509 -sha256 \
         -subj "/C=XX/ST=XX/O=Testing/CN=redis" \
         -reqexts SAN -extensions SAN \
         -config ${PWD}/openssl_test.cnf \
         -keyout ${PWD}/redis.key \
         -out ${PWD}/redis.crt \
         -newkey rsa:4096 \
         -nodes \
         -sha256 \
         -days 10
        ]
    logger.go:42: 19:36:09 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | Generating a RSA private key
    logger.go:42: 19:36:09 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | ........................++++
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:36:10 | 1-062_validate_extra_config/3-update-configmap | ConfigMap:kuttl-test-set-rodent/argocd-cm updated
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:36:10 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | .................................................................................................................................................................................................................++++
    logger.go:42: 19:36:10 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | writing new private key to '/home/skatyal/gitops-operator/test/openshift/e2e/parallel/1-066_validate_redis_secure_comm_no_autotls_no_ha/redis.key'
    logger.go:42: 19:36:10 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | -----
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:36:13 | 1-062_validate_extra_config/3-update-configmap | test step completed 3-update-configmap
    logger.go:42: 19:36:13 | 1-062_validate_extra_config/4-create-dex-without-extraconfig | starting test step 4-create-dex-without-extraconfig
I0607 19:36:14.482715  250633 request.go:655] Throttling request took 1.000866825s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/quota.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:36:17 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/2-generate_cert | test step completed 2-generate_cert
    logger.go:42: 19:36:17 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/3-create_secret | starting test step 3-create_secret
    logger.go:42: 19:36:17 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/3-create_secret | running command: [sh -c set -e
        
        # clean up key, cert and config file
        cleanup() {
           rm -rf ${PWD}/redis.crt && rm -rf ${PWD}/redis.key && rm -rf ${PWD}/openssl_test.cnf
        }
        
        trap cleanup INT TERM EXIT
        
        oc create secret tls argocd-operator-redis-tls --key=${PWD}/redis.key --cert=${PWD}/redis.crt -n $NAMESPACE
        sleep 10
        ]
    logger.go:42: 19:36:18 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/3-create_secret | secret/argocd-operator-redis-tls created
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:36:23 | 1-062_validate_extra_config/4-create-dex-without-extraconfig | ArgoCD:kuttl-test-set-rodent/example-argocd updated
    logger.go:42: 19:36:23 | 1-062_validate_extra_config/4-create-dex-without-extraconfig | test step completed 4-create-dex-without-extraconfig
    logger.go:42: 19:36:23 | 1-062_validate_extra_config/5-override-dex-using-extraconfig | starting test step 5-override-dex-using-extraconfig
=== CONT  kuttl/harness/1-071_validate_SCC_HA
    logger.go:42: 19:36:24 | 1-071_validate_SCC_HA/2-create-argocd | test step completed 2-create-argocd
    logger.go:42: 19:36:24 | 1-071_validate_SCC_HA/3-modify-argocd | starting test step 3-modify-argocd
I0607 19:36:25.310450  250633 request.go:655] Throttling request took 1.000304318s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/events.k8s.io/v1?timeout=32s
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:36:29 | 1-062_validate_extra_config/5-override-dex-using-extraconfig | ArgoCD:kuttl-test-set-rodent/example-argocd updated
I0607 19:36:35.359269  250633 request.go:655] Throttling request took 1.600416425s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1-8-0?timeout=32s
    logger.go:42: 19:36:38 | 1-062_validate_extra_config/5-override-dex-using-extraconfig | test step completed 5-override-dex-using-extraconfig
    logger.go:42: 19:36:38 | 1-062_validate_extra_config | skipping kubernetes event logging
=== CONT  kuttl/harness/1-071_validate_SCC_HA
    logger.go:42: 19:36:38 | 1-071_validate_SCC_HA/3-modify-argocd | ArgoCD:kuttl-test-close-llama/argocd updated
=== CONT  kuttl/harness/1-062_validate_extra_config
    logger.go:42: 19:36:40 | 1-062_validate_extra_config | Deleting namespace: kuttl-test-set-rodent
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:36:40 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/3-create_secret | test step completed 3-create_secret
    logger.go:42: 19:36:40 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/4-create_annotation | starting test step 4-create_annotation
    logger.go:42: 19:36:40 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/4-create_annotation | running command: [sh -c set -e
        oc annotate secret argocd-operator-redis-tls argocds.argoproj.io/name=argocd -n $NAMESPACE
        sleep 30
        ]
=== CONT  kuttl/harness/1-043_validate_log_level_format
    logger.go:42: 19:36:40 | 1-043_validate_log_level_format | Creating namespace: kuttl-test-fit-rhino
    logger.go:42: 19:36:40 | 1-043_validate_log_level_format/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:36:41 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/4-create_annotation | secret/argocd-operator-redis-tls annotated
I0607 19:36:45.385060  250633 request.go:655] Throttling request took 3.843531446s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/triggers.tekton.dev/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-043_validate_log_level_format
    logger.go:42: 19:36:46 | 1-043_validate_log_level_format/1-install | ArgoCD:kuttl-test-fit-rhino/argocd created
    logger.go:42: 19:36:48 | 1-043_validate_log_level_format/1-install | test step completed 1-install
    logger.go:42: 19:36:48 | 1-043_validate_log_level_format/2-change-loglevel | starting test step 2-change-loglevel
    logger.go:42: 19:36:54 | 1-043_validate_log_level_format/2-change-loglevel | ArgoCD:kuttl-test-fit-rhino/argocd updated
    logger.go:42: 19:36:54 | 1-043_validate_log_level_format/2-change-loglevel | test step completed 2-change-loglevel
    logger.go:42: 19:36:54 | 1-043_validate_log_level_format/3-check-loglevel | starting test step 3-check-loglevel
    logger.go:42: 19:36:54 | 1-043_validate_log_level_format/3-check-loglevel | running command: [sh -c sleep 15]
    logger.go:42: 19:37:09 | 1-043_validate_log_level_format/3-check-loglevel | running command: [sh -c set -e
        set -o pipefail
        for wl in deployment/argocd-server deployment/argocd-repo-server statefulset/argocd-application-controller; do
          wlCommand=$(oc get -n $NAMESPACE $wl -o jsonpath='{.spec.template.spec.containers[0].command}'| jq -r '.[]' )
          level="debug"
          format="json"
          if ! echo "$wlCommand" | grep -qPz -- "--loglevel\\n$level(\$|\\n)"; then
            echo "logLevel was not set correctly for $wl"
            echo "CWD: $wlCommand"
            exit 1
          fi
          if ! echo "$wlCommand" | grep -qPz -- "--logformat\\n$format(\$|\\n)"; then
            echo "logFormat was not set correctly for $wl"
            echo "CWD: $wlCommand"
            exit 1
          fi
        done
        ]
I0607 19:37:13.362826  250633 request.go:655] Throttling request took 1.000133526s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/machineconfiguration.openshift.io/v1?timeout=32s
    logger.go:42: 19:37:21 | 1-043_validate_log_level_format/3-check-loglevel | test step completed 3-check-loglevel
    logger.go:42: 19:37:21 | 1-043_validate_log_level_format | skipping kubernetes event logging
    logger.go:42: 19:37:21 | 1-043_validate_log_level_format | Deleting namespace: kuttl-test-fit-rhino
=== CONT  kuttl/harness/1-061_validate_resource_tracking_method
    logger.go:42: 19:37:21 | 1-061_validate_resource_tracking_method | Creating namespace: kuttl-test-definite-buffalo
    logger.go:42: 19:37:22 | 1-061_validate_resource_tracking_method/1-install | starting test step 1-install
I0607 19:37:23.825679  250633 request.go:655] Throttling request took 1.000556424s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/postgresql.k8s.enterprisedb.io/v1?timeout=32s
    logger.go:42: 19:37:28 | 1-061_validate_resource_tracking_method/1-install | ArgoCD:kuttl-test-definite-buffalo/argocd created
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:37:28 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/4-create_annotation | test step completed 4-create_annotation
    logger.go:42: 19:37:28 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/5-check_deployments | starting test step 5-check_deployments
    logger.go:42: 19:37:28 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/5-check_deployments | running command: [sh -c set -e
        
        if test "$(oc get deployments.apps argocd-redis -n $NAMESPACE --template '{{range .spec.template.spec.containers}}{{.args}}{{"\n"}}{{end}}')" != \
        "[redis-server \
        --protected-mode no \
        --save  \
        --appendonly no \
        --tls-port 6379 \
        --port 0 \
        --tls-cert-file /app/config/redis/tls/tls.crt \
        --tls-key-file /app/config/redis/tls/tls.key \
        --tls-auth-clients no]"; then
          echo "TLS .spec.template.spec.containers.args for argocd-redis deployment are wrong"
          exit 1
        fi
        ]
    logger.go:42: 19:37:29 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/5-check_deployments | running command: [sh -c set -e
        
        if test "$(oc get deployments.apps argocd-repo-server -n $NAMESPACE --template '{{range .spec.template.spec.containers}}{{.command}}{{"\n"}}{{end}}')" != \
        "[uid_entrypoint.sh \
        argocd-repo-server \
        --redis argocd-redis.$NAMESPACE.svc.cluster.local:6379 \
        --redis-use-tls \
        --redis-ca-certificate /app/config/reposerver/tls/redis/tls.crt \
        --loglevel info \
        --logformat text]"; then
          echo "TLS .spec.template.spec.containers.command for argocd-repo-server deployment is wrong"
          exit 1
        fi
        ]
    logger.go:42: 19:37:30 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/5-check_deployments | running command: [sh -c set -e
        
        if test "$(oc get deployments.apps argocd-server -n $NAMESPACE --template '{{range .spec.template.spec.containers}}{{.command}}{{"\n"}}{{end}}')" != \
        "[argocd-server \
        --staticassets /shared/app \
        --dex-server https://argocd-dex-server.$NAMESPACE.svc.cluster.local:5556 \
        --repo-server argocd-repo-server.$NAMESPACE.svc.cluster.local:8081 \
        --redis argocd-redis.$NAMESPACE.svc.cluster.local:6379 \
        --redis-use-tls \
        --redis-ca-certificate /app/config/server/tls/redis/tls.crt \
        --loglevel info \
        --logformat text]"; then
          echo "TLS .spec.template.spec.containers.command for argocd-server deployment is wrong"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-061_validate_resource_tracking_method
    logger.go:42: 19:37:31 | 1-061_validate_resource_tracking_method/1-install | test step completed 1-install
    logger.go:42: 19:37:31 | 1-061_validate_resource_tracking_method/2-patch_cm_annotation | starting test step 2-patch_cm_annotation
    logger.go:42: 19:37:31 | 1-061_validate_resource_tracking_method/2-patch_cm_annotation | running command: [sh -c set -e
        
        oc patch argocds.argoproj.io argocd --type=merge -p '{"spec":{"resourceTrackingMethod":"annotation"}}' -n $NAMESPACE
        ]
I0607 19:37:33.844979  250633 request.go:655] Throttling request took 1.292926694s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/machine.openshift.io/v1?timeout=32s
    logger.go:42: 19:37:34 | 1-061_validate_resource_tracking_method/2-patch_cm_annotation | argocd.argoproj.io/argocd patched
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:37:36 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/5-check_deployments | test step completed 5-check_deployments
    logger.go:42: 19:37:36 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/6-check_statefulset | starting test step 6-check_statefulset
    logger.go:42: 19:37:36 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/6-check_statefulset | running command: [sh -c set -e
        
        if test "$(oc get statefulsets.apps argocd-application-controller -n $NAMESPACE --template '{{range .spec.template.spec.containers}}{{.command}}{{"\n"}}{{end}}')" != \
        "[argocd-application-controller \
        --operation-processors 10 \
        --redis argocd-redis.$NAMESPACE.svc.cluster.local:6379 \
        --redis-use-tls \
        --redis-ca-certificate /app/config/controller/tls/redis/tls.crt \
        --repo-server argocd-repo-server.$NAMESPACE.svc.cluster.local:8081 \
        --status-processors 20 \
        --kubectl-parallelism-limit 10 \
        --loglevel info \
        --logformat text]"; then
          echo "TLS .spec.template.spec.containers.command for argocd-application-controller statefulsets is wrong"
          exit 1
        fi
        ]
=== CONT  kuttl/harness/1-061_validate_resource_tracking_method
    logger.go:42: 19:37:42 | 1-061_validate_resource_tracking_method/2-patch_cm_annotation | test step completed 2-patch_cm_annotation
    logger.go:42: 19:37:42 | 1-061_validate_resource_tracking_method/3-patch_cm_annotation_label | starting test step 3-patch_cm_annotation_label
I0607 19:37:43.859576  250633 request.go:655] Throttling request took 1.799209346s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/monitoring.coreos.com/v1beta1?timeout=32s
=== CONT  kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha
    logger.go:42: 19:37:46 | 1-066_validate_redis_secure_comm_no_autotls_no_ha/6-check_statefulset | test step completed 6-check_statefulset
    logger.go:42: 19:37:46 | 1-066_validate_redis_secure_comm_no_autotls_no_ha | skipping kubernetes event logging
    logger.go:42: 19:37:46 | 1-066_validate_redis_secure_comm_no_autotls_no_ha | Deleting namespace: kuttl-test-pumped-anteater
=== CONT  kuttl/harness/1-061_validate_resource_tracking_method
    logger.go:42: 19:37:46 | 1-061_validate_resource_tracking_method/3-patch_cm_annotation_label | running command: [sh -c set -e
        
        oc patch argocds.argoproj.io argocd --type=merge -p '{"spec":{"resourceTrackingMethod":"annotation+label"}}' -n $NAMESPACE
        ]
=== CONT  kuttl/harness/1-063_validate_statefulset_restart
    logger.go:42: 19:37:46 | 1-063_validate_statefulset_restart | Creating namespace: kuttl-test-assuring-ghoul
    logger.go:42: 19:37:46 | 1-063_validate_statefulset_restart/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-061_validate_resource_tracking_method
    logger.go:42: 19:37:49 | 1-061_validate_resource_tracking_method/3-patch_cm_annotation_label | argocd.argoproj.io/argocd patched
=== CONT  kuttl/harness/1-063_validate_statefulset_restart
    logger.go:42: 19:37:52 | 1-063_validate_statefulset_restart/1-install | ArgoCD:kuttl-test-assuring-ghoul/example-argocd created
I0607 19:37:53.890585  250633 request.go:655] Throttling request took 1.898928129s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pipelines.openshift.io/v1alpha1?timeout=32s
=== CONT  kuttl/harness/1-061_validate_resource_tracking_method
    logger.go:42: 19:37:56 | 1-061_validate_resource_tracking_method/3-patch_cm_annotation_label | test step completed 3-patch_cm_annotation_label
    logger.go:42: 19:37:56 | 1-061_validate_resource_tracking_method/4-patch_cm_invalid_method | starting test step 4-patch_cm_invalid_method
    logger.go:42: 19:37:56 | 1-061_validate_resource_tracking_method/4-patch_cm_invalid_method | running command: [sh -c set -e
        
        oc patch argocds.argoproj.io argocd --type=merge -p '{"spec":{"resourceTrackingMethod":"invalid_method"}}' -n $NAMESPACE
        ]
    logger.go:42: 19:37:59 | 1-061_validate_resource_tracking_method/4-patch_cm_invalid_method | argocd.argoproj.io/argocd patched
I0607 19:38:03.912590  250633 request.go:655] Throttling request took 3.899466314s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/flowcontrol.apiserver.k8s.io/v1beta2?timeout=32s
    logger.go:42: 19:38:04 | 1-061_validate_resource_tracking_method/4-patch_cm_invalid_method | test step completed 4-patch_cm_invalid_method
    logger.go:42: 19:38:04 | 1-061_validate_resource_tracking_method | skipping kubernetes event logging
    logger.go:42: 19:38:04 | 1-061_validate_resource_tracking_method | Deleting namespace: kuttl-test-definite-buffalo
=== CONT  kuttl/harness/1-063_validate_statefulset_restart
    logger.go:42: 19:38:04 | 1-063_validate_statefulset_restart/1-install | test step completed 1-install
    logger.go:42: 19:38:04 | 1-063_validate_statefulset_restart/2-change_to_invalid_image | starting test step 2-change_to_invalid_image
    logger.go:42: 19:38:04 | 1-063_validate_statefulset_restart/2-change_to_invalid_image | running command: [sh -c set -e
        
        oc patch statefulset/example-argocd-application-controller \
          -n $NAMESPACE \
          --type "json" \
          -p '[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":"invalid_image"}]'
        sleep 10
        ]
=== CONT  kuttl/harness/1-065_validate_redis_ha_anti_affinity
    logger.go:42: 19:38:05 | 1-065_validate_redis_ha_anti_affinity | Creating namespace: kuttl-test-true-akita
    logger.go:42: 19:38:05 | 1-065_validate_redis_ha_anti_affinity/1-install | starting test step 1-install
=== CONT  kuttl/harness/1-063_validate_statefulset_restart
    logger.go:42: 19:38:06 | 1-063_validate_statefulset_restart/2-change_to_invalid_image | statefulset.apps/example-argocd-application-controller patched
=== CONT  kuttl/harness/1-065_validate_redis_ha_anti_affinity
    logger.go:42: 19:38:11 | 1-065_validate_redis_ha_anti_affinity/1-install | ArgoCD:kuttl-test-true-akita/argocd created
    logger.go:42: 19:38:11 | 1-065_validate_redis_ha_anti_affinity/1-install | test step completed 1-install
    logger.go:42: 19:38:11 | 1-065_validate_redis_ha_anti_affinity/2- | starting test step 2-
I0607 19:38:13.949131  250633 request.go:655] Throttling request took 2.295840801s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/apps.openshift.io/v1?timeout=32s
=== CONT  kuttl/harness/1-063_validate_statefulset_restart
    logger.go:42: 19:38:21 | 1-063_validate_statefulset_restart/2-change_to_invalid_image | test step completed 2-change_to_invalid_image
    logger.go:42: 19:38:21 | 1-063_validate_statefulset_restart/3-check_image_after_change | starting test step 3-check_image_after_change
    logger.go:42: 19:38:21 | 1-063_validate_statefulset_restart/3-check_image_after_change | running command: [sh -c set -e
        
        image=$(oc get statefulset/example-argocd-application-controller -o jsonpath='{.spec.template.spec.containers[].image}' -n $NAMESPACE)
        
        if test "${image}" == "invalid_image"; then
          echo "The application-controller statefulset is using 'invalid_image'"
          exit 1
        fi
        exit 0
        ]
=== CONT  kuttl/harness/1-065_validate_redis_ha_anti_affinity
    logger.go:42: 19:38:21 | 1-065_validate_redis_ha_anti_affinity/2- | test step completed 2-
    logger.go:42: 19:38:21 | 1-065_validate_redis_ha_anti_affinity | skipping kubernetes event logging
    logger.go:42: 19:38:21 | 1-065_validate_redis_ha_anti_affinity | Deleting namespace: kuttl-test-true-akita
=== CONT  kuttl/harness/1-064_validate_security_contexts
    logger.go:42: 19:38:21 | 1-064_validate_security_contexts | Creating namespace: kuttl-test-nearby-toad
    logger.go:42: 19:38:21 | 1-064_validate_security_contexts/1-install | starting test step 1-install
I0607 19:38:23.956773  250633 request.go:655] Throttling request took 1.450028189s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/machine.openshift.io/v1beta1?timeout=32s
    logger.go:42: 19:38:27 | 1-064_validate_security_contexts/1-install | ArgoCD:kuttl-test-nearby-toad/argocd created
=== CONT  kuttl/harness/1-063_validate_statefulset_restart
    logger.go:42: 19:38:31 | 1-063_validate_statefulset_restart/3-check_image_after_change | test step completed 3-check_image_after_change
    logger.go:42: 19:38:31 | 1-063_validate_statefulset_restart | skipping kubernetes event logging
    logger.go:42: 19:38:31 | 1-063_validate_statefulset_restart | Deleting namespace: kuttl-test-assuring-ghoul
=== CONT  kuttl/harness/1-067_validate_redis_secure_comm_no_autotls_ha
    logger.go:42: 19:38:31 | 1-067_validate_redis_secure_comm_no_autotls_ha | Creating namespace: kuttl-test-helped-camel
    logger.go:42: 19:38:31 | 1-067_validate_redis_secure_comm_no_autotls_ha/1-install | starting test step 1-install
I0607 19:38:33.989933  250633 request.go:655] Throttling request took 1.550797177s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/node.k8s.io/v1?timeout=32s
    logger.go:42: 19:38:37 | 1-067_validate_redis_secure_comm_no_autotls_ha/1-install | ArgoCD:kuttl-test-helped-camel/argocd created
=== CONT  kuttl/harness/1-064_validate_security_contexts
    logger.go:42: 19:38:39 | 1-064_validate_security_contexts/1-install | test step completed 1-install
    logger.go:42: 19:38:39 | 1-064_validate_security_contexts | skipping kubernetes event logging
    logger.go:42: 19:38:39 | 1-064_validate_security_contexts | Deleting namespace: kuttl-test-nearby-toad
=== CONT  kuttl/harness/1-069_validate_redis_secure_comm_autotls_ha
    logger.go:42: 19:38:39 | 1-069_validate_redis_secure_comm_autotls_ha | Creating namespace: kuttl-test-famous-swan
    logger.go:42: 19:38:40 | 1-069_validate_redis_secure_comm_autotls_ha/1-install | starting test step 1-install
I0607 19:38:44.034450  250633 request.go:655] Throttling request took 3.194827102s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/postgresql.k8s.enterprisedb.io/v1?timeout=32s
    logger.go:42: 19:38:45 | 1-069_validate_redis_secure_comm_autotls_ha/1-install | ArgoCD:kuttl-test-famous-swan/argocd created
=== CONT  kuttl/harness/1-080_validate_regex_support_argocd_rbac
    logger.go:42: 19:44:22 | 1-080_validate_regex_support_argocd_rbac/2-modify-argocd | test step failed 2-modify-argocd
    case.go:361: failed in step 2-modify-argocd
    case.go:363: --- ConfigMap:kuttl-test-perfect-beagle/argocd-rbac-cm
        +++ ConfigMap:kuttl-test-perfect-beagle/argocd-rbac-cm
        @@ -1,8 +1,42 @@
         apiVersion: v1
         data:
        -  policy.matchMode: regex
        +  policy.csv: ""
        +  policy.default: role:readonly
        +  scopes: '[groups]'
         kind: ConfigMap
         metadata:
        +  labels:
        +    app.kubernetes.io/managed-by: example-argocd
        +    app.kubernetes.io/name: argocd-rbac-cm
        +    app.kubernetes.io/part-of: argocd
        +  managedFields:
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:data:
        +        .: {}
        +        f:policy.csv: {}
        +        f:policy.default: {}
        +        f:scopes: {}
        +      f:metadata:
        +        f:labels:
        +          .: {}
        +          f:app.kubernetes.io/managed-by: {}
        +          f:app.kubernetes.io/name: {}
        +          f:app.kubernetes.io/part-of: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"ba085c3f-b33d-4756-a0b2-5fd0e6200600"}: {}
        +    manager: manager
        +    operation: Update
        +    time: "2023-06-07T13:53:44Z"
           name: argocd-rbac-cm
           namespace: kuttl-test-perfect-beagle
        +  ownerReferences:
        +  - apiVersion: argoproj.io/v1alpha1
        +    blockOwnerDeletion: true
        +    controller: true
        +    kind: ArgoCD
        +    name: example-argocd
        +    uid: ba085c3f-b33d-4756-a0b2-5fd0e6200600
         
        
    case.go:363: resource ConfigMap:kuttl-test-perfect-beagle/argocd-rbac-cm: .data.policy.matchMode: key is missing from map
    logger.go:42: 19:44:22 | 1-080_validate_regex_support_argocd_rbac | skipping kubernetes event logging
    logger.go:42: 19:44:23 | 1-080_validate_regex_support_argocd_rbac | Deleting namespace: kuttl-test-perfect-beagle
=== CONT  kuttl/harness/1-063_validate_dex_liveness_probe
    logger.go:42: 19:44:23 | 1-063_validate_dex_liveness_probe | Creating namespace: kuttl-test-refined-dogfish
    logger.go:42: 19:44:23 | 1-063_validate_dex_liveness_probe/1- | starting test step 1-
I0607 19:44:25.714754  250633 request.go:655] Throttling request took 1.049328251s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/rhoas.redhat.com/v1alpha1?timeout=32s
    logger.go:42: 19:44:29 | 1-063_validate_dex_liveness_probe/1- | test step completed 1-
    logger.go:42: 19:44:29 | 1-063_validate_dex_liveness_probe | skipping kubernetes event logging
    logger.go:42: 19:44:29 | 1-063_validate_dex_liveness_probe | Deleting namespace: kuttl-test-refined-dogfish
=== CONT  kuttl/harness/1-070_validate_config_management_plugin
    logger.go:42: 19:44:30 | 1-070_validate_config_management_plugin | Creating namespace: kuttl-test-alert-tahr
    logger.go:42: 19:44:30 | 1-070_validate_config_management_plugin/1-install | starting test step 1-install
    logger.go:42: 19:44:36 | 1-070_validate_config_management_plugin/1-install | Namespace:/argocd created
    logger.go:42: 19:44:36 | 1-070_validate_config_management_plugin/1-install | ConfigMap:argocd/cmp-plugin created
    logger.go:42: 19:44:37 | 1-070_validate_config_management_plugin/1-install | ArgoCD:argocd/argocd created
    logger.go:42: 19:45:11 | 1-070_validate_config_management_plugin/1-install | test step completed 1-install
    logger.go:42: 19:45:11 | 1-070_validate_config_management_plugin/2-create-app | starting test step 2-create-app
I0607 19:45:13.011257  250633 request.go:655] Throttling request took 1.000659464s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/metal3.io/v1alpha1?timeout=32s
    logger.go:42: 19:45:16 | 1-070_validate_config_management_plugin/2-create-app | Application:argocd/guestbook created
    logger.go:42: 19:45:21 | 1-070_validate_config_management_plugin/2-create-app | test step completed 2-create-app
    logger.go:42: 19:45:21 | 1-070_validate_config_management_plugin/99-delete | starting test step 99-delete
I0607 19:46:01.651282  250633 request.go:655] Throttling request took 1.000649079s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/autoscaling/v2?timeout=32s
    logger.go:42: 19:46:04 | 1-070_validate_config_management_plugin/99-delete | test step completed 99-delete
    logger.go:42: 19:46:04 | 1-070_validate_config_management_plugin | skipping kubernetes event logging
    logger.go:42: 19:46:04 | 1-070_validate_config_management_plugin | Deleting namespace: kuttl-test-alert-tahr
=== CONT  kuttl/harness/1-068_validate_redis_secure_comm_autotls_no_ha
    logger.go:42: 19:46:05 | 1-068_validate_redis_secure_comm_autotls_no_ha | Creating namespace: kuttl-test-verified-guppy
    logger.go:42: 19:46:05 | 1-068_validate_redis_secure_comm_autotls_no_ha/1-install | starting test step 1-install
    logger.go:42: 19:46:11 | 1-068_validate_redis_secure_comm_autotls_no_ha/1-install | ArgoCD:kuttl-test-verified-guppy/argocd created
    logger.go:42: 19:46:46 | 1-068_validate_redis_secure_comm_autotls_no_ha/1-install | test step completed 1-install
    logger.go:42: 19:46:46 | 1-068_validate_redis_secure_comm_autotls_no_ha/2-enable_autotls | starting test step 2-enable_autotls
    logger.go:42: 19:46:46 | 1-068_validate_redis_secure_comm_autotls_no_ha/2-enable_autotls | running command: [sh -c set -e
        
        oc patch argocds.argoproj.io argocd --type=merge -p '{"spec":{"redis":{"autotls":"openshift"}}}' -n $NAMESPACE
        ]
    logger.go:42: 19:46:49 | 1-068_validate_redis_secure_comm_autotls_no_ha/2-enable_autotls | argocd.argoproj.io/argocd patched
I0607 19:46:51.021307  250633 request.go:655] Throttling request took 1.049771605s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/pxc.percona.com/v1?timeout=32s
    logger.go:42: 19:47:23 | 1-068_validate_redis_secure_comm_autotls_no_ha/2-enable_autotls | test step completed 2-enable_autotls
    logger.go:42: 19:47:23 | 1-068_validate_redis_secure_comm_autotls_no_ha/3-check_secret | starting test step 3-check_secret
    logger.go:42: 19:47:23 | 1-068_validate_redis_secure_comm_autotls_no_ha/3-check_secret | running command: [sh -c set -e
        secret_type="$(oc get secrets argocd-operator-redis-tls -n $NAMESPACE --template '{{.type}}')"
        secret_len="$(oc get secrets argocd-operator-redis-tls -n $NAMESPACE --template '{{len .data}}')"
        expected_secret_type="kubernetes.io/tls"
        expected_secret_len=2
        
        if test ${secret_type} != ${expected_secret_type}; then
          echo "argocd-operator-redis-tls secret type is ${secret_type} and should be ${expected_secret_type}"
          exit 1
        fi
        if test ${secret_len} != ${expected_secret_len}; then
          echo "argocd-operator-redis-tls secret length is ${secret_len} and should be ${expected_secret_len}"
          exit 1
        fi
        ]
I0607 19:47:26.688605  250633 request.go:655] Throttling request took 1.001009857s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/apiextensions.k8s.io/v1?timeout=32s
    logger.go:42: 19:47:29 | 1-068_validate_redis_secure_comm_autotls_no_ha/3-check_secret | test step completed 3-check_secret
    logger.go:42: 19:47:29 | 1-068_validate_redis_secure_comm_autotls_no_ha/4- | starting test step 4-
    logger.go:42: 19:47:37 | 1-068_validate_redis_secure_comm_autotls_no_ha/4- | test step completed 4-
    logger.go:42: 19:47:37 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | starting test step 5-check_crt_files
    logger.go:42: 19:47:37 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | running command: [sh -c set -e
        oc exec -i $(oc get pod -l app.kubernetes.io/name=argocd-server -n $NAMESPACE -o=NAME) -n $NAMESPACE -- ls /app/config/server/tls/redis/tls.crt
        oc exec -i $(oc get pod -l app.kubernetes.io/name=argocd-repo-server -n $NAMESPACE -o=NAME) -n $NAMESPACE -- ls /app/config/reposerver/tls/redis/tls.crt
        oc exec -i $(oc get pod -l app.kubernetes.io/name=argocd-redis -n $NAMESPACE -o=NAME) -n $NAMESPACE -- ls /app/config/redis/tls/tls.crt
        oc exec -i $(oc get pod -l app.kubernetes.io/name=argocd-application-controller -n $NAMESPACE -o=NAME) -n $NAMESPACE -- ls /app/config/controller/tls/redis/tls.crt
        ]
    logger.go:42: 19:47:41 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | /app/config/server/tls/redis/tls.crt
    logger.go:42: 19:47:45 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | /app/config/reposerver/tls/redis/tls.crt
    logger.go:42: 19:47:49 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | /app/config/redis/tls/tls.crt
    logger.go:42: 19:47:53 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | /app/config/controller/tls/redis/tls.crt
I0607 19:47:55.077052  250633 request.go:655] Throttling request took 1.048440415s, request: GET:https://api.rhoms-4.12-060704.dev.openshiftappsvc.org:6443/apis/resolution.tekton.dev/v1beta1?timeout=32s
    logger.go:42: 19:47:58 | 1-068_validate_redis_secure_comm_autotls_no_ha/5-check_crt_files | test step completed 5-check_crt_files
    logger.go:42: 19:47:58 | 1-068_validate_redis_secure_comm_autotls_no_ha | skipping kubernetes event logging
    logger.go:42: 19:47:58 | 1-068_validate_redis_secure_comm_autotls_no_ha | Deleting namespace: kuttl-test-verified-guppy
=== CONT  kuttl/harness/1-079_validate_vars_for_notificaitons
    logger.go:42: 19:53:37 | 1-079_validate_vars_for_notificaitons/2-modify-argocd | test step failed 2-modify-argocd
    case.go:361: failed in step 2-modify-argocd
    case.go:363: --- ArgoCD:kuttl-test-adapting-antelope/example-argocd
        +++ ArgoCD:kuttl-test-adapting-antelope/example-argocd
        @@ -1,16 +1,149 @@
         apiVersion: argoproj.io/v1alpha1
         kind: ArgoCD
         metadata:
        +  finalizers:
        +  - argoproj.io/finalizer
        +  managedFields:
        +  - apiVersion: argoproj.io/v1alpha1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:spec:
        +        .: {}
        +        f:notifications:
        +          .: {}
        +          f:enabled: {}
        +        f:server:
        +          .: {}
        +          f:route:
        +            .: {}
        +            f:enabled: {}
        +    manager: kubectl-kuttl
        +    operation: Update
        +    time: "2023-06-07T14:02:44Z"
        +  - apiVersion: argoproj.io/v1alpha1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:finalizers:
        +          .: {}
        +          v:"argoproj.io/finalizer": {}
        +      f:spec:
        +        f:controller:
        +          .: {}
        +          f:processors: {}
        +          f:sharding: {}
        +        f:grafana:
        +          .: {}
        +          f:enabled: {}
        +          f:ingress:
        +            .: {}
        +            f:enabled: {}
        +          f:route:
        +            .: {}
        +            f:enabled: {}
        +        f:ha:
        +          .: {}
        +          f:enabled: {}
        +        f:initialSSHKnownHosts: {}
        +        f:prometheus:
        +          .: {}
        +          f:enabled: {}
        +          f:ingress:
        +            .: {}
        +            f:enabled: {}
        +          f:route:
        +            .: {}
        +            f:enabled: {}
        +        f:rbac: {}
        +        f:redis: {}
        +        f:repo: {}
        +        f:server:
        +          f:autoscale:
        +            .: {}
        +            f:enabled: {}
        +          f:grpc:
        +            .: {}
        +            f:ingress:
        +              .: {}
        +              f:enabled: {}
        +          f:ingress:
        +            .: {}
        +            f:enabled: {}
        +          f:service:
        +            .: {}
        +            f:type: {}
        +        f:tls:
        +          .: {}
        +          f:ca: {}
        +    manager: manager
        +    operation: Update
        +    time: "2023-06-07T14:02:44Z"
        +  - apiVersion: argoproj.io/v1alpha1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:status:
        +        .: {}
        +        f:applicationController: {}
        +        f:dex: {}
        +        f:host: {}
        +        f:notificationsController: {}
        +        f:phase: {}
        +        f:redis: {}
        +        f:repo: {}
        +        f:server: {}
        +        f:ssoConfig: {}
        +    manager: manager
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T14:03:16Z"
           name: example-argocd
           namespace: kuttl-test-adapting-antelope
         spec:
        +  controller:
        +    processors: {}
        +    sharding: {}
        +  grafana:
        +    enabled: false
        +    ingress:
        +      enabled: false
        +    route:
        +      enabled: false
        +  ha:
        +    enabled: false
        +  initialSSHKnownHosts: {}
           notifications:
             enabled: true
        -    env:
        -    - name: foo
        -      value: bar
        +  prometheus:
        +    enabled: false
        +    ingress:
        +      enabled: false
        +    route:
        +      enabled: false
        +  rbac: {}
        +  redis: {}
        +  repo: {}
        +  server:
        +    autoscale:
        +      enabled: false
        +    grpc:
        +      ingress:
        +        enabled: false
        +    ingress:
        +      enabled: false
        +    route:
        +      enabled: true
        +    service:
        +      type: ""
        +  tls:
        +    ca: {}
         status:
        +  applicationController: Running
        +  dex: Unknown
        +  host: example-argocd-server-kuttl-test-adapting-antelope.apps.rhoms-4.12-060704.dev.openshiftappsvc.org
           notificationsController: Running
           phase: Available
        +  redis: Running
        +  repo: Running
           server: Running
        +  ssoConfig: Unknown
         
        
    case.go:363: resource ArgoCD:kuttl-test-adapting-antelope/example-argocd: .spec.notifications.env: key is missing from map
    case.go:363: --- Deployment:kuttl-test-adapting-antelope/example-argocd-notifications-controller
        +++ Deployment:kuttl-test-adapting-antelope/example-argocd-notifications-controller
        @@ -1,13 +1,160 @@
         apiVersion: apps/v1
         kind: Deployment
         metadata:
        +  labels:
        +    app.kubernetes.io/component: controller
        +    app.kubernetes.io/managed-by: example-argocd
        +    app.kubernetes.io/name: example-argocd-notifications-controller
        +    app.kubernetes.io/part-of: argocd
        +  managedFields:
        +  - apiVersion: apps/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:labels:
        +          .: {}
        +          f:app.kubernetes.io/component: {}
        +          f:app.kubernetes.io/managed-by: {}
        +          f:app.kubernetes.io/name: {}
        +          f:app.kubernetes.io/part-of: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"11e0bb3c-8498-439b-842d-cf6058d69c74"}: {}
        +      f:spec:
        +        f:progressDeadlineSeconds: {}
        +        f:replicas: {}
        +        f:revisionHistoryLimit: {}
        +        f:selector: {}
        +        f:strategy:
        +          f:type: {}
        +        f:template:
        +          f:metadata:
        +            f:labels:
        +              .: {}
        +              f:app.kubernetes.io/name: {}
        +          f:spec:
        +            f:containers:
        +              k:{"name":"argocd-notifications-controller"}:
        +                .: {}
        +                f:command: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:livenessProbe:
        +                  .: {}
        +                  f:failureThreshold: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:tcpSocket:
        +                    .: {}
        +                    f:port: {}
        +                  f:timeoutSeconds: {}
        +                f:name: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/reposerver/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/app/config/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                f:workingDir: {}
        +            f:dnsPolicy: {}
        +            f:nodeSelector: {}
        +            f:restartPolicy: {}
        +            f:schedulerName: {}
        +            f:securityContext:
        +              .: {}
        +              f:runAsNonRoot: {}
        +              f:seccompProfile:
        +                .: {}
        +                f:type: {}
        +            f:serviceAccount: {}
        +            f:serviceAccountName: {}
        +            f:terminationGracePeriodSeconds: {}
        +            f:volumes:
        +              .: {}
        +              k:{"name":"argocd-repo-server-tls"}:
        +                .: {}
        +                f:name: {}
        +                f:secret:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:optional: {}
        +                  f:secretName: {}
        +              k:{"name":"tls-certs"}:
        +                .: {}
        +                f:configMap:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:name: {}
        +                f:name: {}
        +    manager: manager
        +    operation: Update
        +    time: "2023-06-07T14:02:45Z"
        +  - apiVersion: apps/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:annotations:
        +          .: {}
        +          f:deployment.kubernetes.io/revision: {}
        +      f:status:
        +        f:availableReplicas: {}
        +        f:conditions:
        +          .: {}
        +          k:{"type":"Available"}:
        +            .: {}
        +            f:lastTransitionTime: {}
        +            f:lastUpdateTime: {}
        +            f:message: {}
        +            f:reason: {}
        +            f:status: {}
        +            f:type: {}
        +          k:{"type":"Progressing"}:
        +            .: {}
        +            f:lastTransitionTime: {}
        +            f:lastUpdateTime: {}
        +            f:message: {}
        +            f:reason: {}
        +            f:status: {}
        +            f:type: {}
        +        f:observedGeneration: {}
        +        f:readyReplicas: {}
        +        f:replicas: {}
        +        f:updatedReplicas: {}
        +    manager: kube-controller-manager
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T14:02:48Z"
           name: example-argocd-notifications-controller
           namespace: kuttl-test-adapting-antelope
        +  ownerReferences:
        +  - apiVersion: argoproj.io/v1alpha1
        +    blockOwnerDeletion: true
        +    controller: true
        +    kind: ArgoCD
        +    name: example-argocd
        +    uid: 11e0bb3c-8498-439b-842d-cf6058d69c74
         spec:
        +  progressDeadlineSeconds: 600
           replicas: 1
        +  revisionHistoryLimit: 10
           selector:
             matchLabels:
               app.kubernetes.io/name: example-argocd-notifications-controller
        +  strategy:
        +    type: Recreate
           template:
             metadata:
               creationTimestamp: null
        @@ -15,10 +162,73 @@
                 app.kubernetes.io/name: example-argocd-notifications-controller
             spec:
               containers:
        -      - env:
        -        - name: foo
        -          value: bar
        +      - command:
        +        - argocd-notifications
        +        - --loglevel
        +        - info
        +        image: brew.registry.redhat.io/rh-osbs/openshift-gitops-1-argocd-rhel8:v99.9.0-95
        +        imagePullPolicy: Always
        +        livenessProbe:
        +          failureThreshold: 3
        +          periodSeconds: 10
        +          successThreshold: 1
        +          tcpSocket:
        +            port: 9001
        +          timeoutSeconds: 1
        +        name: argocd-notifications-controller
                 resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /app/config/tls
        +          name: tls-certs
        +        - mountPath: /app/config/reposerver/tls
        +          name: argocd-repo-server-tls
        +        workingDir: /app
        +      dnsPolicy: ClusterFirst
        +      nodeSelector:
        +        kubernetes.io/os: linux
        +      restartPolicy: Always
        +      schedulerName: default-scheduler
        +      securityContext:
        +        runAsNonRoot: true
        +        seccompProfile:
        +          type: RuntimeDefault
        +      serviceAccount: example-argocd-argocd-notifications-controller
        +      serviceAccountName: example-argocd-argocd-notifications-controller
        +      terminationGracePeriodSeconds: 30
        +      volumes:
        +      - configMap:
        +          defaultMode: 420
        +          name: argocd-tls-certs-cm
        +        name: tls-certs
        +      - name: argocd-repo-server-tls
        +        secret:
        +          defaultMode: 420
        +          optional: true
        +          secretName: argocd-repo-server-tls
         status:
        +  availableReplicas: 1
        +  conditions:
        +  - lastTransitionTime: "2023-06-07T14:02:48Z"
        +    lastUpdateTime: "2023-06-07T14:02:48Z"
        +    message: Deployment has minimum availability.
        +    reason: MinimumReplicasAvailable
        +    status: "True"
        +    type: Available
        +  - lastTransitionTime: "2023-06-07T14:02:45Z"
        +    lastUpdateTime: "2023-06-07T14:02:48Z"
        +    message: ReplicaSet "example-argocd-notifications-controller-d6bf9bbcd" has successfully progressed.
        +    reason: NewReplicaSetAvailable
        +    status: "True"
        +    type: Progressing
        +  observedGeneration: 1
           readyReplicas: 1
        +  replicas: 1
        +  updatedReplicas: 1
         
        
    case.go:363: resource Deployment:kuttl-test-adapting-antelope/example-argocd-notifications-controller: .spec.template.spec.containers.env: key is missing from map
    logger.go:42: 19:53:37 | 1-079_validate_vars_for_notificaitons | skipping kubernetes event logging
    logger.go:42: 19:53:37 | 1-079_validate_vars_for_notificaitons | Deleting namespace: kuttl-test-adapting-antelope
=== CONT  kuttl/harness/1-071_validate_SCC_HA
    logger.go:42: 19:56:40 | 1-071_validate_SCC_HA/3-modify-argocd | test step failed 3-modify-argocd
    case.go:361: failed in step 3-modify-argocd
    case.go:363: --- Pod:kuttl-test-close-llama/argocd-redis-ha-server-2
        +++ Pod:kuttl-test-close-llama/argocd-redis-ha-server-2
        @@ -1,8 +1,470 @@
         apiVersion: v1
         kind: Pod
         metadata:
        +  annotations:
        +    checksum/init-config: 7128bfbb51eafaffe3c33b1b463e15f0cf6514cec570f9d9c4f2396f28c724ac
        +    openshift.io/scc: nonroot-v2
        +    seccomp.security.alpha.kubernetes.io/pod: runtime/default
        +  generateName: argocd-redis-ha-server-
        +  labels:
        +    app.kubernetes.io/name: argocd-redis-ha
        +    controller-revision-hash: argocd-redis-ha-server-797b4b5d4c
        +    statefulset.kubernetes.io/pod-name: argocd-redis-ha-server-2
        +  managedFields:
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:annotations:
        +          .: {}
        +          f:checksum/init-config: {}
        +        f:generateName: {}
        +        f:labels:
        +          .: {}
        +          f:app.kubernetes.io/name: {}
        +          f:controller-revision-hash: {}
        +          f:statefulset.kubernetes.io/pod-name: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"2de74ea4-afff-44f1-84d6-6d000d27b91e"}: {}
        +      f:spec:
        +        f:affinity:
        +          .: {}
        +          f:podAntiAffinity:
        +            .: {}
        +            f:requiredDuringSchedulingIgnoredDuringExecution: {}
        +        f:automountServiceAccountToken: {}
        +        f:containers:
        +          k:{"name":"redis"}:
        +            .: {}
        +            f:args: {}
        +            f:image: {}
        +            f:imagePullPolicy: {}
        +            f:livenessProbe:
        +              .: {}
        +              f:exec:
        +                .: {}
        +                f:command: {}
        +              f:failureThreshold: {}
        +              f:initialDelaySeconds: {}
        +              f:periodSeconds: {}
        +              f:successThreshold: {}
        +              f:timeoutSeconds: {}
        +            f:name: {}
        +            f:ports:
        +              .: {}
        +              k:{"containerPort":6379,"protocol":"TCP"}:
        +                .: {}
        +                f:containerPort: {}
        +                f:name: {}
        +                f:protocol: {}
        +            f:readinessProbe:
        +              .: {}
        +              f:exec:
        +                .: {}
        +                f:command: {}
        +              f:failureThreshold: {}
        +              f:initialDelaySeconds: {}
        +              f:periodSeconds: {}
        +              f:successThreshold: {}
        +              f:timeoutSeconds: {}
        +            f:resources: {}
        +            f:securityContext:
        +              .: {}
        +              f:allowPrivilegeEscalation: {}
        +              f:capabilities:
        +                .: {}
        +                f:drop: {}
        +              f:runAsNonRoot: {}
        +            f:terminationMessagePath: {}
        +            f:terminationMessagePolicy: {}
        +            f:volumeMounts:
        +              .: {}
        +              k:{"mountPath":"/app/config/redis/tls"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +              k:{"mountPath":"/data"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +              k:{"mountPath":"/health"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +          k:{"name":"sentinel"}:
        +            .: {}
        +            f:args: {}
        +            f:image: {}
        +            f:imagePullPolicy: {}
        +            f:livenessProbe:
        +              .: {}
        +              f:exec:
        +                .: {}
        +                f:command: {}
        +              f:failureThreshold: {}
        +              f:initialDelaySeconds: {}
        +              f:periodSeconds: {}
        +              f:successThreshold: {}
        +              f:timeoutSeconds: {}
        +            f:name: {}
        +            f:ports:
        +              .: {}
        +              k:{"containerPort":26379,"protocol":"TCP"}:
        +                .: {}
        +                f:containerPort: {}
        +                f:name: {}
        +                f:protocol: {}
        +            f:readinessProbe:
        +              .: {}
        +              f:exec:
        +                .: {}
        +                f:command: {}
        +              f:failureThreshold: {}
        +              f:initialDelaySeconds: {}
        +              f:periodSeconds: {}
        +              f:successThreshold: {}
        +              f:timeoutSeconds: {}
        +            f:resources: {}
        +            f:securityContext:
        +              .: {}
        +              f:allowPrivilegeEscalation: {}
        +              f:capabilities:
        +                .: {}
        +                f:drop: {}
        +              f:runAsNonRoot: {}
        +            f:terminationMessagePath: {}
        +            f:terminationMessagePolicy: {}
        +            f:volumeMounts:
        +              .: {}
        +              k:{"mountPath":"/app/config/redis/tls"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +              k:{"mountPath":"/data"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +              k:{"mountPath":"/health"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +        f:dnsPolicy: {}
        +        f:enableServiceLinks: {}
        +        f:hostname: {}
        +        f:initContainers:
        +          .: {}
        +          k:{"name":"config-init"}:
        +            .: {}
        +            f:args: {}
        +            f:env:
        +              .: {}
        +              k:{"name":"SENTINEL_ID_0"}:
        +                .: {}
        +                f:name: {}
        +                f:value: {}
        +              k:{"name":"SENTINEL_ID_1"}:
        +                .: {}
        +                f:name: {}
        +                f:value: {}
        +              k:{"name":"SENTINEL_ID_2"}:
        +                .: {}
        +                f:name: {}
        +                f:value: {}
        +            f:image: {}
        +            f:imagePullPolicy: {}
        +            f:name: {}
        +            f:resources: {}
        +            f:securityContext:
        +              .: {}
        +              f:allowPrivilegeEscalation: {}
        +              f:capabilities:
        +                .: {}
        +                f:drop: {}
        +              f:runAsNonRoot: {}
        +            f:terminationMessagePath: {}
        +            f:terminationMessagePolicy: {}
        +            f:volumeMounts:
        +              .: {}
        +              k:{"mountPath":"/app/config/redis/tls"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +              k:{"mountPath":"/data"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +              k:{"mountPath":"/readonly-config"}:
        +                .: {}
        +                f:mountPath: {}
        +                f:name: {}
        +                f:readOnly: {}
        +        f:nodeSelector: {}
        +        f:restartPolicy: {}
        +        f:schedulerName: {}
        +        f:securityContext:
        +          .: {}
        +          f:fsGroup: {}
        +          f:runAsNonRoot: {}
        +          f:runAsUser: {}
        +          f:seccompProfile:
        +            .: {}
        +            f:type: {}
        +        f:serviceAccount: {}
        +        f:serviceAccountName: {}
        +        f:subdomain: {}
        +        f:terminationGracePeriodSeconds: {}
        +        f:volumes:
        +          .: {}
        +          k:{"name":"argocd-operator-redis-tls"}:
        +            .: {}
        +            f:name: {}
        +            f:secret:
        +              .: {}
        +              f:defaultMode: {}
        +              f:optional: {}
        +              f:secretName: {}
        +          k:{"name":"config"}:
        +            .: {}
        +            f:configMap:
        +              .: {}
        +              f:defaultMode: {}
        +              f:name: {}
        +            f:name: {}
        +          k:{"name":"data"}:
        +            .: {}
        +            f:emptyDir: {}
        +            f:name: {}
        +          k:{"name":"health"}:
        +            .: {}
        +            f:configMap:
        +              .: {}
        +              f:defaultMode: {}
        +              f:name: {}
        +            f:name: {}
        +    manager: kube-controller-manager
        +    operation: Update
        +    time: "2023-06-07T14:14:55Z"
        +  - apiVersion: v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:status:
        +        f:conditions:
        +          .: {}
        +          k:{"type":"PodScheduled"}:
        +            .: {}
        +            f:lastProbeTime: {}
        +            f:lastTransitionTime: {}
        +            f:message: {}
        +            f:reason: {}
        +            f:status: {}
        +            f:type: {}
        +    manager: kube-scheduler
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T14:14:55Z"
           name: argocd-redis-ha-server-2
           namespace: kuttl-test-close-llama
        +  ownerReferences:
        +  - apiVersion: apps/v1
        +    blockOwnerDeletion: true
        +    controller: true
        +    kind: StatefulSet
        +    name: argocd-redis-ha-server
        +    uid: 2de74ea4-afff-44f1-84d6-6d000d27b91e
        +spec:
        +  affinity:
        +    podAntiAffinity:
        +      requiredDuringSchedulingIgnoredDuringExecution:
        +      - labelSelector:
        +          matchLabels:
        +            app.kubernetes.io/name: argocd-redis-ha
        +        topologyKey: kubernetes.io/hostname
        +  automountServiceAccountToken: false
        +  containers:
        +  - args:
        +    - redis-server
        +    - /data/conf/redis.conf
        +    image: registry.redhat.io/rhel8/redis-6:1-110
        +    imagePullPolicy: IfNotPresent
        +    livenessProbe:
        +      exec:
        +        command:
        +        - sh
        +        - -c
        +        - /health/redis_liveness.sh
        +      failureThreshold: 5
        +      initialDelaySeconds: 30
        +      periodSeconds: 15
        +      successThreshold: 1
        +      timeoutSeconds: 15
        +    name: redis
        +    ports:
        +    - containerPort: 6379
        +      name: redis
        +      protocol: TCP
        +    readinessProbe:
        +      exec:
        +        command:
        +        - sh
        +        - -c
        +        - /health/redis_readiness.sh
        +      failureThreshold: 5
        +      initialDelaySeconds: 30
        +      periodSeconds: 15
        +      successThreshold: 1
        +      timeoutSeconds: 15
        +    resources: {}
        +    securityContext:
        +      allowPrivilegeEscalation: false
        +      capabilities:
        +        drop:
        +        - ALL
        +      runAsNonRoot: true
        +    terminationMessagePath: /dev/termination-log
        +    terminationMessagePolicy: File
        +    volumeMounts:
        +    - mountPath: /data
        +      name: data
        +    - mountPath: /health
        +      name: health
        +    - mountPath: /app/config/redis/tls
        +      name: argocd-operator-redis-tls
        +  - args:
        +    - redis-sentinel
        +    - /data/conf/sentinel.conf
        +    image: registry.redhat.io/rhel8/redis-6:1-110
        +    imagePullPolicy: IfNotPresent
        +    livenessProbe:
        +      exec:
        +        command:
        +        - sh
        +        - -c
        +        - /health/sentinel_liveness.sh
        +      failureThreshold: 5
        +      initialDelaySeconds: 30
        +      periodSeconds: 15
        +      successThreshold: 1
        +      timeoutSeconds: 15
        +    name: sentinel
        +    ports:
        +    - containerPort: 26379
        +      name: sentinel
        +      protocol: TCP
        +    readinessProbe:
        +      exec:
        +        command:
        +        - sh
        +        - -c
        +        - /health/sentinel_liveness.sh
        +      failureThreshold: 5
        +      initialDelaySeconds: 30
        +      periodSeconds: 15
        +      successThreshold: 1
        +      timeoutSeconds: 15
        +    resources: {}
        +    securityContext:
        +      allowPrivilegeEscalation: false
        +      capabilities:
        +        drop:
        +        - ALL
        +      runAsNonRoot: true
        +    terminationMessagePath: /dev/termination-log
        +    terminationMessagePolicy: File
        +    volumeMounts:
        +    - mountPath: /data
        +      name: data
        +    - mountPath: /health
        +      name: health
        +    - mountPath: /app/config/redis/tls
        +      name: argocd-operator-redis-tls
        +  dnsPolicy: ClusterFirst
        +  enableServiceLinks: true
        +  hostname: argocd-redis-ha-server-2
        +  imagePullSecrets:
        +  - name: argocd-argocd-redis-ha-dockercfg-gj479
        +  initContainers:
        +  - args:
        +    - sh
        +    - /readonly-config/init.sh
        +    env:
        +    - name: SENTINEL_ID_0
        +      value: 3c0d9c0320bb34888c2df5757c718ce6ca992ce6
        +    - name: SENTINEL_ID_1
        +      value: 40000915ab58c3fa8fd888fb8b24711944e6cbb4
        +    - name: SENTINEL_ID_2
        +      value: 2bbec7894d954a8af3bb54d13eaec53cb024e2ca
        +    image: registry.redhat.io/rhel8/redis-6:1-110
        +    imagePullPolicy: IfNotPresent
        +    name: config-init
        +    resources: {}
        +    securityContext:
        +      allowPrivilegeEscalation: false
        +      capabilities:
        +        drop:
        +        - ALL
        +      runAsNonRoot: true
        +    terminationMessagePath: /dev/termination-log
        +    terminationMessagePolicy: File
        +    volumeMounts:
        +    - mountPath: /readonly-config
        +      name: config
        +      readOnly: true
        +    - mountPath: /data
        +      name: data
        +    - mountPath: /app/config/redis/tls
        +      name: argocd-operator-redis-tls
        +  nodeSelector:
        +    kubernetes.io/os: linux
        +  preemptionPolicy: PreemptLowerPriority
        +  priority: 0
        +  restartPolicy: Always
        +  schedulerName: default-scheduler
        +  securityContext:
        +    fsGroup: 1000
        +    runAsNonRoot: true
        +    runAsUser: 1000
        +    seLinuxOptions:
        +      level: s0:c37,c9
        +    seccompProfile:
        +      type: RuntimeDefault
        +  serviceAccount: argocd-argocd-redis-ha
        +  serviceAccountName: argocd-argocd-redis-ha
        +  subdomain: argocd-redis-ha
        +  terminationGracePeriodSeconds: 60
        +  tolerations:
        +  - effect: NoExecute
        +    key: node.kubernetes.io/not-ready
        +    operator: Exists
        +    tolerationSeconds: 300
        +  - effect: NoExecute
        +    key: node.kubernetes.io/unreachable
        +    operator: Exists
        +    tolerationSeconds: 300
        +  volumes:
        +  - configMap:
        +      defaultMode: 420
        +      name: argocd-redis-ha-configmap
        +    name: config
        +  - configMap:
        +      defaultMode: 493
        +      name: argocd-redis-ha-health-configmap
        +    name: health
        +  - emptyDir: {}
        +    name: data
        +  - name: argocd-operator-redis-tls
        +    secret:
        +      defaultMode: 420
        +      optional: true
        +      secretName: argocd-operator-redis-tls
         status:
        -  phase: Running
        +  conditions:
        +  - lastProbeTime: null
        +    lastTransitionTime: "2023-06-07T14:14:55Z"
        +    message: '0/4 nodes are available: 2 node(s) didn''t match pod anti-affinity rules, 2 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/4 nodes are available: 2 No preemption victims found for incoming pod, 2 Preemption is not helpful for scheduling.'
        +    reason: Unschedulable
        +    status: "False"
        +    type: PodScheduled
        +  phase: Pending
        +  qosClass: BestEffort
         
        
    case.go:363: resource Pod:kuttl-test-close-llama/argocd-redis-ha-server-2: .status.phase: value mismatch, expected: Running != actual: Pending
    logger.go:42: 19:56:40 | 1-071_validate_SCC_HA | skipping kubernetes event logging
    logger.go:42: 19:56:41 | 1-071_validate_SCC_HA | Deleting namespace: kuttl-test-close-llama
=== CONT  kuttl/harness/1-067_validate_redis_secure_comm_no_autotls_ha
    logger.go:42: 19:58:37 | 1-067_validate_redis_secure_comm_no_autotls_ha/1-install | test step failed 1-install
    case.go:361: failed in step 1-install
    case.go:363: --- StatefulSet:kuttl-test-helped-camel/argocd-redis-ha-server
        +++ StatefulSet:kuttl-test-helped-camel/argocd-redis-ha-server
        @@ -1,9 +1,474 @@
         apiVersion: apps/v1
         kind: StatefulSet
         metadata:
        +  labels:
        +    app.kubernetes.io/component: redis
        +    app.kubernetes.io/managed-by: argocd
        +    app.kubernetes.io/name: argocd-redis-ha-server
        +    app.kubernetes.io/part-of: argocd
        +  managedFields:
        +  - apiVersion: apps/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:labels:
        +          .: {}
        +          f:app.kubernetes.io/component: {}
        +          f:app.kubernetes.io/managed-by: {}
        +          f:app.kubernetes.io/name: {}
        +          f:app.kubernetes.io/part-of: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"d7daa139-efd2-46af-846d-9846e34dfa15"}: {}
        +      f:spec:
        +        f:podManagementPolicy: {}
        +        f:replicas: {}
        +        f:revisionHistoryLimit: {}
        +        f:selector: {}
        +        f:serviceName: {}
        +        f:template:
        +          f:metadata:
        +            f:annotations:
        +              .: {}
        +              f:checksum/init-config: {}
        +            f:labels:
        +              .: {}
        +              f:app.kubernetes.io/name: {}
        +          f:spec:
        +            f:affinity:
        +              .: {}
        +              f:podAntiAffinity:
        +                .: {}
        +                f:requiredDuringSchedulingIgnoredDuringExecution: {}
        +            f:automountServiceAccountToken: {}
        +            f:containers:
        +              k:{"name":"redis"}:
        +                .: {}
        +                f:args: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:livenessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:name: {}
        +                f:ports:
        +                  .: {}
        +                  k:{"containerPort":6379,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                f:readinessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                  f:runAsNonRoot: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/redis/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/data"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/health"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +              k:{"name":"sentinel"}:
        +                .: {}
        +                f:args: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:livenessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:name: {}
        +                f:ports:
        +                  .: {}
        +                  k:{"containerPort":26379,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                f:readinessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                  f:runAsNonRoot: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/redis/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/data"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/health"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +            f:dnsPolicy: {}
        +            f:initContainers:
        +              .: {}
        +              k:{"name":"config-init"}:
        +                .: {}
        +                f:args: {}
        +                f:env:
        +                  .: {}
        +                  k:{"name":"SENTINEL_ID_0"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SENTINEL_ID_1"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SENTINEL_ID_2"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:name: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                  f:runAsNonRoot: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/redis/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/data"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/readonly-config"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                    f:readOnly: {}
        +            f:nodeSelector: {}
        +            f:restartPolicy: {}
        +            f:schedulerName: {}
        +            f:securityContext:
        +              .: {}
        +              f:fsGroup: {}
        +              f:runAsNonRoot: {}
        +              f:runAsUser: {}
        +              f:seccompProfile:
        +                .: {}
        +                f:type: {}
        +            f:serviceAccount: {}
        +            f:serviceAccountName: {}
        +            f:terminationGracePeriodSeconds: {}
        +            f:volumes:
        +              .: {}
        +              k:{"name":"argocd-operator-redis-tls"}:
        +                .: {}
        +                f:name: {}
        +                f:secret:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:optional: {}
        +                  f:secretName: {}
        +              k:{"name":"config"}:
        +                .: {}
        +                f:configMap:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:name: {}
        +                f:name: {}
        +              k:{"name":"data"}:
        +                .: {}
        +                f:emptyDir: {}
        +                f:name: {}
        +              k:{"name":"health"}:
        +                .: {}
        +                f:configMap:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:name: {}
        +                f:name: {}
        +        f:updateStrategy:
        +          f:type: {}
        +    manager: manager
        +    operation: Update
        +    time: "2023-06-07T14:08:38Z"
        +  - apiVersion: apps/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:status:
        +        f:availableReplicas: {}
        +        f:collisionCount: {}
        +        f:currentReplicas: {}
        +        f:currentRevision: {}
        +        f:observedGeneration: {}
        +        f:readyReplicas: {}
        +        f:replicas: {}
        +        f:updateRevision: {}
        +        f:updatedReplicas: {}
        +    manager: kube-controller-manager
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T14:16:55Z"
           name: argocd-redis-ha-server
           namespace: kuttl-test-helped-camel
        +  ownerReferences:
        +  - apiVersion: argoproj.io/v1alpha1
        +    blockOwnerDeletion: true
        +    controller: true
        +    kind: ArgoCD
        +    name: argocd
        +    uid: d7daa139-efd2-46af-846d-9846e34dfa15
        +spec:
        +  podManagementPolicy: OrderedReady
        +  replicas: 3
        +  revisionHistoryLimit: 10
        +  selector:
        +    matchLabels:
        +      app.kubernetes.io/name: argocd-redis-ha
        +  serviceName: argocd-redis-ha
        +  template:
        +    metadata:
        +      annotations:
        +        checksum/init-config: 7128bfbb51eafaffe3c33b1b463e15f0cf6514cec570f9d9c4f2396f28c724ac
        +      creationTimestamp: null
        +      labels:
        +        app.kubernetes.io/name: argocd-redis-ha
        +    spec:
        +      affinity:
        +        podAntiAffinity:
        +          requiredDuringSchedulingIgnoredDuringExecution:
        +          - labelSelector:
        +              matchLabels:
        +                app.kubernetes.io/name: argocd-redis-ha
        +            topologyKey: kubernetes.io/hostname
        +      automountServiceAccountToken: false
        +      containers:
        +      - args:
        +        - redis-server
        +        - /data/conf/redis.conf
        +        image: registry.redhat.io/rhel8/redis-6:1-110
        +        imagePullPolicy: IfNotPresent
        +        livenessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/redis_liveness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        name: redis
        +        ports:
        +        - containerPort: 6379
        +          name: redis
        +          protocol: TCP
        +        readinessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/redis_readiness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +          runAsNonRoot: true
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /data
        +          name: data
        +        - mountPath: /health
        +          name: health
        +        - mountPath: /app/config/redis/tls
        +          name: argocd-operator-redis-tls
        +      - args:
        +        - redis-sentinel
        +        - /data/conf/sentinel.conf
        +        image: registry.redhat.io/rhel8/redis-6:1-110
        +        imagePullPolicy: IfNotPresent
        +        livenessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/sentinel_liveness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        name: sentinel
        +        ports:
        +        - containerPort: 26379
        +          name: sentinel
        +          protocol: TCP
        +        readinessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/sentinel_liveness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +          runAsNonRoot: true
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /data
        +          name: data
        +        - mountPath: /health
        +          name: health
        +        - mountPath: /app/config/redis/tls
        +          name: argocd-operator-redis-tls
        +      dnsPolicy: ClusterFirst
        +      initContainers:
        +      - args:
        +        - sh
        +        - /readonly-config/init.sh
        +        env:
        +        - name: SENTINEL_ID_0
        +          value: 3c0d9c0320bb34888c2df5757c718ce6ca992ce6
        +        - name: SENTINEL_ID_1
        +          value: 40000915ab58c3fa8fd888fb8b24711944e6cbb4
        +        - name: SENTINEL_ID_2
        +          value: 2bbec7894d954a8af3bb54d13eaec53cb024e2ca
        +        image: registry.redhat.io/rhel8/redis-6:1-110
        +        imagePullPolicy: IfNotPresent
        +        name: config-init
        +        resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +          runAsNonRoot: true
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /readonly-config
        +          name: config
        +          readOnly: true
        +        - mountPath: /data
        +          name: data
        +        - mountPath: /app/config/redis/tls
        +          name: argocd-operator-redis-tls
        +      nodeSelector:
        +        kubernetes.io/os: linux
        +      restartPolicy: Always
        +      schedulerName: default-scheduler
        +      securityContext:
        +        fsGroup: 1000
        +        runAsNonRoot: true
        +        runAsUser: 1000
        +        seccompProfile:
        +          type: RuntimeDefault
        +      serviceAccount: argocd-argocd-redis-ha
        +      serviceAccountName: argocd-argocd-redis-ha
        +      terminationGracePeriodSeconds: 60
        +      volumes:
        +      - configMap:
        +          defaultMode: 420
        +          name: argocd-redis-ha-configmap
        +        name: config
        +      - configMap:
        +          defaultMode: 493
        +          name: argocd-redis-ha-health-configmap
        +        name: health
        +      - emptyDir: {}
        +        name: data
        +      - name: argocd-operator-redis-tls
        +        secret:
        +          defaultMode: 420
        +          optional: true
        +          secretName: argocd-operator-redis-tls
        +  updateStrategy:
        +    type: RollingUpdate
         status:
        -  readyReplicas: 3
        +  availableReplicas: 2
        +  collisionCount: 0
        +  currentReplicas: 3
        +  currentRevision: argocd-redis-ha-server-797b4b5d4c
        +  observedGeneration: 1
        +  readyReplicas: 2
           replicas: 3
        +  updateRevision: argocd-redis-ha-server-797b4b5d4c
        +  updatedReplicas: 3
         
        
    case.go:363: resource StatefulSet:kuttl-test-helped-camel/argocd-redis-ha-server: .status.readyReplicas: value mismatch, expected: 3 != actual: 2
    logger.go:42: 19:58:37 | 1-067_validate_redis_secure_comm_no_autotls_ha | skipping kubernetes event logging
    logger.go:42: 19:58:37 | 1-067_validate_redis_secure_comm_no_autotls_ha | Deleting namespace: kuttl-test-helped-camel
=== CONT  kuttl/harness/1-069_validate_redis_secure_comm_autotls_ha
    logger.go:42: 19:58:47 | 1-069_validate_redis_secure_comm_autotls_ha/1-install | test step failed 1-install
    case.go:361: failed in step 1-install
    case.go:363: --- StatefulSet:kuttl-test-famous-swan/argocd-redis-ha-server
        +++ StatefulSet:kuttl-test-famous-swan/argocd-redis-ha-server
        @@ -1,9 +1,474 @@
         apiVersion: apps/v1
         kind: StatefulSet
         metadata:
        +  labels:
        +    app.kubernetes.io/component: redis
        +    app.kubernetes.io/managed-by: argocd
        +    app.kubernetes.io/name: argocd-redis-ha-server
        +    app.kubernetes.io/part-of: argocd
        +  managedFields:
        +  - apiVersion: apps/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:metadata:
        +        f:labels:
        +          .: {}
        +          f:app.kubernetes.io/component: {}
        +          f:app.kubernetes.io/managed-by: {}
        +          f:app.kubernetes.io/name: {}
        +          f:app.kubernetes.io/part-of: {}
        +        f:ownerReferences:
        +          .: {}
        +          k:{"uid":"4810d745-1a93-4ae7-b00a-76b9f566c5aa"}: {}
        +      f:spec:
        +        f:podManagementPolicy: {}
        +        f:replicas: {}
        +        f:revisionHistoryLimit: {}
        +        f:selector: {}
        +        f:serviceName: {}
        +        f:template:
        +          f:metadata:
        +            f:annotations:
        +              .: {}
        +              f:checksum/init-config: {}
        +            f:labels:
        +              .: {}
        +              f:app.kubernetes.io/name: {}
        +          f:spec:
        +            f:affinity:
        +              .: {}
        +              f:podAntiAffinity:
        +                .: {}
        +                f:requiredDuringSchedulingIgnoredDuringExecution: {}
        +            f:automountServiceAccountToken: {}
        +            f:containers:
        +              k:{"name":"redis"}:
        +                .: {}
        +                f:args: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:livenessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:name: {}
        +                f:ports:
        +                  .: {}
        +                  k:{"containerPort":6379,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                f:readinessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                  f:runAsNonRoot: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/redis/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/data"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/health"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +              k:{"name":"sentinel"}:
        +                .: {}
        +                f:args: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:livenessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:name: {}
        +                f:ports:
        +                  .: {}
        +                  k:{"containerPort":26379,"protocol":"TCP"}:
        +                    .: {}
        +                    f:containerPort: {}
        +                    f:name: {}
        +                    f:protocol: {}
        +                f:readinessProbe:
        +                  .: {}
        +                  f:exec:
        +                    .: {}
        +                    f:command: {}
        +                  f:failureThreshold: {}
        +                  f:initialDelaySeconds: {}
        +                  f:periodSeconds: {}
        +                  f:successThreshold: {}
        +                  f:timeoutSeconds: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                  f:runAsNonRoot: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/redis/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/data"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/health"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +            f:dnsPolicy: {}
        +            f:initContainers:
        +              .: {}
        +              k:{"name":"config-init"}:
        +                .: {}
        +                f:args: {}
        +                f:env:
        +                  .: {}
        +                  k:{"name":"SENTINEL_ID_0"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SENTINEL_ID_1"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                  k:{"name":"SENTINEL_ID_2"}:
        +                    .: {}
        +                    f:name: {}
        +                    f:value: {}
        +                f:image: {}
        +                f:imagePullPolicy: {}
        +                f:name: {}
        +                f:resources: {}
        +                f:securityContext:
        +                  .: {}
        +                  f:allowPrivilegeEscalation: {}
        +                  f:capabilities:
        +                    .: {}
        +                    f:drop: {}
        +                  f:runAsNonRoot: {}
        +                f:terminationMessagePath: {}
        +                f:terminationMessagePolicy: {}
        +                f:volumeMounts:
        +                  .: {}
        +                  k:{"mountPath":"/app/config/redis/tls"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/data"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                  k:{"mountPath":"/readonly-config"}:
        +                    .: {}
        +                    f:mountPath: {}
        +                    f:name: {}
        +                    f:readOnly: {}
        +            f:nodeSelector: {}
        +            f:restartPolicy: {}
        +            f:schedulerName: {}
        +            f:securityContext:
        +              .: {}
        +              f:fsGroup: {}
        +              f:runAsNonRoot: {}
        +              f:runAsUser: {}
        +              f:seccompProfile:
        +                .: {}
        +                f:type: {}
        +            f:serviceAccount: {}
        +            f:serviceAccountName: {}
        +            f:terminationGracePeriodSeconds: {}
        +            f:volumes:
        +              .: {}
        +              k:{"name":"argocd-operator-redis-tls"}:
        +                .: {}
        +                f:name: {}
        +                f:secret:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:optional: {}
        +                  f:secretName: {}
        +              k:{"name":"config"}:
        +                .: {}
        +                f:configMap:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:name: {}
        +                f:name: {}
        +              k:{"name":"data"}:
        +                .: {}
        +                f:emptyDir: {}
        +                f:name: {}
        +              k:{"name":"health"}:
        +                .: {}
        +                f:configMap:
        +                  .: {}
        +                  f:defaultMode: {}
        +                  f:name: {}
        +                f:name: {}
        +        f:updateStrategy:
        +          f:type: {}
        +    manager: manager
        +    operation: Update
        +    time: "2023-06-07T14:08:47Z"
        +  - apiVersion: apps/v1
        +    fieldsType: FieldsV1
        +    fieldsV1:
        +      f:status:
        +        f:availableReplicas: {}
        +        f:collisionCount: {}
        +        f:currentReplicas: {}
        +        f:currentRevision: {}
        +        f:observedGeneration: {}
        +        f:readyReplicas: {}
        +        f:replicas: {}
        +        f:updateRevision: {}
        +        f:updatedReplicas: {}
        +    manager: kube-controller-manager
        +    operation: Update
        +    subresource: status
        +    time: "2023-06-07T14:17:04Z"
           name: argocd-redis-ha-server
           namespace: kuttl-test-famous-swan
        +  ownerReferences:
        +  - apiVersion: argoproj.io/v1alpha1
        +    blockOwnerDeletion: true
        +    controller: true
        +    kind: ArgoCD
        +    name: argocd
        +    uid: 4810d745-1a93-4ae7-b00a-76b9f566c5aa
        +spec:
        +  podManagementPolicy: OrderedReady
        +  replicas: 3
        +  revisionHistoryLimit: 10
        +  selector:
        +    matchLabels:
        +      app.kubernetes.io/name: argocd-redis-ha
        +  serviceName: argocd-redis-ha
        +  template:
        +    metadata:
        +      annotations:
        +        checksum/init-config: 7128bfbb51eafaffe3c33b1b463e15f0cf6514cec570f9d9c4f2396f28c724ac
        +      creationTimestamp: null
        +      labels:
        +        app.kubernetes.io/name: argocd-redis-ha
        +    spec:
        +      affinity:
        +        podAntiAffinity:
        +          requiredDuringSchedulingIgnoredDuringExecution:
        +          - labelSelector:
        +              matchLabels:
        +                app.kubernetes.io/name: argocd-redis-ha
        +            topologyKey: kubernetes.io/hostname
        +      automountServiceAccountToken: false
        +      containers:
        +      - args:
        +        - redis-server
        +        - /data/conf/redis.conf
        +        image: registry.redhat.io/rhel8/redis-6:1-110
        +        imagePullPolicy: IfNotPresent
        +        livenessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/redis_liveness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        name: redis
        +        ports:
        +        - containerPort: 6379
        +          name: redis
        +          protocol: TCP
        +        readinessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/redis_readiness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +          runAsNonRoot: true
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /data
        +          name: data
        +        - mountPath: /health
        +          name: health
        +        - mountPath: /app/config/redis/tls
        +          name: argocd-operator-redis-tls
        +      - args:
        +        - redis-sentinel
        +        - /data/conf/sentinel.conf
        +        image: registry.redhat.io/rhel8/redis-6:1-110
        +        imagePullPolicy: IfNotPresent
        +        livenessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/sentinel_liveness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        name: sentinel
        +        ports:
        +        - containerPort: 26379
        +          name: sentinel
        +          protocol: TCP
        +        readinessProbe:
        +          exec:
        +            command:
        +            - sh
        +            - -c
        +            - /health/sentinel_liveness.sh
        +          failureThreshold: 5
        +          initialDelaySeconds: 30
        +          periodSeconds: 15
        +          successThreshold: 1
        +          timeoutSeconds: 15
        +        resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +          runAsNonRoot: true
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /data
        +          name: data
        +        - mountPath: /health
        +          name: health
        +        - mountPath: /app/config/redis/tls
        +          name: argocd-operator-redis-tls
        +      dnsPolicy: ClusterFirst
        +      initContainers:
        +      - args:
        +        - sh
        +        - /readonly-config/init.sh
        +        env:
        +        - name: SENTINEL_ID_0
        +          value: 3c0d9c0320bb34888c2df5757c718ce6ca992ce6
        +        - name: SENTINEL_ID_1
        +          value: 40000915ab58c3fa8fd888fb8b24711944e6cbb4
        +        - name: SENTINEL_ID_2
        +          value: 2bbec7894d954a8af3bb54d13eaec53cb024e2ca
        +        image: registry.redhat.io/rhel8/redis-6:1-110
        +        imagePullPolicy: IfNotPresent
        +        name: config-init
        +        resources: {}
        +        securityContext:
        +          allowPrivilegeEscalation: false
        +          capabilities:
        +            drop:
        +            - ALL
        +          runAsNonRoot: true
        +        terminationMessagePath: /dev/termination-log
        +        terminationMessagePolicy: File
        +        volumeMounts:
        +        - mountPath: /readonly-config
        +          name: config
        +          readOnly: true
        +        - mountPath: /data
        +          name: data
        +        - mountPath: /app/config/redis/tls
        +          name: argocd-operator-redis-tls
        +      nodeSelector:
        +        kubernetes.io/os: linux
        +      restartPolicy: Always
        +      schedulerName: default-scheduler
        +      securityContext:
        +        fsGroup: 1000
        +        runAsNonRoot: true
        +        runAsUser: 1000
        +        seccompProfile:
        +          type: RuntimeDefault
        +      serviceAccount: argocd-argocd-redis-ha
        +      serviceAccountName: argocd-argocd-redis-ha
        +      terminationGracePeriodSeconds: 60
        +      volumes:
        +      - configMap:
        +          defaultMode: 420
        +          name: argocd-redis-ha-configmap
        +        name: config
        +      - configMap:
        +          defaultMode: 493
        +          name: argocd-redis-ha-health-configmap
        +        name: health
        +      - emptyDir: {}
        +        name: data
        +      - name: argocd-operator-redis-tls
        +        secret:
        +          defaultMode: 420
        +          optional: true
        +          secretName: argocd-operator-redis-tls
        +  updateStrategy:
        +    type: RollingUpdate
         status:
        -  readyReplicas: 3
        +  availableReplicas: 2
        +  collisionCount: 0
        +  currentReplicas: 3
        +  currentRevision: argocd-redis-ha-server-797b4b5d4c
        +  observedGeneration: 1
        +  readyReplicas: 2
           replicas: 3
        +  updateRevision: argocd-redis-ha-server-797b4b5d4c
        +  updatedReplicas: 3
         
        
    case.go:363: resource StatefulSet:kuttl-test-famous-swan/argocd-redis-ha-server: .status.readyReplicas: value mismatch, expected: 3 != actual: 2
    logger.go:42: 19:58:47 | 1-069_validate_redis_secure_comm_autotls_ha | skipping kubernetes event logging
    logger.go:42: 19:58:47 | 1-069_validate_redis_secure_comm_autotls_ha | Deleting namespace: kuttl-test-famous-swan
=== CONT  kuttl
    harness.go:399: run tests finished
    harness.go:508: cleaning up
    harness.go:563: removing temp folder: ""
--- FAIL: kuttl (3003.45s)
    --- FAIL: kuttl/harness (0.00s)
        --- PASS: kuttl/harness/1-003_validate_console_link (10.00s)
        --- FAIL: kuttl/harness/1-038_validate_productized_images (47.07s)
        --- PASS: kuttl/harness/1-058_validate_prometheus_rule (56.59s)
        --- PASS: kuttl/harness/1-053_validate_cluster_admin_rbac (39.52s)
        --- PASS: kuttl/harness/1-049_validate_parallelism_limit (115.68s)
        --- PASS: kuttl/harness/1-054_validate_deploymentconfig (102.40s)
        --- PASS: kuttl/harness/1-055_validate_notification_controller (159.85s)
        --- PASS: kuttl/harness/1-057_validate_notifications (199.10s)
        --- PASS: kuttl/harness/1-033_validate_resource_exclusions (30.41s)
        --- PASS: kuttl/harness/1-037_validate_argocd_setting_replicas (84.57s)
        --- PASS: kuttl/harness/1-025-validate-managed-by-change (123.50s)
        --- FAIL: kuttl/harness/1-051-validate_csv_permissions (24.03s)
        --- PASS: kuttl/harness/1-052_validate_rolebinding_number (29.81s)
        --- PASS: kuttl/harness/1-032_validate_resource_inclusions (30.31s)
        --- PASS: kuttl/harness/1-098_validate_dex_clientsecret_deprecated (38.54s)
        --- PASS: kuttl/harness/1-030_validate_reencrypt (92.65s)
        --- PASS: kuttl/harness/1-087_validate_repo_server_settings (64.82s)
        --- PASS: kuttl/harness/1-084_validate_status_host_ingress (17.41s)
        --- PASS: kuttl/harness/1-029_validate_tls_secret_no_scale (55.72s)
        --- PASS: kuttl/harness/1-012_validate-managed-by-chain (100.35s)
        --- PASS: kuttl/harness/1-008_validate-custom-argocd-namespace (77.86s)
        --- PASS: kuttl/harness/1-083_validate_kustomize_namereference (39.80s)
        --- PASS: kuttl/harness/1-009_validate-manage-other-namespace (75.47s)
        --- PASS: kuttl/harness/1-023_validate_repo_server_tls (84.28s)
        --- PASS: kuttl/harness/1-021_validate_rolebindings (8.56s)
        --- PASS: kuttl/harness/1-007_validate_namespace_scoped_install (49.96s)
        --- PASS: kuttl/harness/1-082_validate_node_placement (82.28s)
        --- PASS: kuttl/harness/1-081_validate_applicationset_deployment (6.04s)
        --- PASS: kuttl/harness/1-019_validate_volume_mounts (8.40s)
        --- FAIL: kuttl/harness/1-073_validate_rhsso (1209.17s)
        --- FAIL: kuttl/harness/1-077_validate_disable_dex_removed (8.98s)
        --- FAIL: kuttl/harness/1-036_validate_keycloak_resource_reqs (1275.36s)
        --- FAIL: kuttl/harness/1-045_validate_repo_exec_timeout (30.36s)
        --- FAIL: kuttl/harness/1-090_validate_permissions (1210.59s)
        --- PASS: kuttl/harness/1-075_validate_dex_anyuid (33.37s)
        --- PASS: kuttl/harness/1-047_validate_custom_env (38.69s)
        --- PASS: kuttl/harness/1-044_validate_resource_limit_changes (39.13s)
        --- PASS: kuttl/harness/1-074_validate_terminating_namespace_block (55.53s)
        --- PASS: kuttl/harness/1-039_validate_fix_argocd-tls-certs-cm (33.99s)
        --- FAIL: kuttl/harness/1-083_validate_resource_customization_subkeys (1211.64s)
        --- PASS: kuttl/harness/1-048_validate_controller_sharding (65.85s)
        --- PASS: kuttl/harness/1-072_validate_liveness_probe_removed (19.80s)
        --- PASS: kuttl/harness/1-062_validate_extra_config (61.62s)
        --- PASS: kuttl/harness/1-043_validate_log_level_format (41.20s)
        --- PASS: kuttl/harness/1-066_validate_redis_secure_comm_no_autotls_no_ha (142.95s)
        --- PASS: kuttl/harness/1-061_validate_resource_tracking_method (43.40s)
        --- PASS: kuttl/harness/1-065_validate_redis_ha_anti_affinity (16.50s)
        --- PASS: kuttl/harness/1-063_validate_statefulset_restart (45.16s)
        --- PASS: kuttl/harness/1-064_validate_security_contexts (18.23s)
        --- FAIL: kuttl/harness/1-080_validate_regex_support_argocd_rbac (1245.55s)
        --- PASS: kuttl/harness/1-063_validate_dex_liveness_probe (6.70s)
        --- PASS: kuttl/harness/1-070_validate_config_management_plugin (94.63s)
        --- PASS: kuttl/harness/1-068_validate_redis_secure_comm_autotls_no_ha (113.36s)
        --- FAIL: kuttl/harness/1-079_validate_vars_for_notificaitons (1259.22s)
        --- FAIL: kuttl/harness/1-071_validate_SCC_HA (1253.15s)
        --- FAIL: kuttl/harness/1-067_validate_redis_secure_comm_no_autotls_ha (1206.49s)
        --- FAIL: kuttl/harness/1-069_validate_redis_secure_comm_autotls_ha (1208.23s)
FAIL
